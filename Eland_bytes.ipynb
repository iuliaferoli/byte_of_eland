{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eland Model Import | Elasticsearch & Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "We will connect to our Elastic portal for our project and interact with the bytes cluster through both the dev console and python clients.\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eland as ed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use whatever method of storing and entering your authentication details you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('foobar.ini')\n",
    "cloud_id = config[\"cloud-connection\"][\"cloud_id\"]\n",
    "user = config[\"cloud-connection\"][\"user\"] # by default user = \"elastic\"\n",
    "password = config[\"cloud-connection\"][\"password\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the Elastic client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'instance-0000000000', 'cluster_name': '08432564cc524d578f625c659b781d42', 'cluster_uuid': 'CmD383sySgagfyqUCNlHhA', 'version': {'number': '8.10.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '6d20dd8ce62365be9b1aca96427de4622e970e9e', 'build_date': '2023-09-19T08:16:24.564900370Z', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = Elasticsearch(\n",
    "    cloud_id=cloud_id,  # cloud id can be found under deployment management\n",
    "    basic_auth=(user, password) # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")\n",
    "\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-index data for our session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'bytes-discuss-06'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = \"bytes-discuss-06\"\n",
    "settings = {\n",
    "    \"index\": {\n",
    "      \"number_of_shards\": 1,\n",
    "      \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"english_stemmer\": {\n",
    "          \"type\": \"stemmer\",\n",
    "          \"language\": \"english\"\n",
    "        }\n",
    "      },\n",
    "      \"tokenizer\": {\n",
    "        \"char_group\": {\n",
    "          \"type\": \"char_group\",\n",
    "          \"tokenize_on_chars\": [\n",
    "            \"whitespace\",\n",
    "            \"punctuation\"\n",
    "          ]\n",
    "        }\n",
    "      }, \n",
    "      \"analyzer\": {\n",
    "        \"html_analyzer\": {\n",
    "          \"filter\": [ \"lowercase\" ],\n",
    "          \"char_filter\": [ \"html_strip\" ],\n",
    "          \"tokenizer\": \"standard\"\n",
    "        },\n",
    "        \"title_analyzer\": {\n",
    "          \"char_filter\": [ \"html_strip\" ],\n",
    "          \"tokenizer\": \"char_group\",\n",
    "          \"filter\": [ \"lowercase\", \"english_stemmer\" ]\n",
    "        },\n",
    "        \"path_analyzer\": {\n",
    "          \"filter\": [ \"lowercase\" ],\n",
    "          \"tokenizer\": \"path_hierarchy\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "}\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "      \"category_name\": {\n",
    "        \"type\": \"keyword\"\n",
    "      },\n",
    "      \"duration\": {\n",
    "        \"type\": \"unsigned_long\"\n",
    "      },\n",
    "      \"question\": {\n",
    "        \"properties\": {\n",
    "          \"author\": {\n",
    "            \"properties\": {\n",
    "              \"avatar_template\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"path_analyzer\"\n",
    "              },\n",
    "              \"name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                  \"keyword\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                  }\n",
    "                }\n",
    "              },\n",
    "              \"username\": {\n",
    "                \"type\": \"search_as_you_type\"\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"date\": {\n",
    "            \"type\": \"date\"\n",
    "          },\n",
    "          \"reads\": {\n",
    "            \"type\": \"short\"\n",
    "          },\n",
    "          \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"html_analyzer\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"solution\": {\n",
    "        \"properties\": {\n",
    "          \"author\": {\n",
    "            \"properties\": {\n",
    "              \"avatar_template\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"path_analyzer\"\n",
    "              },\n",
    "              \"name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fields\": {\n",
    "                  \"keyword\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                  }\n",
    "                }\n",
    "              },\n",
    "              \"username\": {\n",
    "                \"type\": \"search_as_you_type\"\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"date\": {\n",
    "            \"type\": \"date\"\n",
    "          },\n",
    "          \"post_number\": {\n",
    "            \"type\": \"short\"\n",
    "          },\n",
    "          \"reads\": {\n",
    "            \"type\": \"short\"\n",
    "          },\n",
    "          \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"html_analyzer\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"title\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"title_analyzer\"\n",
    "      },\n",
    "      \"topic\": {\n",
    "        \"type\": \"unsigned_long\"\n",
    "      }\n",
    "    }\n",
    "}\n",
    "\n",
    "client.indices.create(index=index, settings=settings, mappings=mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26153, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import helpers\n",
    "\n",
    "helpers.reindex(client = client, source_index=\"bytes-discuss-04\", target_index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking our connection to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get back 10000 results, here are the top ones:\n",
      "1.0 {'topic': 194038, 'title': '_grokparsefailure problem', 'category_name': 'Logstash', 'question': {'text': '<p>Hello,<br>\\nI have a problem with my filter, i get the \"_grokparsefailure\" tag when the concerned logs are processed.<br>\\nHere is my filter file:<br>\\nfilter {<br>\\nif [source] == \"/var/log/auth.log\"{<br>\\ngrok {<br>\\nmatch =&gt;  [ \"message\", \"%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:[%{POSINT:pid}])?: %{GREEDYDATA:smt}: %{GREEDYDATA:smt2} for user %{USER:user}\" ]<br>\\nadd_field =&gt; [ \"received_at\", \"%{<span class=\"mention\">@timestamp</span>}\" ]<br>\\nadd_field =&gt; [ \"received_from\", \"%{host}\" ]<br>\\n}<br>\\ndate {<br>\\nmatch =&gt;  [ \"date\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ]<br>\\n}<br>\\n}<br>\\n}<br>\\nI tried it with the input: \"Aug  6 12:17:01 stack CRON[14336]: pam_unix(cron:session): session closed for user root\" on  <a href=\"http://grokdebug.herokuapp.com/\" rel=\"nofollow noopener\">http://grokdebug.herokuapp.com/</a>  and it works fine.<br>\\nIs there any problem with my code ?<br>\\nThank you,<br>\\nManal</p>', 'author': {'username': 'Manal_Sadgal', 'name': 'Manal Sadgal', 'avatar_template': '/user_avatar/discuss.elastic.co/manal_sadgal/{size}/51763_2.png'}, 'date': '2019-08-06T14:48:36.345Z', 'reads': 5}, 'solution': {'text': \"<p>A space was missing , that's why it wasn't working. I fixed it. Thank you !</p>\", 'post_number': 3, 'author': {'username': 'Manal_Sadgal', 'name': 'Manal Sadgal', 'avatar_template': '/user_avatar/discuss.elastic.co/manal_sadgal/{size}/51763_2.png'}, 'date': '2019-08-07T07:42:22.016Z', 'reads': 5}, 'duration': 1014}\n",
      "1.0 {'topic': 194051, 'title': 'How to update alias to capture new matching index patterns?', 'category_name': 'Kibana', 'question': {'text': '<h1>TL;DR</h1>\\n<p>How do I continuously update an alias to capture new matching index patterns?</p>\\n<h1>Longer Version...</h1>\\n<p>We use a non-standard format that looks like this:</p>\\n<pre><code>myapp-beatname-version-YYYY.mm.dd\\n</code></pre>\\n<p>I.e., a <code>heartbeat</code> index will look like:</p>\\n<pre><code>myapp-heartbeat-7.2.0-2019.08.06\\n</code></pre>\\n<p>When using the Uptime module, the <a href=\"https://www.elastic.co/guide/en/uptime/current/install-heartbeat.html\" rel=\"nofollow noopener\">heartbeat documentation</a> notes that the Uptime module matches the index pattern <code>heartbeat-7*</code> and suggests using an alias if the index format doesn\\'t match that. However, the alias documentation makes it clear that the following alias specification:</p>\\n<pre><code>{\\n    \"actions\" : [\\n        { \"add\" : { \"index\" : \"*-heartbeat-*\", \"alias\" : \"heartbeat-7.x.x\" } }\\n    ]\\n}\\n</code></pre>\\n<p>However, the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html\" rel=\"nofollow noopener\">index alias documentation</a> states that:</p>\\n<blockquote>\\n<p>In this case, the alias is a point-in-time alias that will group all current indices that match, it will not automatically update as new indices that match this pattern are added/removed.</p>\\n</blockquote>\\n<p>Obviously, this does not work well with the Uptime module, where the alias must continuously update and capture newly-created aliases.</p>\\n<p>I\\'m certain there is a way around this, but I\\'m still enough of a novice in the ELK stack that I don\\'t see it and haven\\'t been able to find it in my searches.</p>', 'author': {'username': 'DougR', 'name': 'DougR', 'avatar_template': '/user_avatar/discuss.elastic.co/dougr/{size}/48095_2.png'}, 'date': '2019-08-06T16:04:21.724Z', 'reads': 16}, 'solution': {'text': '<p>My workaround for this was to add the following to the template:</p>\\n<pre><code>\"aliases\": {\\n  \"heartbeat-7.3.0-alias\": {}\\n},\\n</code></pre>\\n<p>This has resolved the issue. I already use a script to upload templates to Elasticsearch. This script also edits the templates on the fly to add an additional index pattern match in order to apply it to our alternate index-name convention, so I should be able to update it to add the alias on the fly as well.</p>', 'post_number': 4, 'author': {'username': 'DougR', 'name': 'DougR', 'avatar_template': '/user_avatar/discuss.elastic.co/dougr/{size}/48095_2.png'}, 'date': '2019-08-07T13:35:33.476Z', 'reads': 12}, 'duration': 1291}\n",
      "1.0 {'topic': 194052, 'title': 'Streamed Logs not loaded into Kibana', 'category_name': 'Kibana', 'question': {'text': '<p>Hi there,</p>\\n<p>I am attempting to stream logs from AWS Lambda into Elasticsearch/Kibana.</p>\\n<p>I am doing so via a POST to the Elasticsearch endpoint, + the index and document type (e.g. POST <a href=\"https://example-es-domain.com/alblogs/alb-access-logs/\" rel=\"nofollow noopener\">https://example-es-domain.com/alblogs/alb-access-logs/</a>).</p>\\n<p>I have verified that the lambda is sending out the request by sending to requestbin, and have verified that the information contained in each doc is correct.</p>\\n<p>However I can\\'t see any of the information sent to Elasticsearch being loaded into Kibana, and was wondering if I was missing something.</p>\\n<p>The full flow for the operation is:</p>\\n<p>ALB -&gt; S3 -&gt; AWS Lambda -&gt; Elasticsearch</p>', 'author': {'username': 'ben.sharp', 'name': '', 'avatar_template': 'https://avatars.discourse-cdn.com/v4/letter/b/f08c70/{size}.png'}, 'date': '2019-08-06T16:09:05.638Z', 'reads': 30}, 'solution': {'text': '<p>Please try the following, tested locally in 6.8, I\\'ve used you index + type naming, which is in your case identical.</p>\\n<p>PUT a pattern for you index</p>\\n<pre><code>PUT alb-access-logs\\n{\\n  \"mappings\": {\\n    \"alb-access-logs\": {\\n      \"properties\": {\\n        \"timestamp\": {\\n          \"type\": \"date\"\\n        }\\n      }\\n    }\\n  }\\n}\\n</code></pre>\\n<p>POST an example</p>\\n<pre><code>POST alb-access-logs/alb-access-logs\\n {\\n    \"timestamp\": \"2019-08-06T15:54:46.701974Z\"\\n }\\n</code></pre>\\n<p>Let\\'s check the results:</p>\\n<pre><code> GET alb-access-logs/_search \\n{\\n  \"sort\" : [\\n      {\"timestamp\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\\n   ]\\n}\\n</code></pre>\\n<p>this should work</p>', 'post_number': 16, 'author': {'username': 'matw', 'name': 'Matthias Wilhelm', 'avatar_template': '/user_avatar/discuss.elastic.co/matw/{size}/13913_2.png'}, 'date': '2019-08-12T14:51:28.673Z', 'reads': 11}, 'duration': 8562}\n",
      "1.0 {'topic': 194058, 'title': None, 'category_name': None, 'question': {'text': '<ol>\\n<li>\\n<p>There is no pipeline plugin. Unlike other inputs it is implemented in the logstash core.</p>\\n</li>\\n<li>\\n<p>Yes, you can keep using files, no need to use config.string. config.string is simpler to use in the documentation.</p>\\n</li>\\n<li>\\n<p>No, weblogs is not the pipeline id, it is the virtual address of the pipeline input. The pipeline can be called anything.</p>\\n</li>\\n</ol>', 'author': {'username': 'Badger', 'name': '', 'avatar_template': '/user_avatar/discuss.elastic.co/badger/{size}/25190_2.png'}, 'date': '2019-08-06T16:41:52.004Z', 'reads': 102}, 'solution': {'text': '<p>Okay. I have solved the pipeline virtual address problem. The name cannot contain <code>-</code> character, so renaming it to \"legacyva\" resolved my problem.</p>\\n<p>There is some weirdness with yaml that I don\\'t quite understand. As you have described, putting quotes around the [type] variable causes the pipeline to fail to load. Also, setting the type in filebeat to anything other than <code>log</code> causes filebeat to be unable to start. I\\'m thinking it may be expecting certain inputs, but in any case I cannot seem to name this what I want, with or without quotes.</p>\\n<p>So I really need \"tags\" or some other construct in order to perform the loop properly, and I must put quotes and must not put dashes. I am concerned that using \"tags\" affects the logic, as I am only looking for a single flag match, and == I believe means matches exactly. I\\'ll keep testing to see if I can work out this last piece.</p>\\n<p>edit&gt; Did some searching on \"logstash tags conditionals\" and found that the syntax is different. Not sure where this is documented but I copied a gentleman\\'s <a href=\"https://discuss.elastic.co/t/logstash-filter-conditional-if-tags-or/102479\">example</a> and changed from <code>if [tags] == \"tagname\"</code> to <code>if \"tagname\" in [tags]</code>. I do feel better about this, even if I don\\'t understand it, as that == was really bothering me since it needed to be more of a \"contains\" rather than \"is equal to\" (if I\\'m remembering things correctly). Anyways, glad I learned something, and now my conditional statement is catching my file based on a tag! HOORAY!</p>\\n<p>Big thanks to <a class=\"mention\" href=\"/u/badger\">@Badger</a>, I kept trying to drive off the course and you definitely got me back on path more than once!</p>\\n<p>For anyone else who is trying to do specifically this thing in the future, that is to say <strong>ship many different files with filebeat to a single logstash listener, then separate the pipelines by some pre-defined value so that many filters can be used and some modularity is provided for the index choice</strong>, here is the base template you need for your pipeline.conf files:</p>\\n<pre><code>master-pipeline.conf\\ninput {\\n  beats {\\n    port =&gt; \"5000\"\\n  }\\n}\\noutput {\\n### conditional statement to separate logs by tag ###\\n  if \"primary\" in [tags] {\\n    pipeline { send_to =&gt; primaryvirtualaddress }\\n  } else {\\n    pipeline { send_to =&gt; fallbackvirtualaddress }\\n  }\\n}\\n</code></pre>\\n<p>each subsequent pipeline:</p>\\n<pre><code>primary-pipeline.conf\\ninput {\\n  pipeline {\\n    address =&gt; \"primaryvirtualaddress\"\\n  }\\n}\\noutput {\\n  elasticsearch {\\n    hosts =&gt; [\"ES CLUSTER ADDRESS\"]\\n    index =&gt; \"primary_index\"\\n  }\\n}\\n</code></pre>\\n<p>You may be able to remove the quotes from the address, but I\\'m quite sick of re-testing at this point and I know this template works <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>', 'post_number': 21, 'author': {'username': 'penguinairlines', 'name': 'Andy', 'avatar_template': 'https://avatars.discourse-cdn.com/v4/letter/p/9d8465/{size}.png'}, 'date': '2019-08-07T23:08:41.522Z', 'reads': 48}, 'duration': 1827}\n",
      "1.0 {'topic': 194074, 'title': 'Error in filter elasticsearch plugin', 'category_name': 'Logstash', 'question': {'text': '<p>Hi all,</p>\\n<p>I am using version 7.2.</p>\\n<p>I encountered some trouble while using the filter elasticsearch plugin.</p>\\n<p>The behavior is very random, some of the data uploaded correctly but I also get some error</p>\\n<p>Here is the log from logstash</p>\\n<blockquote>\\n<p>[2019-08-06T10:07:02,177][WARN ][logstash.filters.elasticsearch] Failed to query elasticsearch for previous event {:index=&gt;\"mapping_device_sn\", :error=&gt;\"[400] {\"error\":{\"root_cause\":[{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"mapping_device_sn\",\"node\":\"kuAYIY0yTlCZM7Du464gYg\",\"reason\":{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Cannot parse \\'sn:%{[device_sn]}\\': Encountered \\\\\" \\\\\"]\\\\\" \\\\\"] \\\\\"\\\\\" at line 1, column 15.\\\\nWas expecting:\\\\n    \\\\\"TO\\\\\" ...\\\\n    \",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Encountered \\\\\" \\\\\"]\\\\\" \\\\\"] \\\\\"\\\\\" at line 1, column 15.\\\\nWas expecting:\\\\n    \\\\\"TO\\\\\" ...\\\\n    \"}}}}]},\"status\":400}\"}</p>\\n</blockquote>\\n<p>Log from elasticsearch</p>\\n<pre><code>[elasticsearch.server][DEBUG] All shards failed for phase: [query]\\n\\n[elasticsearch.server][DEBUG] [0], node[kuAYIY0yTlCZM7Du464gYg], [P], s[STARTED], a[id=NKeTm6rjTgCj_zwuFPyacA]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[mapping_device_sn], indicesOptions=IndicesOptions[ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_aliases_to_multiple_indices=true, forbid_closed_indices=true, ignore_aliases=false, ignore_throttled=true], types=[], routing=\\'null\\', preference=\\'null\\', requestCache=null, scroll=null, maxConcurrentShardRequests=0, batchedReduceSize=512, preFilterShardSize=128, allowPartialSearchResults=true, localClusterAlias=null, getOrCreateAbsoluteStartMillis=-1, ccsMinimizeRoundtrips=true, source={\"size\":1,\"query\":{\"query_string\":{\"query\":\"sn:%{[device_sn]}\",\"fields\":[],\"type\":\"best_fields\",\"default_operator\":\"or\",\"max_determinized_states\":10000,\"enable_position_increments\":true,\"fuzziness\":\"AUTO\",\"fuzzy_prefix_length\":0,\"fuzzy_max_expansions\":50,\"phrase_slop\":0,\"analyze_wildcard\":false,\"escape\":false,\"auto_generate_synonyms_phrase_query\":true,\"fuzzy_transpositions\":true,\"boost\":1.0}},\"sort\":[{\"@timestamp\":{\"order\":\"desc\"}}]}}]\\n</code></pre>\\n<p>My logstash config</p>\\n<pre><code>input {\\n    file {\\n        path =&gt; \"/data/ELK_raw/IPS/data/*/ips_aggregate.csv\"\\n        sincedb_path =&gt; \"/dev/null\"\\n        mode =&gt; \"read\"\\n        file_completed_action =&gt; \"log\"\\n        file_completed_log_path =&gt; \"/data/ELK/read_log/ips_read_log.txt\"\\n        type =&gt; \"ips\"\\n    }\\n}\\n\\nfilter {\\n    csv {\\n        autodetect_column_names =&gt; \"true\"\\n        autogenerate_column_names =&gt; \"true\"\\n        skip_header =&gt; \"true\"\\n        separator =&gt; \",\"\\n    }\\nelasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_ips\"\\n        query =&gt; \"id:%{[id]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \" signature_name\" =&gt; \"signature_name\"\\n            \" engine_rule\" =&gt; \"engine_rule\"\\n        }\\n    }\\n\\n   elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_device_sn\"\\n        query =&gt; \"sn:%{[device_sn]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \"first_industry\" =&gt; \"first_industry\"\\n            \"customer\" =&gt; \"customer\"\\n            \"is_trial\" =&gt; \"is_trial\"\\n            \"product_type\" =&gt; \"product_type\"\\n            \"second_industry\" =&gt; \"second_industry\"\\n            \"warranty_date\" =&gt; \"warranty_date\"\\n        }\\n    }\\n\\n   mutate {\\n        remove_field =&gt; [ \"@timestamp\" ]\\n        remove_field =&gt; [ \"@version\" ]\\n        remove_field =&gt; [ \"host\" ]\\n        remove_field =&gt; [ \"message\" ]\\n        remove_field =&gt; [ \"path\" ] \\n        remove_field =&gt; [ \"type\" ] \\n    }   \\n}\\n\\noutput {\\n    elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"cv_ips\"\\n    }\\n}\\n</code></pre>\\n<p>Have anyone encountered this before?</p>\\n<p>Thanks</p>', 'author': {'username': 'leeyu', 'name': '', 'avatar_template': '/user_avatar/discuss.elastic.co/leeyu/{size}/48561_2.png'}, 'date': '2019-08-06T17:51:54.408Z', 'reads': 36}, 'solution': {'text': \"<p>Can you do the 'grep -rn ...' and verify that the line number is always 1?</p>\\n<p>You do have --pipeline.workers 1 set, right?</p>\", 'post_number': 7, 'author': {'username': 'Badger', 'name': '', 'avatar_template': '/user_avatar/discuss.elastic.co/badger/{size}/25190_2.png'}, 'date': '2019-08-06T22:32:24.947Z', 'reads': 21}, 'duration': 281}\n",
      "1.0 {'topic': 194077, 'title': 'Kibana Maps: Results limited to first 10000 documents', 'category_name': 'Kibana', 'question': {'text': '<p>Hi all,</p>\\n<p>I\\'m testing the Kibana Maps feature (7.3 version) and it\\'s really great, kudos! However, as I intend to work with a huge (about billions of documents) index, I could not help but notice that a message saying \"Results limited to first 10000 documents\" is shown when I hover the mouse over each layer header.</p>\\n<p>I would like to configure this limit... Is it possible?</p>\\n<p>I tried to increase the value of the \"index.max_result_window\" setting, to no avail.</p>\\n<p>Thanks in advance.</p>', 'author': {'username': 'Celio_Nogueira_de_Fa', 'name': 'Celio Jr', 'avatar_template': '/user_avatar/discuss.elastic.co/celio_nogueira_de_fa/{size}/859_2.png'}, 'date': '2019-08-06T18:15:07.170Z', 'reads': 33}, 'solution': {'text': '<p>It looks like this setting is not based on the <code>index.max_result_window</code> setting, as you can see in the constants file in the new maps app: <a href=\"https://github.com/elastic/kibana/blob/master/x-pack/legacy/plugins/maps/common/constants.js#L41\" rel=\"nofollow noopener\">https://github.com/elastic/kibana/blob/master/x-pack/legacy/plugins/maps/common/constants.js#L41</a></p>\\n<p>This is probably worth filing an issue on Github for.</p>', 'post_number': 2, 'author': {'username': 'wylie', 'name': 'Wylie Conlon', 'avatar_template': '/user_avatar/discuss.elastic.co/wylie/{size}/81794_2.png'}, 'date': '2019-08-06T19:12:56.896Z', 'reads': 32}, 'duration': 58}\n",
      "1.0 {'topic': 194085, 'title': 'Grok Stops Reading at a Backslash?', 'category_name': 'Logstash', 'question': {'text': '<p>This is an extension to a <a href=\"https://discuss.elastic.co/t/grok-parsing-a-string-with-spaces/193855\">previously solved topic</a>.</p>\\n<p>I have this log here (as it appears in the console)</p>\\n<p><code>2019-08-05 08:55:15 - jcramer(Software Programmer) - Successfully updated profile</code></p>\\n<p>that I\\'ve successfully been able to parse with this Grok configuration,</p>\\n<pre><code class=\"lang-auto\">grok { \\n    match =&gt; { \"message\" =&gt; \"^%{TIMESTAMP_ISO8601:syslog_timestamp} - \\n    %{USERNAME:syslog_username}\\\\((?&lt;syslog_userinfo&gt;[^)]*)\\\\)\\\\s+- \\n    %{GREEDYDATA:syslog_message}\" } \\n}\\n</code></pre>\\n<p>however, I receive a _grokparsefailure when the username appears like this in the console:</p>\\n<p><code>2019-08-05 08:55:15 - STEM\\\\\\\\psmith(Hardware Engineer) - Successfully updated profile</code></p>\\n<p>The acronym before the username is meant to represent something like a department or location. Somehow the inclusion of the backslash causes Grok to not recognize the pattern. I would like to parse the username so that the backslash and information prior to it are included in the username.</p>\\n<p>So, the end result can look like this</p>\\n<pre><code class=\"lang-auto\">\"syslog_username\" =&gt; \"jcramer\"\\n</code></pre>\\n<p>or this</p>\\n<pre><code class=\"lang-auto\">\"syslog_username\" =&gt; \"STEM\\\\\\\\psmith\"\\n</code></pre>', 'author': {'username': 'jakecramer17', 'name': 'Jake Cramer', 'avatar_template': 'https://avatars.discourse-cdn.com/v4/letter/j/9dc877/{size}.png'}, 'date': '2019-08-06T18:50:04.643Z', 'reads': 36}, 'solution': {'text': '<p>Use a custom pattern with the first part optional</p>\\n<pre><code>(?&lt;syslog_username&gt;([a-zA-Z0-9\\\\._-]+\\\\\\\\\\\\\\\\)?[a-zA-Z0-9\\\\._-]+)\\\\((?&lt;syslog_userinfo&gt;[^)]*)\\\\)</code></pre>', 'post_number': 2, 'author': {'username': 'Badger', 'name': '', 'avatar_template': '/user_avatar/discuss.elastic.co/badger/{size}/25190_2.png'}, 'date': '2019-08-06T18:58:33.701Z', 'reads': 34}, 'duration': 8}\n",
      "1.0 {'topic': 194091, 'title': 'Deleting an index does not seem to delete external versioning', 'category_name': 'Elasticsearch', 'question': {'text': '<p>I am running into an issue using the external versioning where deleting the index does not seem to clear the external versioning for that given index.</p>\\n<p>Steps to recreate:</p>\\n<ol>\\n<li>Create an index</li>\\n<li>Put an item with</li>\\n</ol>\\n<pre><code class=\"lang-auto\">id: \\'someid\\',\\nversion: 0,\\nversion_type: \\'external\\',\\n</code></pre>\\n<p>into the index</p>\\n<ol start=\"3\">\\n<li>delete the index</li>\\n<li>Wait arbitrarily long</li>\\n<li>Create the same index</li>\\n<li>Put the same item with</li>\\n</ol>\\n<pre><code class=\"lang-auto\">id: \\'someid\\',\\nversion: 0,\\nversion_type: \\'external\\',\\n</code></pre>\\n<p>into the index and I get a <code>version_conflict_engine_exception</code></p>\\n<pre><code class=\"lang-auto\">\"name\": \"ResponseError\",\\n  \"meta\": {\\n    \"body\": {\\n      \"error\": {\\n        \"root_cause\": [\\n          {\\n            \"type\": \"version_conflict_engine_exception\",\\n            \"reason\": \"[contact_v1][someid]: version conflict, current version [1] is higher or equal to the one provided [1]\",\\n            \"index_uuid\": \"PvmFtcibTfmBCu8C7iQ43Q\",\\n            \"shard\": \"0\",\\n            \"index\": \"stpze-contacts-3\"\\n          }\\n        ],\\n        \"type\": \"version_conflict_engine_exception\",\\n        \"reason\": \"[contact_v1][someid]: version conflict, current version [1] is higher or equal to the one provided [1]\",\\n        \"index_uuid\": \"PvmFtcibTfmBCu8C7iQ43Q\",\\n        \"shard\": \"0\",\\n        \"index\": \"stpze-contacts-3\"\\n      },\\n      \"status\": 409\\n    },\\n    \"statusCode\": 409,\\n</code></pre>\\n<p>My elastic cluster is running in a docker container</p>\\n<pre><code class=\"lang-auto\">{\\n  \"name\": \"0_cod3F\",\\n  \"cluster_name\": \"docker-cluster\",\\n  \"cluster_uuid\": \"Cvy3FFwdTuuwXHG8S6ws6A\",\\n  \"version\": {\\n    \"number\": \"6.4.3\",\\n    \"build_flavor\": \"default\",\\n    \"build_type\": \"tar\",\\n    \"build_hash\": \"fe40335\",\\n    \"build_date\": \"2018-10-30T23:17:19.084789Z\",\\n    \"build_snapshot\": false,\\n    \"lucene_version\": \"7.4.0\",\\n    \"minimum_wire_compatibility_version\": \"5.6.0\",\\n    \"minimum_index_compatibility_version\": \"5.0.0\"\\n  },\\n  \"tagline\": \"You Know, for Search\"\\n}\\n</code></pre>\\n<p>When printing all entries in the index it returns an empty list</p>\\n<pre><code class=\"lang-auto\">{\\n    \"took\": 1,\\n    \"timed_out\": false,\\n    \"_shards\": {\\n        \"total\": 3,\\n        \"successful\": 3,\\n        \"skipped\": 0,\\n        \"failed\": 0\\n    },\\n    \"hits\": {\\n        \"total\": 0,\\n        \"max_score\": null,\\n        \"hits\": []\\n    }\\n}\\n</code></pre>\\n<p>Is there some extra step involved in deleting the <code>external</code> verisoning metadata?</p>', 'author': {'username': 'Philiiiiiipp', 'name': 'Philipp', 'avatar_template': '/user_avatar/discuss.elastic.co/philiiiiiipp/{size}/51782_2.png'}, 'date': '2019-08-06T19:16:56.311Z', 'reads': 15}, 'solution': {'text': '<p>Ok yes deleting an externally-versioned document from an index that does not exist will actually create the index in order to record the deletion:</p>\\n<pre><code class=\"lang-auto\">GET /_cat/indices\\n\\n# 200 OK\\n\\nDELETE /i/_doc/someid?version=0&amp;version_type=external\\n\\n# 404 Not Found\\n# {\\n#   \"_type\": \"_doc\",\\n#   \"_primary_term\": 1,\\n#   \"_id\": \"someid\",\\n#   \"_shards\": {\\n#     \"successful\": 1,\\n#     \"total\": 2,\\n#     \"failed\": 0\\n#   },\\n#   \"_index\": \"i\",\\n#   \"result\": \"not_found\",\\n#   \"_version\": 0,\\n#   \"_seq_no\": 0\\n# }\\n\\nGET /_cat/indices\\n\\n# 200 OK\\n# yellow open i Pqo2tMyMRyOTOXvFAUk0Tw 5 1 0 0 1.1kb 1.1kb\\n#\\n</code></pre>', 'post_number': 6, 'author': {'username': 'DavidTurner', 'name': 'David Turner', 'avatar_template': '/user_avatar/discuss.elastic.co/davidturner/{size}/22453_2.png'}, 'date': '2019-08-07T05:54:55.862Z', 'reads': 10}, 'duration': 638}\n",
      "1.0 {'topic': 194111, 'title': 'NGram search troubles(Is it possible to match entire string)', 'category_name': 'Elasticsearch', 'question': {'text': '<p>I am almost positive that this is a simple misunderstanding on my part as I\\'m very new to Elasticsearch.  I am trying to implement a substring matching search using ngrams.  I\\'ve got it set as max_ngram_diff of 10. My setup looks something like:</p>\\n<pre><code>  \"filter\": {\\n      \"barcode_filter\": {\\n        \"type\": \"nGram\",\\n        \"min_gram\": \"4\",\\n        \"max_gram\": \"14\"\\n      }\\n    },\\n    \"analyzer\": {\\n      \"barcode_filter_analyzer\": {\\n        \"filter\": [\\n          \"lowercase\",\\n          \"barcode_filter\"\\n        ],\\n        \"type\": \"custom\",\\n        \"tokenizer\": \"standard\"\\n      }\\n    }\\n</code></pre>\\n<p>My field is defined as barcode with analyzer set to barcode_filter_analyzer.</p>\\n<p>My goal is to be able to find substrings. So something like \"V741\"  should find every barcode with that substring.  Something like \"ZR000041\" should also find every barcode containing that entire substring.  While the second example will find every barcode containing the entire substring, it also finds every barcode containing \"0000\" for example. My initial thought was to use the score but that doesn\\'t seem to work as a barcode like 00000000000101 will end up having a very high score.</p>\\n<p>Is there any good way to require that the entire query string be found? This is currently only in testing on my PC so anything requiring scrapping and restarting the index is not a big deal so if I\\'m approaching this in entirely the wrong way I\\'m happy to adjust.  Any help at all would be greatly appreciated!</p>', 'author': {'username': 'N_Antech', 'name': '', 'avatar_template': 'https://avatars.discourse-cdn.com/v4/letter/n/898d66/{size}.png'}, 'date': '2019-08-06T22:45:52.047Z', 'reads': 20}, 'solution': {'text': '<p>To add to Alex\\' explanation here, one solution could be to provide a search analyzer that does not apply the ngram filter to the query terms. You could do that by configuring a <code>search_analyzer</code> in your mapping, or by providing an <code>analyzer</code> in your query:</p>\\n<pre><code class=\"lang-auto\">GET test/_search \\n{\\n  \"query\": {\\n    \"match\": {\\n      \"my_field\": {\\n        \"query\": \"ZR000041\",\\n        \"analyzer\": \"standard\"\\n      }\\n    }\\n  }\\n}\\n</code></pre>', 'post_number': 3, 'author': {'username': 'abdon', 'name': 'Abdon Pijpelink', 'avatar_template': '/user_avatar/discuss.elastic.co/abdon/{size}/9195_2.png'}, 'date': '2019-08-07T15:03:13.749Z', 'reads': 14}, 'duration': 977}\n",
      "1.0 {'topic': 194129, 'title': 'Functionbeat license error', 'category_name': 'Beats', 'question': {'text': '<p>I have successfully setup functionbeat on aws, but I am getting following license error, license manager stops and function stops:</p>\\n<p>2019-08-07T03:11:55.435Z\\tINFO\\t[functionbeat]\\tbeater/functionbeat.go:74\\tFunctionbeat is running<br>\\n2019-08-07T03:11:55.435Z\\tINFO\\telasticsearch/client.go:170\\tElasticsearch url: <a href=\"http://10.56.0.100:9200\" rel=\"nofollow noopener\">http://10.56.0.100:9200</a><br>\\n2019-08-07T03:11:55.435Z\\tINFO\\t[license-manager]\\tlicenser/manager.go:184\\tLicense manager started, retrieving initial license<br>\\n2019-08-07T03:11:55.435Z\\tINFO\\t[functionbeat]\\tlicenser/manager.go:331\\tWaiting on synchronous license check<br>\\n2019-08-07T03:11:55.435Z\\tINFO\\t[monitoring]\\tlog/log.go:118\\tStarting metrics logging every 30s<br>\\n2019-08-07T03:11:55.987Z\\tINFO\\telasticsearch/client.go:743\\tAttempting to connect to Elasticsearch version 7.3.0<br>\\n2019-08-07T03:11:56.026Z\\tINFO\\t[license-manager]\\tlicenser/manager.go:282\\tValid license retrieved\\t{\"license mode\": \"Open source\", \"type\": \"Open source\", \"status\": \"Active\"}<br>\\n2019-08-07T03:11:56.026Z\\tINFO\\t[functionbeat]\\tlicenser/check.go:35\\tLicense is active for Basic<br>\\n2019-08-07T03:11:56.026Z\\tINFO\\t[functionbeat]\\tlicenser/manager.go:339\\tLicense is valid, mode: Open source<br>\\n2019-08-07T03:11:56.026Z\\tINFO\\t[license-manager]\\tlicenser/manager.go:211\\tLicense manager stopped<br>\\n2019-08-07T03:11:56.026Z\\tINFO\\t[functionbeat]\\tbeater/functionbeat.go:86\\tFunctionbeat stopped running<br>\\n2019-08-07T03:11:56.028Z\\tINFO\\t[monitoring]\\tlog/log.go:154\\tUptime: 639.743036ms<br>\\n2019-08-07T03:11:56.028Z\\tINFO\\t[monitoring]\\tlog/log.go:131\\tStopping metrics logging.<br>\\n2019-08-07T03:11:56.028Z\\tINFO\\tinstance/beat.go:431\\tfunctionbeat stopped.<br>\\n2019-08-07T03:11:56.028Z\\tERROR\\tinstance/beat.go:877\\tExiting: invalid license<br>\\nExiting: invalid license</p>\\n<p>Will appreciate quick response</p>', 'author': {'username': 'smahmud', 'name': 'Shahid Mahmud', 'avatar_template': '/user_avatar/discuss.elastic.co/smahmud/{size}/51804_2.png'}, 'date': '2019-08-07T03:19:04.013Z', 'reads': 16}, 'solution': {'text': '<p>You are trying to connect to an Elasticsearch with OSS license. You need at least Basic license in ES for Functionbeat to run. You can download ES with basic license here: <a href=\"https://www.elastic.co/subscriptions\" rel=\"nofollow noopener\">https://www.elastic.co/subscriptions</a></p>', 'post_number': 2, 'author': {'username': 'kvch', 'name': 'Noémi Ványi', 'avatar_template': '/user_avatar/discuss.elastic.co/kvch/{size}/72058_2.png'}, 'date': '2019-08-07T12:12:23.783Z', 'reads': 15}, 'duration': 533}\n"
     ]
    }
   ],
   "source": [
    "index = \"bytes-discuss-06\"\n",
    "response = client.search(index = index)\n",
    "\n",
    "\n",
    "print(\"We get back {total} results, here are the top ones:\".format(total=response[\"hits\"]['total']['value']))\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_score\"], hit['_source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eland is Elastic's Python client for Machine Learning capabilities. \n",
    "\n",
    "Check out the docs here: https://eland.readthedocs.io/en/v8.10.1/index.html \n",
    "\n",
    "\n",
    "We can use the native data frame introduced by Eland for data analysis, and we can import models for both simple ML tasks like classification, as well as more complex external models for NLP capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>question.author.avatar_template</th>\n",
       "      <th>question.author.name</th>\n",
       "      <th>question.author.username</th>\n",
       "      <th>question.date</th>\n",
       "      <th>question.reads</th>\n",
       "      <th>question.text</th>\n",
       "      <th>solution.author.avatar_template</th>\n",
       "      <th>solution.author.name</th>\n",
       "      <th>solution.author.username</th>\n",
       "      <th>solution.date</th>\n",
       "      <th>solution.post_number</th>\n",
       "      <th>solution.reads</th>\n",
       "      <th>solution.text</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194038</th>\n",
       "      <td>Logstash</td>\n",
       "      <td>1014</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/manal_sadgal/{...</td>\n",
       "      <td>Manal Sadgal</td>\n",
       "      <td>Manal_Sadgal</td>\n",
       "      <td>2019-08-06 14:48:36.345000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;p&gt;Hello,&lt;br&gt;\\nI have a problem with my filter...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/manal_sadgal/{...</td>\n",
       "      <td>Manal Sadgal</td>\n",
       "      <td>Manal_Sadgal</td>\n",
       "      <td>2019-08-07 07:42:22.016000+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;p&gt;A space was missing , that's why it wasn't ...</td>\n",
       "      <td>_grokparsefailure problem</td>\n",
       "      <td>194038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194051</th>\n",
       "      <td>Kibana</td>\n",
       "      <td>1291</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/dougr/{size}/4...</td>\n",
       "      <td>DougR</td>\n",
       "      <td>DougR</td>\n",
       "      <td>2019-08-06 16:04:21.724000+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>&lt;h1&gt;TL;DR&lt;/h1&gt;\\n&lt;p&gt;How do I continuously updat...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/dougr/{size}/4...</td>\n",
       "      <td>DougR</td>\n",
       "      <td>DougR</td>\n",
       "      <td>2019-08-07 13:35:33.476000+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>&lt;p&gt;My workaround for this was to add the follo...</td>\n",
       "      <td>How to update alias to capture new matching in...</td>\n",
       "      <td>194051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194052</th>\n",
       "      <td>Kibana</td>\n",
       "      <td>8562</td>\n",
       "      <td>https://avatars.discourse-cdn.com/v4/letter/b/...</td>\n",
       "      <td></td>\n",
       "      <td>ben.sharp</td>\n",
       "      <td>2019-08-06 16:09:05.638000+00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>&lt;p&gt;Hi there,&lt;/p&gt;\\n&lt;p&gt;I am attempting to stream...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/matw/{size}/13...</td>\n",
       "      <td>Matthias Wilhelm</td>\n",
       "      <td>matw</td>\n",
       "      <td>2019-08-12 14:51:28.673000+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;p&gt;Please try the following, tested locally in...</td>\n",
       "      <td>Streamed Logs not loaded into Kibana</td>\n",
       "      <td>194052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194058</th>\n",
       "      <td>None</td>\n",
       "      <td>1827</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/badger/{size}/...</td>\n",
       "      <td></td>\n",
       "      <td>Badger</td>\n",
       "      <td>2019-08-06 16:41:52.004000+00:00</td>\n",
       "      <td>102</td>\n",
       "      <td>&lt;ol&gt;\\n&lt;li&gt;\\n&lt;p&gt;There is no pipeline plugin. Un...</td>\n",
       "      <td>https://avatars.discourse-cdn.com/v4/letter/p/...</td>\n",
       "      <td>Andy</td>\n",
       "      <td>penguinairlines</td>\n",
       "      <td>2019-08-07 23:08:41.522000+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>&lt;p&gt;Okay. I have solved the pipeline virtual ad...</td>\n",
       "      <td>None</td>\n",
       "      <td>194058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194074</th>\n",
       "      <td>Logstash</td>\n",
       "      <td>281</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/leeyu/{size}/4...</td>\n",
       "      <td></td>\n",
       "      <td>leeyu</td>\n",
       "      <td>2019-08-06 17:51:54.408000+00:00</td>\n",
       "      <td>36</td>\n",
       "      <td>&lt;p&gt;Hi all,&lt;/p&gt;\\n&lt;p&gt;I am using version 7.2.&lt;/p&gt;...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/badger/{size}/...</td>\n",
       "      <td></td>\n",
       "      <td>Badger</td>\n",
       "      <td>2019-08-06 22:32:24.947000+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>&lt;p&gt;Can you do the 'grep -rn ...' and verify th...</td>\n",
       "      <td>Error in filter elasticsearch plugin</td>\n",
       "      <td>194074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343469</th>\n",
       "      <td>Elastic Agent</td>\n",
       "      <td>89</td>\n",
       "      <td>https://avatars.discourse-cdn.com/v4/letter/r/...</td>\n",
       "      <td>Rachel Yang</td>\n",
       "      <td>rachelyang</td>\n",
       "      <td>2023-09-20 14:35:11.214000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;p&gt;Hi,&lt;br&gt;\\nI want to integrate the AWS logs i...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/leandrojmp/{si...</td>\n",
       "      <td>Leandro Pereira</td>\n",
       "      <td>leandrojmp</td>\n",
       "      <td>2023-09-20 16:04:35.452000+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;aside class=\"quote no-group\" data-username=\"r...</td>\n",
       "      <td>AWS logging integration error</td>\n",
       "      <td>343469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343471</th>\n",
       "      <td>Kibana</td>\n",
       "      <td>275</td>\n",
       "      <td>https://avatars.discourse-cdn.com/v4/letter/e/...</td>\n",
       "      <td></td>\n",
       "      <td>emmanuel_t</td>\n",
       "      <td>2023-09-20 14:49:21.600000+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;p&gt;I see that the saved objects API is depreca...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/priscilla_paro...</td>\n",
       "      <td>Priscilla Parodi</td>\n",
       "      <td>Priscilla_Parodi</td>\n",
       "      <td>2023-09-20 19:24:16.162000+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;p&gt;Hello &lt;a class=\"mention\" href=\"/u/emmanuel_...</td>\n",
       "      <td>REST api: delete dashboard</td>\n",
       "      <td>343471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343472</th>\n",
       "      <td>Beats</td>\n",
       "      <td>281</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/krische/{size}...</td>\n",
       "      <td></td>\n",
       "      <td>krische</td>\n",
       "      <td>2023-09-20 15:12:50.580000+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;p&gt;I have ECK setup and running on my kubernet...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/krische/{size}...</td>\n",
       "      <td></td>\n",
       "      <td>krische</td>\n",
       "      <td>2023-09-20 19:54:06.290000+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;p&gt;It looks like part of my issue was a typo i...</td>\n",
       "      <td>Can&amp;rsquo;t get Filebeat to ship Nginx Ingress...</td>\n",
       "      <td>343472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343478</th>\n",
       "      <td>Logstash</td>\n",
       "      <td>16</td>\n",
       "      <td>https://avatars.discourse-cdn.com/v4/letter/m/...</td>\n",
       "      <td></td>\n",
       "      <td>mikec1</td>\n",
       "      <td>2023-09-20 16:37:50.130000+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;p&gt;I have a pipeline whereby the input section...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/leandrojmp/{si...</td>\n",
       "      <td>Leandro Pereira</td>\n",
       "      <td>leandrojmp</td>\n",
       "      <td>2023-09-20 16:54:12.050000+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;aside class=\"quote no-group\" data-username=\"m...</td>\n",
       "      <td>Invalid cron expression for schedule field of ...</td>\n",
       "      <td>343478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343495</th>\n",
       "      <td>APM</td>\n",
       "      <td>727</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/senyam08/{size...</td>\n",
       "      <td>Senthilkumar rasan</td>\n",
       "      <td>senyam08</td>\n",
       "      <td>2023-09-21 01:24:30.282000+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;p&gt;Could you help with setting up async profil...</td>\n",
       "      <td>/user_avatar/discuss.elastic.co/senyam08/{size...</td>\n",
       "      <td>Senthilkumar rasan</td>\n",
       "      <td>senyam08</td>\n",
       "      <td>2023-09-21 13:31:50.352000+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;Thanks&lt;/p&gt;</td>\n",
       "      <td>Implement Async profiler with elastic APM</td>\n",
       "      <td>343495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<p>26153 rows × 17 columns</p>"
      ],
      "text/plain": [
       "        category_name  duration  \\\n",
       "194038       Logstash      1014   \n",
       "194051         Kibana      1291   \n",
       "194052         Kibana      8562   \n",
       "194058           None      1827   \n",
       "194074       Logstash       281   \n",
       "...               ...       ...   \n",
       "343469  Elastic Agent        89   \n",
       "343471         Kibana       275   \n",
       "343472          Beats       281   \n",
       "343478       Logstash        16   \n",
       "343495            APM       727   \n",
       "\n",
       "                                        question.author.avatar_template  \\\n",
       "194038  /user_avatar/discuss.elastic.co/manal_sadgal/{size}/51763_2.png   \n",
       "194051         /user_avatar/discuss.elastic.co/dougr/{size}/48095_2.png   \n",
       "194052  https://avatars.discourse-cdn.com/v4/letter/b/f08c70/{size}.png   \n",
       "194058        /user_avatar/discuss.elastic.co/badger/{size}/25190_2.png   \n",
       "194074         /user_avatar/discuss.elastic.co/leeyu/{size}/48561_2.png   \n",
       "...                                                                 ...   \n",
       "343469  https://avatars.discourse-cdn.com/v4/letter/r/51bf81/{size}.png   \n",
       "343471  https://avatars.discourse-cdn.com/v4/letter/e/c5a1d2/{size}.png   \n",
       "343472      /user_avatar/discuss.elastic.co/krische/{size}/112302_2.png   \n",
       "343478  https://avatars.discourse-cdn.com/v4/letter/m/e0b2c6/{size}.png   \n",
       "343495     /user_avatar/discuss.elastic.co/senyam08/{size}/106230_2.png   \n",
       "\n",
       "       question.author.name question.author.username  \\\n",
       "194038         Manal Sadgal             Manal_Sadgal   \n",
       "194051                DougR                    DougR   \n",
       "194052                                     ben.sharp   \n",
       "194058                                        Badger   \n",
       "194074                                         leeyu   \n",
       "...                     ...                      ...   \n",
       "343469          Rachel Yang               rachelyang   \n",
       "343471                                    emmanuel_t   \n",
       "343472                                       krische   \n",
       "343478                                        mikec1   \n",
       "343495   Senthilkumar rasan                 senyam08   \n",
       "\n",
       "                          question.date  question.reads  \\\n",
       "194038 2019-08-06 14:48:36.345000+00:00               5   \n",
       "194051 2019-08-06 16:04:21.724000+00:00              16   \n",
       "194052 2019-08-06 16:09:05.638000+00:00              30   \n",
       "194058 2019-08-06 16:41:52.004000+00:00             102   \n",
       "194074 2019-08-06 17:51:54.408000+00:00              36   \n",
       "...                                 ...             ...   \n",
       "343469 2023-09-20 14:35:11.214000+00:00               5   \n",
       "343471 2023-09-20 14:49:21.600000+00:00               5   \n",
       "343472 2023-09-20 15:12:50.580000+00:00               7   \n",
       "343478 2023-09-20 16:37:50.130000+00:00               4   \n",
       "343495 2023-09-21 01:24:30.282000+00:00               3   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      question.text  \\\n",
       "194038                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <p>Hello,<br>\\nI have a problem with my filter, i get the \"_grokparsefailure\" tag when the concerned logs are processed.<br>\\nHere is my filter file:<br>\\nfilter {<br>\\nif [source] == \"/var/log/auth.log\"{<br>\\ngrok {<br>\\nmatch =&gt;  [ \"message\", \"%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:[%{POSINT:pid}])?: %{GREEDYDATA:smt}: %{GREEDYDATA:smt2} for user %{USER:user}\" ]<br>\\nadd_field =&gt; [ \"received_at\", \"%{<span class=\"mention\">@timestamp</span>}\" ]<br>\\nadd_field =&gt; [ \"received_from\", \"%{host}\" ]<br>\\n}<br>\\ndate {<br>\\nmatch =&gt;  [ \"date\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ]<br>\\n}<br>\\n}<br>\\n}<br>\\nI tried it with the input: \"Aug  6 12:17:01 stack CRON[14336]: pam_unix(cron:session): session closed for user root\" on  <a href=\"http://grokdebug.herokuapp.com/\" rel=\"nofollow noopener\">http://grokdebug.herokuapp.com/</a>  and it works fine.<br>\\nIs there any problem with my code ?<br>\\nThank you,<br>\\nManal</p>   \n",
       "194051                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <h1>TL;DR</h1>\\n<p>How do I continuously update an alias to capture new matching index patterns?</p>\\n<h1>Longer Version...</h1>\\n<p>We use a non-standard format that looks like this:</p>\\n<pre><code>myapp-beatname-version-YYYY.mm.dd\\n</code></pre>\\n<p>I.e., a <code>heartbeat</code> index will look like:</p>\\n<pre><code>myapp-heartbeat-7.2.0-2019.08.06\\n</code></pre>\\n<p>When using the Uptime module, the <a href=\"https://www.elastic.co/guide/en/uptime/current/install-heartbeat.html\" rel=\"nofollow noopener\">heartbeat documentation</a> notes that the Uptime module matches the index pattern <code>heartbeat-7*</code> and suggests using an alias if the index format doesn't match that. However, the alias documentation makes it clear that the following alias specification:</p>\\n<pre><code>{\\n    \"actions\" : [\\n        { \"add\" : { \"index\" : \"*-heartbeat-*\", \"alias\" : \"heartbeat-7.x.x\" } }\\n    ]\\n}\\n</code></pre>\\n<p>However, the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html\" rel=\"nofollow noopener\">index alias documentation</a> states that:</p>\\n<blockquote>\\n<p>In this case, the alias is a point-in-time alias that will group all current indices that match, it will not automatically update as new indices that match this pattern are added/removed.</p>\\n</blockquote>\\n<p>Obviously, this does not work well with the Uptime module, where the alias must continuously update and capture newly-created aliases.</p>\\n<p>I'm certain there is a way around this, but I'm still enough of a novice in the ELK stack that I don't see it and haven't been able to find it in my searches.</p>   \n",
       "194052                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <p>Hi there,</p>\\n<p>I am attempting to stream logs from AWS Lambda into Elasticsearch/Kibana.</p>\\n<p>I am doing so via a POST to the Elasticsearch endpoint, + the index and document type (e.g. POST <a href=\"https://example-es-domain.com/alblogs/alb-access-logs/\" rel=\"nofollow noopener\">https://example-es-domain.com/alblogs/alb-access-logs/</a>).</p>\\n<p>I have verified that the lambda is sending out the request by sending to requestbin, and have verified that the information contained in each doc is correct.</p>\\n<p>However I can't see any of the information sent to Elasticsearch being loaded into Kibana, and was wondering if I was missing something.</p>\\n<p>The full flow for the operation is:</p>\\n<p>ALB -&gt; S3 -&gt; AWS Lambda -&gt; Elasticsearch</p>   \n",
       "194058                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <ol>\\n<li>\\n<p>There is no pipeline plugin. Unlike other inputs it is implemented in the logstash core.</p>\\n</li>\\n<li>\\n<p>Yes, you can keep using files, no need to use config.string. config.string is simpler to use in the documentation.</p>\\n</li>\\n<li>\\n<p>No, weblogs is not the pipeline id, it is the virtual address of the pipeline input. The pipeline can be called anything.</p>\\n</li>\\n</ol>   \n",
       "194074                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <p>Hi all,</p>\\n<p>I am using version 7.2.</p>\\n<p>I encountered some trouble while using the filter elasticsearch plugin.</p>\\n<p>The behavior is very random, some of the data uploaded correctly but I also get some error</p>\\n<p>Here is the log from logstash</p>\\n<blockquote>\\n<p>[2019-08-06T10:07:02,177][WARN ][logstash.filters.elasticsearch] Failed to query elasticsearch for previous event {:index=&gt;\"mapping_device_sn\", :error=&gt;\"[400] {\"error\":{\"root_cause\":[{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"mapping_device_sn\",\"node\":\"kuAYIY0yTlCZM7Du464gYg\",\"reason\":{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Cannot parse 'sn:%{[device_sn]}': Encountered \\\" \\\"]\\\" \\\"] \\\"\\\" at line 1, column 15.\\nWas expecting:\\n    \\\"TO\\\" ...\\n    \",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Encountered \\\" \\\"]\\\" \\\"] \\\"\\\" at line 1, column 15.\\nWas expecting:\\n    \\\"TO\\\" ...\\n    \"}}}}]},\"status\":400}\"}</p>\\n</blockquote>\\n<p>Log from elasticsearch</p>\\n<pre><code>[elasticsearch.server][DEBUG] All shards failed for phase: [query]\\n\\n[elasticsearch.server][DEBUG] [0], node[kuAYIY0yTlCZM7Du464gYg], [P], s[STARTED], a[id=NKeTm6rjTgCj_zwuFPyacA]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[mapping_device_sn], indicesOptions=IndicesOptions[ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_aliases_to_multiple_indices=true, forbid_closed_indices=true, ignore_aliases=false, ignore_throttled=true], types=[], routing='null', preference='null', requestCache=null, scroll=null, maxConcurrentShardRequests=0, batchedReduceSize=512, preFilterShardSize=128, allowPartialSearchResults=true, localClusterAlias=null, getOrCreateAbsoluteStartMillis=-1, ccsMinimizeRoundtrips=true, source={\"size\":1,\"query\":{\"query_string\":{\"query\":\"sn:%{[device_sn]}\",\"fields\":[],\"type\":\"best_fields\",\"default_operator\":\"or\",\"max_determinized_states\":10000,\"enable_position_increments\":true,\"fuzziness\":\"AUTO\",\"fuzzy_prefix_length\":0,\"fuzzy_max_expansions\":50,\"phrase_slop\":0,\"analyze_wildcard\":false,\"escape\":false,\"auto_generate_synonyms_phrase_query\":true,\"fuzzy_transpositions\":true,\"boost\":1.0}},\"sort\":[{\"@timestamp\":{\"order\":\"desc\"}}]}}]\\n</code></pre>\\n<p>My logstash config</p>\\n<pre><code>input {\\n    file {\\n        path =&gt; \"/data/ELK_raw/IPS/data/*/ips_aggregate.csv\"\\n        sincedb_path =&gt; \"/dev/null\"\\n        mode =&gt; \"read\"\\n        file_completed_action =&gt; \"log\"\\n        file_completed_log_path =&gt; \"/data/ELK/read_log/ips_read_log.txt\"\\n        type =&gt; \"ips\"\\n    }\\n}\\n\\nfilter {\\n    csv {\\n        autodetect_column_names =&gt; \"true\"\\n        autogenerate_column_names =&gt; \"true\"\\n        skip_header =&gt; \"true\"\\n        separator =&gt; \",\"\\n    }\\nelasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_ips\"\\n        query =&gt; \"id:%{[id]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \" signature_name\" =&gt; \"signature_name\"\\n            \" engine_rule\" =&gt; \"engine_rule\"\\n        }\\n    }\\n\\n   elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_device_sn\"\\n        query =&gt; \"sn:%{[device_sn]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \"first_industry\" =&gt; \"first_industry\"\\n            \"customer\" =&gt; \"customer\"\\n            \"is_trial\" =&gt; \"is_trial\"\\n            \"product_type\" =&gt; \"product_type\"\\n            \"second_industry\" =&gt; \"second_industry\"\\n            \"warranty_date\" =&gt; \"warranty_date\"\\n        }\\n    }\\n\\n   mutate {\\n        remove_field =&gt; [ \"@timestamp\" ]\\n        remove_field =&gt; [ \"@version\" ]\\n        remove_field =&gt; [ \"host\" ]\\n        remove_field =&gt; [ \"message\" ]\\n        remove_field =&gt; [ \"path\" ] \\n        remove_field =&gt; [ \"type\" ] \\n    }   \\n}\\n\\noutput {\\n    elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"cv_ips\"\\n    }\\n}\\n</code></pre>\\n<p>Have anyone encountered this before?</p>\\n<p>Thanks</p>   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ...   \n",
       "343469  <p>Hi,<br>\\nI want to integrate the AWS logs into my Kibana Observability log stream.<br>\\nHere is the access policy of SQS queue:</p>\\n<pre><code class=\"lang-auto\">{\\n  \"Version\": \"2012-10-17\",\\n  \"Id\": \"arn:aws:sqs:us-east-1:334811116626:ingest-ec2-logs-sqs/SQSDefaultPolicy\",\\n  \"Statement\": [\\n    {\\n      \"Sid\": \"Sid334811116626\",\\n      \"Effect\": \"Allow\",\\n      \"Principal\": {\\n        \"Service\": \"s3.amazonaws.com\"\\n      },\\n      \"Action\": [\\n        \"sqs:SendMessage\",\\n        \"sqs:ReceiveMessage\"\\n      ],\\n      \"Resource\": \"arn:aws:sqs:us-east-1:334811116626:ingest-ec2-logs-sqs\"\\n    }\\n  ]\\n}\\n</code></pre>\\n<p>I set the SQS queue as the S3 event notification destination. Here is the S3 bucket event notification setting:<br>\\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/elastic/original/3X/1/3/138c7c1d4a74326ad23bfd8d763d60464fa1caff.png\" data-download-href=\"/uploads/short-url/2MW3FSpXSqxXFs8eScfy8IG3mHR.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/elastic/optimized/3X/1/3/138c7c1d4a74326ad23bfd8d763d60464fa1caff_2_690x264.png\" alt=\"image\" data-base62-sha1=\"2MW3FSpXSqxXFs8eScfy8IG3mHR\" width=\"690\" height=\"264\" srcset=\"https://global.discourse-cdn.com/elastic/optimized/3X/1/3/138c7c1d4a74326ad23bfd8d763d60464fa1caff_2_690x264.png, https://global.discourse-cdn.com/elastic/optimized/3X/1/3/138c7c1d4a74326ad23bfd8d763d60464fa1caff_2_1035x396.png 1.5x, https://global.discourse-cdn.com/elastic/optimized/3X/1/3/138c7c1d4a74326ad23bfd8d763d60464fa1caff_2_1380x528.png 2x\" data-dominant-color=\"F4F5F5\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2853×1094 234 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div><br>\\nI also configured integration in Kibana. I entered the SQS queue URL under the \"Collect EC2 logs from S3\" category.<br>\\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https://global.discourse-cdn.com/elastic/original/3X/5/9/594b7c3dbad0128ad3525624f386e6d8a617e410.png\" data-download-href=\"/uploads/short-url/cJWdJ7xdnk7xcjrVOcQWfEurRyU.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https://global.discourse-cdn.com/elastic/optimized/3X/5/9/594b7c3dbad0128ad3525624f386e6d8a617e410_2_690x242.png\" alt=\"image\" data-base62-sha1=\"cJWdJ7xdnk7xcjrVOcQWfEurRyU\" width=\"690\" height=\"242\" srcset=\"https://global.discourse-cdn.com/elastic/optimized/3X/5/9/594b7c3dbad0128ad3525624f386e6d8a617e410_2_690x242.png, https://global.discourse-cdn.com/elastic/optimized/3X/5/9/594b7c3dbad0128ad3525624f386e6d8a617e410_2_1035x363.png 1.5x, https://global.discourse-cdn.com/elastic/optimized/3X/5/9/594b7c3dbad0128ad3525624f386e6d8a617e410_2_1380x484.png 2x\" data-dominant-color=\"F9F9FA\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"></use></svg><span class=\"filename\">image</span><span class=\"informations\">2060×725 62.1 KB</span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"></use></svg></div></a></div><br>\\nI installed elastic agent on my elasticsearch server. The data stream in Kibana is empty. Here is the error in the elastic agent log:</p>\\n<pre><code class=\"lang-auto\">{\"log.level\":\"error\",\"@timestamp\":\"2023-09-20T13:57:02.888Z\",\"message\":\"failed to perform any bulk index operations: 500 Internal Server Error: {\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"illegal_state_exception\\\",\\\"reason\\\":\\\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\\\"}],\\\"type\\\":\\\"illegal_state_exception\\\",\\\"reason\\\":\\\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\\\"},\\\"status\\\":500}\",\"component\":{\"binary\":\"metricbeat\",\"dataset\":\"elastic_agent.metricbeat\",\"id\":\"http/metrics-monitoring\",\"type\":\"http/metrics\"},\"log\":{\"source\":\"http/metrics-monitoring\"},\"log.logger\":\"elasticsearch\",\"log.origin\":{\"file.line\":258,\"file.name\":\"elasticsearch/client.go\"},\"service.name\":\"metricbeat\",\"ecs.version\":\"1.6.0\",\"ecs.version\":\"1.6.0\"}\\n{\"log.level\":\"error\",\"@timestamp\":\"2023-09-20T13:57:02.979Z\",\"message\":\"failed to publish events: 500 Internal Server Error: {\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"illegal_state_exception\\\",\\\"reason\\\":\\\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\\\"}],\\\"type\\\":\\\"illegal_state_exception\\\",\\\"reason\\\":\\\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\\\"},\\\"status\\\":500}\",\"component\":{\"binary\":\"filebeat\",\"dataset\":\"elastic_agent.filebeat\",\"id\":\"filestream-monitoring\",\"type\":\"filestream\"},\"log\":{\"source\":\"filestream-monitoring\"},\"log.logger\":\"publisher_pipeline_output\",\"log.origin\":{\"file.line\":174,\"file.name\":\"pipeline/client_worker.go\"},\"service.name\":\"filebeat\",\"ecs.version\":\"1.6.0\",\"ecs.version\":\"1.6.0\"}\\n</code></pre>\\n<p>How can I fix this issue?<br>\\nThanks</p>   \n",
       "343471                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <p>I see that the saved objects API is deprecated, and that's a real bummer, the API was extremely useful for us. For most of our use cases we can probably get by using the import/export API as a worse (from our point of view) alternative, except at least for deletion.</p>\\n<p>How could we delete a dashboard, or any other saved object, using non-deprecated APIs? Thank you!</p>\\n<p>Emmanuel</p>   \n",
       "343472                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>I have ECK setup and running on my kubernetes cluster. I followed the <a href=\"https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-beat-configuration-examples.html\">Configuration Examples</a> to setup filebeat ship all container logs to Elasticsearch. That is working fine.</p>\\n<p>However, now I am trying to parse the logs of my Nginx Ingress Controller using the nginx filebeat module so that I can better search the ingress logs. But I just can't seem to get it to work, no matter how many different examples I see online. This is what my Beat resource looks like:</p>\\n<pre><code class=\"lang-yaml\">apiVersion: beat.k8s.elastic.co/v1beta1\\nkind: Beat\\nmetadata:\\n  name: container-logs\\n  namespace: elasticsearch\\nspec:\\n  config:\\n    filebeat:\\n      autodiscover:\\n        providers:\\n          - hints:\\n              default_config:\\n                enabled: false\\n            node: ${NODE_NAME}\\n            templates:\\n              - config:\\n                  - paths:\\n                      - /var/log/containers/*${data.kubernetes.container.id}.log\\n                    type: container\\n              - condition:\\n                  equals:\\n                    kuberentes.container.name: controller\\n                config:\\n                  - ingress_controller:\\n                      enabled: true\\n                      vars:\\n                        paths:\\n                          - /var/log/containers/*${data.kubernetes.container.id}.log\\n                    module: nginx\\n            type: kubernetes\\n    processors:\\n      - add_cloud_metadata: {}\\n      - add_host_metadata: {}\\n    setup:\\n      dashboards:\\n        enabled: false\\n  daemonSet:\\n    podTemplate:\\n      spec:\\n        automountServiceAccountToken: true\\n        containers:\\n          - env:\\n              - name: NODE_NAME\\n                valueFrom:\\n                  fieldRef:\\n                    fieldPath: spec.nodeName\\n            name: filebeat\\n            volumeMounts:\\n              - mountPath: /var/log/containers\\n                name: varlogcontainers\\n              - mountPath: /var/log/pods\\n                name: varlogpods\\n              - mountPath: /var/lib/docker/containers\\n                name: varlibdockercontainers\\n        dnsPolicy: ClusterFirstWithHostNet\\n        hostNetwork: true\\n        securityContext:\\n          runAsUser: 0\\n        serviceAccountName: filebeat\\n        volumes:\\n          - hostPath:\\n              path: /var/log/containers\\n            name: varlogcontainers\\n          - hostPath:\\n              path: /var/log/pods\\n            name: varlogpods\\n          - hostPath:\\n              path: /var/lib/docker/containers\\n            name: varlibdockercontainers\\n  elasticsearchRef:\\n    name: elasticsearch\\n  kibanaRef:\\n    name: kibana\\n  monitoring:\\n    logs: {}\\n    metrics:\\n      elasticsearchRefs:\\n        - name: elasticsearch\\n          namespace: elasticsearch\\n  type: filebeat\\n  version: 8.10.1\\n</code></pre>\\n<p>Has anyone else gotten this to work on their setup? I feel like I've tried everything.</p>   \n",
       "343478                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <p>I have a pipeline whereby the input section looks like this:</p>\\n<pre><code class=\"lang-auto\">input {\\n    elasticsearch {\\n        hosts =&gt; [\"elasticsearch.my.host.here:port\"]\\n        index =&gt; 'my_index_name_pattern'\\n        query =&gt; '{ \"query\": {  \"match_all\": {} } }'\\n    }\\n}\\n</code></pre>\\n<p>I have logstash running via docker and it's being orchestrated by Marathon/Mesos. I'm trying to add a \"schedule\" field to the input plugin section so that the container stays up and the pipeline just runs once a day. Otherwise, the container closes itself once the pipeline completes which then causes Marathon to endlessly attempt to redeploy it. From my understanding of the documentation, I thought that adding the following to the elasticsearch input section would be valid:</p>\\n<pre><code class=\"lang-auto\">schedule =&gt; \"0 0 0 * * ?\"\\n</code></pre>\\n<p>This should have the pipeline get triggered once a day, every day, at midnight. I verified via elasticsearch-croneval that elasticsearch deems this as a valid expression and will create the intended interval.</p>\\n<p>However, the stdout logs are complaining that this is an invalid cron expression that cannot be handled. Can someone point out where I'm going wrong here?</p>\\n<p>Note: I've already tried using '' instead of \"\", but nothing changed.</p>\\n<p>Thanks!</p>   \n",
       "343495                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <p>Could you help with setting up async profiler to use with APM Java agent.</p>\\n<p>I have following properties added</p>\\n<pre><code class=\"lang-auto\">profiling_inferred_spans_enabled=true\\nprofiling_inferred_spans_sampling_interval=50ms\\nprofiling_inferred_spans_min_duration=150ms\\nprofiling_inferred_spans_logging_enabled=true\\n\\nDo i need to download asyncprofiler latest version and add it container with following? \\n\\nprofiling_inferred_spans_lib_directory=/opt/async-profiler-2.9-linux-x64/build/libasyncProfiler.so\\n</code></pre>\\n<p>Thanks<br>\\nSenthil</p>   \n",
       "\n",
       "                                            solution.author.avatar_template  \\\n",
       "194038      /user_avatar/discuss.elastic.co/manal_sadgal/{size}/51763_2.png   \n",
       "194051             /user_avatar/discuss.elastic.co/dougr/{size}/48095_2.png   \n",
       "194052              /user_avatar/discuss.elastic.co/matw/{size}/13913_2.png   \n",
       "194058      https://avatars.discourse-cdn.com/v4/letter/p/9d8465/{size}.png   \n",
       "194074            /user_avatar/discuss.elastic.co/badger/{size}/25190_2.png   \n",
       "...                                                                     ...   \n",
       "343469       /user_avatar/discuss.elastic.co/leandrojmp/{size}/107231_2.png   \n",
       "343471  /user_avatar/discuss.elastic.co/priscilla_parodi/{size}/43047_2.png   \n",
       "343472          /user_avatar/discuss.elastic.co/krische/{size}/112302_2.png   \n",
       "343478       /user_avatar/discuss.elastic.co/leandrojmp/{size}/107231_2.png   \n",
       "343495         /user_avatar/discuss.elastic.co/senyam08/{size}/106230_2.png   \n",
       "\n",
       "       solution.author.name solution.author.username  \\\n",
       "194038         Manal Sadgal             Manal_Sadgal   \n",
       "194051                DougR                    DougR   \n",
       "194052     Matthias Wilhelm                     matw   \n",
       "194058                 Andy          penguinairlines   \n",
       "194074                                        Badger   \n",
       "...                     ...                      ...   \n",
       "343469      Leandro Pereira               leandrojmp   \n",
       "343471     Priscilla Parodi         Priscilla_Parodi   \n",
       "343472                                       krische   \n",
       "343478      Leandro Pereira               leandrojmp   \n",
       "343495   Senthilkumar rasan                 senyam08   \n",
       "\n",
       "                          solution.date  solution.post_number  solution.reads  \\\n",
       "194038 2019-08-07 07:42:22.016000+00:00                     3               5   \n",
       "194051 2019-08-07 13:35:33.476000+00:00                     4              12   \n",
       "194052 2019-08-12 14:51:28.673000+00:00                    16              11   \n",
       "194058 2019-08-07 23:08:41.522000+00:00                    21              48   \n",
       "194074 2019-08-06 22:32:24.947000+00:00                     7              21   \n",
       "...                                 ...                   ...             ...   \n",
       "343469 2023-09-20 16:04:35.452000+00:00                     4               2   \n",
       "343471 2023-09-20 19:24:16.162000+00:00                     2               4   \n",
       "343472 2023-09-20 19:54:06.290000+00:00                     3               5   \n",
       "343478 2023-09-20 16:54:12.050000+00:00                     2               4   \n",
       "343495 2023-09-21 13:31:50.352000+00:00                     3               1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               solution.text  \\\n",
       "194038                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>A space was missing , that's why it wasn't working. I fixed it. Thank you !</p>   \n",
       "194051                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <p>My workaround for this was to add the following to the template:</p>\\n<pre><code>\"aliases\": {\\n  \"heartbeat-7.3.0-alias\": {}\\n},\\n</code></pre>\\n<p>This has resolved the issue. I already use a script to upload templates to Elasticsearch. This script also edits the templates on the fly to add an additional index pattern match in order to apply it to our alternate index-name convention, so I should be able to update it to add the alias on the fly as well.</p>   \n",
       "194052                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <p>Please try the following, tested locally in 6.8, I've used you index + type naming, which is in your case identical.</p>\\n<p>PUT a pattern for you index</p>\\n<pre><code>PUT alb-access-logs\\n{\\n  \"mappings\": {\\n    \"alb-access-logs\": {\\n      \"properties\": {\\n        \"timestamp\": {\\n          \"type\": \"date\"\\n        }\\n      }\\n    }\\n  }\\n}\\n</code></pre>\\n<p>POST an example</p>\\n<pre><code>POST alb-access-logs/alb-access-logs\\n {\\n    \"timestamp\": \"2019-08-06T15:54:46.701974Z\"\\n }\\n</code></pre>\\n<p>Let's check the results:</p>\\n<pre><code> GET alb-access-logs/_search \\n{\\n  \"sort\" : [\\n      {\"timestamp\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\\n   ]\\n}\\n</code></pre>\\n<p>this should work</p>   \n",
       "194058  <p>Okay. I have solved the pipeline virtual address problem. The name cannot contain <code>-</code> character, so renaming it to \"legacyva\" resolved my problem.</p>\\n<p>There is some weirdness with yaml that I don't quite understand. As you have described, putting quotes around the [type] variable causes the pipeline to fail to load. Also, setting the type in filebeat to anything other than <code>log</code> causes filebeat to be unable to start. I'm thinking it may be expecting certain inputs, but in any case I cannot seem to name this what I want, with or without quotes.</p>\\n<p>So I really need \"tags\" or some other construct in order to perform the loop properly, and I must put quotes and must not put dashes. I am concerned that using \"tags\" affects the logic, as I am only looking for a single flag match, and == I believe means matches exactly. I'll keep testing to see if I can work out this last piece.</p>\\n<p>edit&gt; Did some searching on \"logstash tags conditionals\" and found that the syntax is different. Not sure where this is documented but I copied a gentleman's <a href=\"https://discuss.elastic.co/t/logstash-filter-conditional-if-tags-or/102479\">example</a> and changed from <code>if [tags] == \"tagname\"</code> to <code>if \"tagname\" in [tags]</code>. I do feel better about this, even if I don't understand it, as that == was really bothering me since it needed to be more of a \"contains\" rather than \"is equal to\" (if I'm remembering things correctly). Anyways, glad I learned something, and now my conditional statement is catching my file based on a tag! HOORAY!</p>\\n<p>Big thanks to <a class=\"mention\" href=\"/u/badger\">@Badger</a>, I kept trying to drive off the course and you definitely got me back on path more than once!</p>\\n<p>For anyone else who is trying to do specifically this thing in the future, that is to say <strong>ship many different files with filebeat to a single logstash listener, then separate the pipelines by some pre-defined value so that many filters can be used and some modularity is provided for the index choice</strong>, here is the base template you need for your pipeline.conf files:</p>\\n<pre><code>master-pipeline.conf\\ninput {\\n  beats {\\n    port =&gt; \"5000\"\\n  }\\n}\\noutput {\\n### conditional statement to separate logs by tag ###\\n  if \"primary\" in [tags] {\\n    pipeline { send_to =&gt; primaryvirtualaddress }\\n  } else {\\n    pipeline { send_to =&gt; fallbackvirtualaddress }\\n  }\\n}\\n</code></pre>\\n<p>each subsequent pipeline:</p>\\n<pre><code>primary-pipeline.conf\\ninput {\\n  pipeline {\\n    address =&gt; \"primaryvirtualaddress\"\\n  }\\n}\\noutput {\\n  elasticsearch {\\n    hosts =&gt; [\"ES CLUSTER ADDRESS\"]\\n    index =&gt; \"primary_index\"\\n  }\\n}\\n</code></pre>\\n<p>You may be able to remove the quotes from the address, but I'm quite sick of re-testing at this point and I know this template works <img src=\"https://emoji.discourse-cdn.com/twitter/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"></p>   \n",
       "194074                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <p>Can you do the 'grep -rn ...' and verify that the line number is always 1?</p>\\n<p>You do have --pipeline.workers 1 set, right?</p>   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...   \n",
       "343469                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <aside class=\"quote no-group\" data-username=\"rachelyang\" data-post=\"3\" data-topic=\"343469\">\\n<div class=\"title\">\\n<div class=\"quote-controls\"></div>\\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://avatars.discourse-cdn.com/v4/letter/r/51bf81/48.png\" class=\"avatar\"> rachelyang:</div>\\n<blockquote>\\n<p>10.0.9.12 46 96 2 1.51 2.36 1.93 dm * cvelk5<br>\\n10.0.9.13 6 97 2 0.81 2.25 1.98 dm - cvelk6</p>\\n</blockquote>\\n</aside>\\n<p>Yeah, none of the nodes has the <code>ingest</code> role, you will need to add the <code>ingest</code> role as a role for at least one of the nodes in <code>elasticsearch.yml</code> and restart the node.</p>\\n<p>A node with the ingest role is required for the integrations to work.</p>\\n<p>You can check here for the <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html\">documentation about node roles</a>.</p>   \n",
       "343471                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 <p>Hello <a class=\"mention\" href=\"/u/emmanuel_t\">@emmanuel_t</a>,</p>\\n<p>This API should still work. Deprecated shouldn't mean that it isn't working. Those won't be removed before there's alternative APIs to use for at least the most commonly used objects.</p>   \n",
       "343472                                                                                                                               <p>It looks like part of my issue was a typo in the condition, I had \"kuber<strong>en</strong>tes\" instead of \"kuber<strong>ne</strong>tes\".</p>\\n<p>And I don't know if it was necessary, but I also overrode some of the input stuff. So this is what ended up working for me:</p>\\n<pre><code class=\"lang-yaml\">apiVersion: beat.k8s.elastic.co/v1beta1\\nkind: Beat\\nmetadata:\\n  name: container-logs\\n  namespace: elasticsearch\\nspec:\\n  config:\\n    filebeat:\\n      autodiscover:\\n        providers:\\n          - hints:\\n              default_config:\\n                enabled: false\\n            node: ${NODE_NAME}\\n            templates:\\n              - config:\\n                  - paths:\\n                      - /var/log/containers/*${data.kubernetes.container.id}.log\\n                    type: container\\n              - condition:\\n                  equals:\\n                    kubernetes.container.name: controller\\n                config:\\n                  - ingress_controller:\\n                      enabled: true\\n                      input:\\n                        paths:\\n                          - /var/log/containers/*${data.kubernetes.container.id}.log\\n                        type: container\\n                      vars:\\n                        paths:\\n                          - /var/log/containers/*${data.kubernetes.container.id}.log\\n                    module: nginx\\n            type: kubernetes\\n    processors:\\n      - add_cloud_metadata: {}\\n      - add_host_metadata: {}\\n    setup:\\n      dashboards:\\n        enabled: false\\n  daemonSet:\\n    podTemplate:\\n      spec:\\n        automountServiceAccountToken: true\\n        containers:\\n          - env:\\n              - name: NODE_NAME\\n                valueFrom:\\n                  fieldRef:\\n                    fieldPath: spec.nodeName\\n            name: filebeat\\n            volumeMounts:\\n              - mountPath: /var/log/containers\\n                name: varlogcontainers\\n              - mountPath: /var/log/pods\\n                name: varlogpods\\n              - mountPath: /var/lib/docker/containers\\n                name: varlibdockercontainers\\n        dnsPolicy: ClusterFirstWithHostNet\\n        hostNetwork: true\\n        securityContext:\\n          runAsUser: 0\\n        serviceAccountName: filebeat\\n        volumes:\\n          - hostPath:\\n              path: /var/log/containers\\n            name: varlogcontainers\\n          - hostPath:\\n              path: /var/log/pods\\n            name: varlogpods\\n          - hostPath:\\n              path: /var/lib/docker/containers\\n            name: varlibdockercontainers\\n  elasticsearchRef:\\n    name: elasticsearch\\n  kibanaRef:\\n    name: kibana\\n  monitoring:\\n    logs: {}\\n    metrics:\\n      elasticsearchRefs:\\n        - name: elasticsearch\\n          namespace: elasticsearch\\n  type: filebeat\\n  version: 8.10.1\\n</code></pre>   \n",
       "343478                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <aside class=\"quote no-group\" data-username=\"mikec1\" data-post=\"1\" data-topic=\"343478\">\\n<div class=\"title\">\\n<div class=\"quote-controls\"></div>\\n<img loading=\"lazy\" alt=\"\" width=\"24\" height=\"24\" src=\"https://avatars.discourse-cdn.com/v4/letter/m/e0b2c6/48.png\" class=\"avatar\"> mikec1:</div>\\n<blockquote>\\n<p><code>schedule =&gt; \"0 0 0 * * ?\"</code></p>\\n</blockquote>\\n</aside>\\n<p>I don't think this is valid, the third column represents the day and it needs to be a valid day between <code>1</code> and <code>31</code>, <code>0</code> is not valid.</p>\\n<p>If you want to run it every day at midnight you need this:</p>\\n<pre><code class=\"lang-auto\">0 0 * * *\\n</code></pre>\\n<p>This will run at 00:00 every day on UTC, if you need it to run on another timezone you need to specify it using the canonical name</p>\\n<p>Something like:</p>\\n<pre><code class=\"lang-auto\">0 0 * * * America/Chicago\\n</code></pre>   \n",
       "343495                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <p>Thanks</p>   \n",
       "\n",
       "                                                                           title  \\\n",
       "194038                                                 _grokparsefailure problem   \n",
       "194051               How to update alias to capture new matching index patterns?   \n",
       "194052                                      Streamed Logs not loaded into Kibana   \n",
       "194058                                                                      None   \n",
       "194074                                      Error in filter elasticsearch plugin   \n",
       "...                                                                          ...   \n",
       "343469                                             AWS logging integration error   \n",
       "343471                                                REST api: delete dashboard   \n",
       "343472  Can&rsquo;t get Filebeat to ship Nginx Ingress Controller logs using ECK   \n",
       "343478  Invalid cron expression for schedule field of elasticsearch input plugin   \n",
       "343495                                 Implement Async profiler with elastic APM   \n",
       "\n",
       "         topic  \n",
       "194038  194038  \n",
       "194051  194051  \n",
       "194052  194052  \n",
       "194058  194058  \n",
       "194074  194074  \n",
       "...        ...  \n",
       "343469  343469  \n",
       "343471  343471  \n",
       "343472  343472  \n",
       "343478  343478  \n",
       "343495  343495  \n",
       "\n",
       "[26153 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ed.DataFrame(client, es_index_pattern=index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use most familiar ways of interacting with data frames. See a comparison between pandas and Eland dataframes here: https://eland.readthedocs.io/en/v8.10.1/examples/demo_notebook.html#Compare-Eland-DataFrame-vs-pandas-DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_low = df[\"question.reads\"].quantile(0.01)\n",
    "q_hi  = df[\"question.reads\"].quantile(0.99)\n",
    "\n",
    "df_filtered = df[(df[\"question.reads\"] < q_hi) & (df[\"question.reads\"] > q_low)]\n",
    "\n",
    "q_low = df[\"solution.post_number\"].quantile(0.01)\n",
    "q_hi  = df[\"solution.post_number\"].quantile(0.99)\n",
    "\n",
    "df_filtered = df_filtered[(df_filtered[\"solution.post_number\"] < q_hi) & (df_filtered[\"solution.post_number\"] > q_low)]\n",
    "\n",
    "df_filtered.select_dtypes(include=np.number).hist(figsize=[10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eland Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use Eland to bring ML models into Elastic and use them on your data. \n",
    "\n",
    "See docs here: https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = df[[\"duration\", \"question.reads\", \"solution.post_number\", \"solution.reads\"]].to_pandas().iloc[:200]\n",
    "new = df[[\"duration\", \"question.reads\", \"solution.post_number\", \"solution.reads\"]].to_pandas().iloc[200:220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = df[[\"category_name\"]].to_pandas().iloc[0:220]\n",
    "encodings = pd.factorize(pd_df[\"category_name\"], use_na_sentinel=False)[0]\n",
    "\n",
    "training_target = pd_df.iloc[:200]\n",
    "training_target[\"encodings\"] = encodings[0:200]\n",
    "new_target = pd_df.iloc[200:220]\n",
    "new_target[\"encodings\"] = encodings[200:220]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        duration  question.reads  solution.post_number  solution.reads\n",
      "194038      1014               5                     3               5\n",
      "194051      1291              16                     4              12\n",
      "194052      8562              30                    16              11\n",
      "194058      1827             102                    21              48\n",
      "194074       281              36                     7              21\n",
      "...          ...             ...                   ...             ...\n",
      "205094        60               8                     2               7\n",
      "205106       363               6                     2               5\n",
      "205114       614              31                     2              27\n",
      "205116      1359              14                     4              13\n",
      "205119      7147              64                     6              33\n",
      "\n",
      "[200 rows x 4 columns]\n",
      "        category_name  encodings\n",
      "194038       Logstash          0\n",
      "194051         Kibana          1\n",
      "194052         Kibana          1\n",
      "194058           None          2\n",
      "194074       Logstash          0\n",
      "...               ...        ...\n",
      "205094       Logstash          0\n",
      "205106         Kibana          1\n",
      "205114  Elasticsearch          3\n",
      "205116         Kibana          1\n",
      "205119         Kibana          1\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(training)\n",
    "print(training_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can take a simple model from scikit-learn and use it on our numeric features. You can see the comparison between running the model in this python environment, or using the trained model directly from Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 0, 1, 1, 1, 6, 3, 1, 1, 3, 3, 4, 4, 3, 3, 1, 3, 3, 1])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from xgboost import XGBClassifier\n",
    "from eland.ml import MLModel\n",
    "\n",
    "# Train and exercise an XGBoost ML model locally\n",
    "training_data = datasets.make_classification(n_features=5)\n",
    "xgb_model = XGBClassifier(booster=\"gbtree\")\n",
    "xgb_model.fit(training.values, training_target[\"encodings\"].values)\n",
    "\n",
    "xgb_model.predict(new.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 0, 1, 1, 1, 6, 3, 1, 1, 3, 3, 4, 4, 3, 3, 1, 3, 3, 1])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model into Elasticsearch\n",
    "es_model = MLModel.import_model(\n",
    "    es_client=client,\n",
    "    model_id=\"xgb-classifier_bytes_0\",\n",
    "    model=xgb_model,\n",
    "    feature_names=[\"f0\", \"f1\", \"f2\", \"f3\", \"f4\"],\n",
    ")\n",
    "\n",
    "# Exercise the ML model in Elasticsearch with the training data\n",
    "es_model.predict(new.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eland For NLP + Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take a look at what we can do with our non-numeric features. There are a large number of supported models we support with Eland for various tasks like sentiment analysis, embeddings, or entity recognition. \n",
    "\n",
    "See full list of supproted models here: https://www.elastic.co/guide/en/machine-learning/current/ml-nlp-model-ref.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pd_df = df[['question.text', 'solution.text']].to_pandas()\n",
    "pd_df_clean = pd_df.applymap(lambda x: re.sub(\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});r'[^ \\w+]'\", '', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question.text</th>\n",
       "      <th>solution.text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194038</th>\n",
       "      <td>Hello,\\nI have a problem with my filter, i get...</td>\n",
       "      <td>A space was missing , that's why it wasn't wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194051</th>\n",
       "      <td>TL;DR\\nHow do I continuously update an alias t...</td>\n",
       "      <td>My workaround for this was to add the followin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194052</th>\n",
       "      <td>Hi there,\\nI am attempting to stream logs from...</td>\n",
       "      <td>Please try the following, tested locally in 6....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194058</th>\n",
       "      <td>\\n\\nThere is no pipeline plugin. Unlike other ...</td>\n",
       "      <td>Okay. I have solved the pipeline virtual addre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194074</th>\n",
       "      <td>Hi all,\\nI am using version 7.2.\\nI encountere...</td>\n",
       "      <td>Can you do the 'grep -rn ...' and verify that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343469</th>\n",
       "      <td>Hi,\\nI want to integrate the AWS logs into my ...</td>\n",
       "      <td>\\n\\n\\n rachelyang:\\n\\n10.0.9.12 46 96 2 1.51 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343471</th>\n",
       "      <td>I see that the saved objects API is deprecated...</td>\n",
       "      <td>Hello @emmanuel_t,\\nThis API should still work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343472</th>\n",
       "      <td>I have ECK setup and running on my kubernetes ...</td>\n",
       "      <td>It looks like part of my issue was a typo in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343478</th>\n",
       "      <td>I have a pipeline whereby the input section lo...</td>\n",
       "      <td>\\n\\n\\n mikec1:\\n\\nschedule =&amp;gt; \"0 0 0 * * ?\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343495</th>\n",
       "      <td>Could you help with setting up async profiler ...</td>\n",
       "      <td>Thanks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26153 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question.text  \\\n",
       "194038  Hello,\\nI have a problem with my filter, i get...   \n",
       "194051  TL;DR\\nHow do I continuously update an alias t...   \n",
       "194052  Hi there,\\nI am attempting to stream logs from...   \n",
       "194058  \\n\\nThere is no pipeline plugin. Unlike other ...   \n",
       "194074  Hi all,\\nI am using version 7.2.\\nI encountere...   \n",
       "...                                                   ...   \n",
       "343469  Hi,\\nI want to integrate the AWS logs into my ...   \n",
       "343471  I see that the saved objects API is deprecated...   \n",
       "343472  I have ECK setup and running on my kubernetes ...   \n",
       "343478  I have a pipeline whereby the input section lo...   \n",
       "343495  Could you help with setting up async profiler ...   \n",
       "\n",
       "                                            solution.text  \n",
       "194038  A space was missing , that's why it wasn't wor...  \n",
       "194051  My workaround for this was to add the followin...  \n",
       "194052  Please try the following, tested locally in 6....  \n",
       "194058  Okay. I have solved the pipeline virtual addre...  \n",
       "194074  Can you do the 'grep -rn ...' and verify that ...  \n",
       "...                                                   ...  \n",
       "343469  \\n\\n\\n rachelyang:\\n\\n10.0.9.12 46 96 2 1.51 2...  \n",
       "343471  Hello @emmanuel_t,\\nThis API should still work...  \n",
       "343472  It looks like part of my issue was a typo in t...  \n",
       "343478  \\n\\n\\n mikec1:\\n\\nschedule =&gt; \"0 0 0 * * ?\"...  \n",
       "343495                                             Thanks  \n",
       "\n",
       "[26153 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have cleaned the data and we will now get it into the right format to make calls to our inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text_field': 'Hello,\\nI have a problem with my filter, i get the \"_grokparsefailure\" tag when the concerned logs are processed.\\nHere is my filter file:\\nfilter {\\nif [source] == \"/var/log/auth.log\"{\\ngrok {\\nmatch =&gt;  [ \"message\", \"%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:[%{POSINT:pid}])?: %{GREEDYDATA:smt}: %{GREEDYDATA:smt2} for user %{USER:user}\" ]\\nadd_field =&gt; [ \"received_at\", \"%{@timestamp}\" ]\\nadd_field =&gt; [ \"received_from\", \"%{host}\" ]\\n}\\ndate {\\nmatch =&gt;  [ \"date\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ]\\n}\\n}\\n}\\nI tried it with the input: \"Aug  6 12:17:01 stack CRON[14336]: pam_unix(cron:session): session closed for user root\" on  http://grokdebug.herokuapp.com/  and it works fine.\\nIs there any problem with my code ?\\nThank you,\\nManal'},\n",
       " {'text_field': \"A space was missing , that's why it wasn't working. I fixed it. Thank you !\"},\n",
       " {'text_field': 'TL;DR\\nHow do I continuously update an alias to capture new matching index patterns?\\nLonger Version...\\nWe use a non-standard format that looks like this:\\nmyapp-beatname-version-YYYY.mm.dd\\n\\nI.e., a heartbeat index will look like:\\nmyapp-heartbeat-7.2.0-2019.08.06\\n\\nWhen using the Uptime module, the heartbeat documentation notes that the Uptime module matches the index pattern heartbeat-7* and suggests using an alias if the index format doesn\\'t match that. However, the alias documentation makes it clear that the following alias specification:\\n{\\n    \"actions\" : [\\n        { \"add\" : { \"index\" : \"*-heartbeat-*\", \"alias\" : \"heartbeat-7.x.x\" } }\\n    ]\\n}\\n\\nHowever, the index alias documentation states that:\\n\\nIn this case, the alias is a point-in-time alias that will group all current indices that match, it will not automatically update as new indices that match this pattern are added/removed.\\n\\nObviously, this does not work well with the Uptime module, where the alias must continuously update and capture newly-created aliases.\\nI\\'m certain there is a way around this, but I\\'m still enough of a novice in the ELK stack that I don\\'t see it and haven\\'t been able to find it in my searches.'},\n",
       " {'text_field': 'My workaround for this was to add the following to the template:\\n\"aliases\": {\\n  \"heartbeat-7.3.0-alias\": {}\\n},\\n\\nThis has resolved the issue. I already use a script to upload templates to Elasticsearch. This script also edits the templates on the fly to add an additional index pattern match in order to apply it to our alternate index-name convention, so I should be able to update it to add the alias on the fly as well.'},\n",
       " {'text_field': \"Hi there,\\nI am attempting to stream logs from AWS Lambda into Elasticsearch/Kibana.\\nI am doing so via a POST to the Elasticsearch endpoint, + the index and document type (e.g. POST https://example-es-domain.com/alblogs/alb-access-logs/).\\nI have verified that the lambda is sending out the request by sending to requestbin, and have verified that the information contained in each doc is correct.\\nHowever I can't see any of the information sent to Elasticsearch being loaded into Kibana, and was wondering if I was missing something.\\nThe full flow for the operation is:\\nALB -&gt; S3 -&gt; AWS Lambda -&gt; Elasticsearch\"},\n",
       " {'text_field': 'Please try the following, tested locally in 6.8, I\\'ve used you index + type naming, which is in your case identical.\\nPUT a pattern for you index\\nPUT alb-access-logs\\n{\\n  \"mappings\": {\\n    \"alb-access-logs\": {\\n      \"properties\": {\\n        \"timestamp\": {\\n          \"type\": \"date\"\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nPOST an example\\nPOST alb-access-logs/alb-access-logs\\n {\\n    \"timestamp\": \"2019-08-06T15:54:46.701974Z\"\\n }\\n\\nLet\\'s check the results:\\n GET alb-access-logs/_search \\n{\\n  \"sort\" : [\\n      {\"timestamp\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\\n   ]\\n}\\n\\nthis should work'},\n",
       " {'text_field': '\\n\\nThere is no pipeline plugin. Unlike other inputs it is implemented in the logstash core.\\n\\n\\nYes, you can keep using files, no need to use config.string. config.string is simpler to use in the documentation.\\n\\n\\nNo, weblogs is not the pipeline id, it is the virtual address of the pipeline input. The pipeline can be called anything.\\n\\n'},\n",
       " {'text_field': 'Okay. I have solved the pipeline virtual address problem. The name cannot contain - character, so renaming it to \"legacyva\" resolved my problem.\\nThere is some weirdness with yaml that I don\\'t quite understand. As you have described, putting quotes around the [type] variable causes the pipeline to fail to load. Also, setting the type in filebeat to anything other than log causes filebeat to be unable to start. I\\'m thinking it may be expecting certain inputs, but in any case I cannot seem to name this what I want, with or without quotes.\\nSo I really need \"tags\" or some other construct in order to perform the loop properly, and I must put quotes and must not put dashes. I am concerned that using \"tags\" affects the logic, as I am only looking for a single flag match, and == I believe means matches exactly. I\\'ll keep testing to see if I can work out this last piece.\\nedit&gt; Did some searching on \"logstash tags conditionals\" and found that the syntax is different. Not sure where this is documented but I copied a gentleman\\'s example and changed from if [tags] == \"tagname\" to if \"tagname\" in [tags]. I do feel better about this, even if I don\\'t understand it, as that == was really bothering me since it needed to be more of a \"contains\" rather than \"is equal to\" (if I\\'m remembering things correctly). Anyways, glad I learned something, and now my conditional statement is catching my file based on a tag! HOORAY!\\nBig thanks to @Badger, I kept trying to drive off the course and you definitely got me back on path more than once!\\nFor anyone else who is trying to do specifically this thing in the future, that is to say ship many different files with filebeat to a single logstash listener, then separate the pipelines by some pre-defined value so that many filters can be used and some modularity is provided for the index choice, here is the base template you need for your pipeline.conf files:\\nmaster-pipeline.conf\\ninput {\\n  beats {\\n    port =&gt; \"5000\"\\n  }\\n}\\noutput {\\n### conditional statement to separate logs by tag ###\\n  if \"primary\" in [tags] {\\n    pipeline { send_to =&gt; primaryvirtualaddress }\\n  } else {\\n    pipeline { send_to =&gt; fallbackvirtualaddress }\\n  }\\n}\\n\\neach subsequent pipeline:\\nprimary-pipeline.conf\\ninput {\\n  pipeline {\\n    address =&gt; \"primaryvirtualaddress\"\\n  }\\n}\\noutput {\\n  elasticsearch {\\n    hosts =&gt; [\"ES CLUSTER ADDRESS\"]\\n    index =&gt; \"primary_index\"\\n  }\\n}\\n\\nYou may be able to remove the quotes from the address, but I\\'m quite sick of re-testing at this point and I know this template works '},\n",
       " {'text_field': 'Hi all,\\nI am using version 7.2.\\nI encountered some trouble while using the filter elasticsearch plugin.\\nThe behavior is very random, some of the data uploaded correctly but I also get some error\\nHere is the log from logstash\\n\\n[2019-08-06T10:07:02,177][WARN ][logstash.filters.elasticsearch] Failed to query elasticsearch for previous event {:index=&gt;\"mapping_device_sn\", :error=&gt;\"[400] {\"error\":{\"root_cause\":[{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"mapping_device_sn\",\"node\":\"kuAYIY0yTlCZM7Du464gYg\",\"reason\":{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Cannot parse \\'sn:%{[device_sn]}\\': Encountered \\\\\" \\\\\"]\\\\\" \\\\\"] \\\\\"\\\\\" at line 1, column 15.\\\\nWas expecting:\\\\n    \\\\\"TO\\\\\" ...\\\\n    \",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Encountered \\\\\" \\\\\"]\\\\\" \\\\\"] \\\\\"\\\\\" at line 1, column 15.\\\\nWas expecting:\\\\n    \\\\\"TO\\\\\" ...\\\\n    \"}}}}]},\"status\":400}\"}\\n\\nLog from elasticsearch\\n[elasticsearch.server][DEBUG] All shards failed for phase: [query]\\n\\n[elasticsearch.server][DEBUG] [0], node[kuAYIY0yTlCZM7Du464gYg], [P], s[STARTED], a[id=NKeTm6rjTgCj_zwuFPyacA]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[mapping_device_sn], indicesOptions=IndicesOptions[ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_aliases_to_multiple_indices=true, forbid_closed_indices=true, ignore_aliases=false, ignore_throttled=true], types=[], routing=\\'null\\', preference=\\'null\\', requestCache=null, scroll=null, maxConcurrentShardRequests=0, batchedReduceSize=512, preFilterShardSize=128, allowPartialSearchResults=true, localClusterAlias=null, getOrCreateAbsoluteStartMillis=-1, ccsMinimizeRoundtrips=true, source={\"size\":1,\"query\":{\"query_string\":{\"query\":\"sn:%{[device_sn]}\",\"fields\":[],\"type\":\"best_fields\",\"default_operator\":\"or\",\"max_determinized_states\":10000,\"enable_position_increments\":true,\"fuzziness\":\"AUTO\",\"fuzzy_prefix_length\":0,\"fuzzy_max_expansions\":50,\"phrase_slop\":0,\"analyze_wildcard\":false,\"escape\":false,\"auto_generate_synonyms_phrase_query\":true,\"fuzzy_transpositions\":true,\"boost\":1.0}},\"sort\":[{\"@timestamp\":{\"order\":\"desc\"}}]}}]\\n\\nMy logstash config\\ninput {\\n    file {\\n        path =&gt; \"/data/ELK_raw/IPS/data/*/ips_aggregate.csv\"\\n        sincedb_path =&gt; \"/dev/null\"\\n        mode =&gt; \"read\"\\n        file_completed_action =&gt; \"log\"\\n        file_completed_log_path =&gt; \"/data/ELK/read_log/ips_read_log.txt\"\\n        type =&gt; \"ips\"\\n    }\\n}\\n\\nfilter {\\n    csv {\\n        autodetect_column_names =&gt; \"true\"\\n        autogenerate_column_names =&gt; \"true\"\\n        skip_header =&gt; \"true\"\\n        separator =&gt; \",\"\\n    }\\nelasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_ips\"\\n        query =&gt; \"id:%{[id]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \" signature_name\" =&gt; \"signature_name\"\\n            \" engine_rule\" =&gt; \"engine_rule\"\\n        }\\n    }\\n\\n   elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_device_sn\"\\n        query =&gt; \"sn:%{[device_sn]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \"first_industry\" =&gt; \"first_industry\"\\n            \"customer\" =&gt; \"customer\"\\n            \"is_trial\" =&gt; \"is_trial\"\\n            \"product_type\" =&gt; \"product_type\"\\n            \"second_industry\" =&gt; \"second_industry\"\\n            \"warranty_date\" =&gt; \"warranty_date\"\\n        }\\n    }\\n\\n   mutate {\\n        remove_field =&gt; [ \"@timestamp\" ]\\n        remove_field =&gt; [ \"@version\" ]\\n        remove_field =&gt; [ \"host\" ]\\n        remove_field =&gt; [ \"message\" ]\\n        remove_field =&gt; [ \"path\" ] \\n        remove_field =&gt; [ \"type\" ] \\n    }   \\n}\\n\\noutput {\\n    elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"cv_ips\"\\n    }\\n}\\n\\nHave anyone encountered this before?\\nThanks'},\n",
       " {'text_field': \"Can you do the 'grep -rn ...' and verify that the line number is always 1?\\nYou do have --pipeline.workers 1 set, right?\"},\n",
       " {'text_field': 'Hi all,\\nI\\'m testing the Kibana Maps feature (7.3 version) and it\\'s really great, kudos! However, as I intend to work with a huge (about billions of documents) index, I could not help but notice that a message saying \"Results limited to first 10000 documents\" is shown when I hover the mouse over each layer header.\\nI would like to configure this limit... Is it possible?\\nI tried to increase the value of the \"index.max_result_window\" setting, to no avail.\\nThanks in advance.'},\n",
       " {'text_field': 'It looks like this setting is not based on the index.max_result_window setting, as you can see in the constants file in the new maps app: https://github.com/elastic/kibana/blob/master/x-pack/legacy/plugins/maps/common/constants.js#L41\\nThis is probably worth filing an issue on Github for.'},\n",
       " {'text_field': 'This is an extension to a previously solved topic.\\nI have this log here (as it appears in the console)\\n2019-08-05 08:55:15 - jcramer(Software Programmer) - Successfully updated profile\\nthat I\\'ve successfully been able to parse with this Grok configuration,\\ngrok { \\n    match =&gt; { \"message\" =&gt; \"^%{TIMESTAMP_ISO8601:syslog_timestamp} - \\n    %{USERNAME:syslog_username}\\\\((?&lt;syslog_userinfo&gt;[^)]*)\\\\)\\\\s+- \\n    %{GREEDYDATA:syslog_message}\" } \\n}\\n\\nhowever, I receive a _grokparsefailure when the username appears like this in the console:\\n2019-08-05 08:55:15 - STEM\\\\\\\\psmith(Hardware Engineer) - Successfully updated profile\\nThe acronym before the username is meant to represent something like a department or location. Somehow the inclusion of the backslash causes Grok to not recognize the pattern. I would like to parse the username so that the backslash and information prior to it are included in the username.\\nSo, the end result can look like this\\n\"syslog_username\" =&gt; \"jcramer\"\\n\\nor this\\n\"syslog_username\" =&gt; \"STEM\\\\\\\\psmith\"\\n'},\n",
       " {'text_field': 'Use a custom pattern with the first part optional\\n(?&lt;syslog_username&gt;([a-zA-Z0-9\\\\._-]+\\\\\\\\\\\\\\\\)?[a-zA-Z0-9\\\\._-]+)\\\\((?&lt;syslog_userinfo&gt;[^)]*)\\\\)'},\n",
       " {'text_field': 'I am running into an issue using the external versioning where deleting the index does not seem to clear the external versioning for that given index.\\nSteps to recreate:\\n\\nCreate an index\\nPut an item with\\n\\nid: \\'someid\\',\\nversion: 0,\\nversion_type: \\'external\\',\\n\\ninto the index\\n\\ndelete the index\\nWait arbitrarily long\\nCreate the same index\\nPut the same item with\\n\\nid: \\'someid\\',\\nversion: 0,\\nversion_type: \\'external\\',\\n\\ninto the index and I get a version_conflict_engine_exception\\n\"name\": \"ResponseError\",\\n  \"meta\": {\\n    \"body\": {\\n      \"error\": {\\n        \"root_cause\": [\\n          {\\n            \"type\": \"version_conflict_engine_exception\",\\n            \"reason\": \"[contact_v1][someid]: version conflict, current version [1] is higher or equal to the one provided [1]\",\\n            \"index_uuid\": \"PvmFtcibTfmBCu8C7iQ43Q\",\\n            \"shard\": \"0\",\\n            \"index\": \"stpze-contacts-3\"\\n          }\\n        ],\\n        \"type\": \"version_conflict_engine_exception\",\\n        \"reason\": \"[contact_v1][someid]: version conflict, current version [1] is higher or equal to the one provided [1]\",\\n        \"index_uuid\": \"PvmFtcibTfmBCu8C7iQ43Q\",\\n        \"shard\": \"0\",\\n        \"index\": \"stpze-contacts-3\"\\n      },\\n      \"status\": 409\\n    },\\n    \"statusCode\": 409,\\n\\nMy elastic cluster is running in a docker container\\n{\\n  \"name\": \"0_cod3F\",\\n  \"cluster_name\": \"docker-cluster\",\\n  \"cluster_uuid\": \"Cvy3FFwdTuuwXHG8S6ws6A\",\\n  \"version\": {\\n    \"number\": \"6.4.3\",\\n    \"build_flavor\": \"default\",\\n    \"build_type\": \"tar\",\\n    \"build_hash\": \"fe40335\",\\n    \"build_date\": \"2018-10-30T23:17:19.084789Z\",\\n    \"build_snapshot\": false,\\n    \"lucene_version\": \"7.4.0\",\\n    \"minimum_wire_compatibility_version\": \"5.6.0\",\\n    \"minimum_index_compatibility_version\": \"5.0.0\"\\n  },\\n  \"tagline\": \"You Know, for Search\"\\n}\\n\\nWhen printing all entries in the index it returns an empty list\\n{\\n    \"took\": 1,\\n    \"timed_out\": false,\\n    \"_shards\": {\\n        \"total\": 3,\\n        \"successful\": 3,\\n        \"skipped\": 0,\\n        \"failed\": 0\\n    },\\n    \"hits\": {\\n        \"total\": 0,\\n        \"max_score\": null,\\n        \"hits\": []\\n    }\\n}\\n\\nIs there some extra step involved in deleting the external verisoning metadata?'},\n",
       " {'text_field': 'Ok yes deleting an externally-versioned document from an index that does not exist will actually create the index in order to record the deletion:\\nGET /_cat/indices\\n\\n# 200 OK\\n\\nDELETE /i/_doc/someid?version=0&amp;version_type=external\\n\\n# 404 Not Found\\n# {\\n#   \"_type\": \"_doc\",\\n#   \"_primary_term\": 1,\\n#   \"_id\": \"someid\",\\n#   \"_shards\": {\\n#     \"successful\": 1,\\n#     \"total\": 2,\\n#     \"failed\": 0\\n#   },\\n#   \"_index\": \"i\",\\n#   \"result\": \"not_found\",\\n#   \"_version\": 0,\\n#   \"_seq_no\": 0\\n# }\\n\\nGET /_cat/indices\\n\\n# 200 OK\\n# yellow open i Pqo2tMyMRyOTOXvFAUk0Tw 5 1 0 0 1.1kb 1.1kb\\n#\\n'},\n",
       " {'text_field': 'I am almost positive that this is a simple misunderstanding on my part as I\\'m very new to Elasticsearch.  I am trying to implement a substring matching search using ngrams.  I\\'ve got it set as max_ngram_diff of 10. My setup looks something like:\\n  \"filter\": {\\n      \"barcode_filter\": {\\n        \"type\": \"nGram\",\\n        \"min_gram\": \"4\",\\n        \"max_gram\": \"14\"\\n      }\\n    },\\n    \"analyzer\": {\\n      \"barcode_filter_analyzer\": {\\n        \"filter\": [\\n          \"lowercase\",\\n          \"barcode_filter\"\\n        ],\\n        \"type\": \"custom\",\\n        \"tokenizer\": \"standard\"\\n      }\\n    }\\n\\nMy field is defined as barcode with analyzer set to barcode_filter_analyzer.\\nMy goal is to be able to find substrings. So something like \"V741\"  should find every barcode with that substring.  Something like \"ZR000041\" should also find every barcode containing that entire substring.  While the second example will find every barcode containing the entire substring, it also finds every barcode containing \"0000\" for example. My initial thought was to use the score but that doesn\\'t seem to work as a barcode like 00000000000101 will end up having a very high score.\\nIs there any good way to require that the entire query string be found? This is currently only in testing on my PC so anything requiring scrapping and restarting the index is not a big deal so if I\\'m approaching this in entirely the wrong way I\\'m happy to adjust.  Any help at all would be greatly appreciated!'},\n",
       " {'text_field': 'To add to Alex\\' explanation here, one solution could be to provide a search analyzer that does not apply the ngram filter to the query terms. You could do that by configuring a search_analyzer in your mapping, or by providing an analyzer in your query:\\nGET test/_search \\n{\\n  \"query\": {\\n    \"match\": {\\n      \"my_field\": {\\n        \"query\": \"ZR000041\",\\n        \"analyzer\": \"standard\"\\n      }\\n    }\\n  }\\n}\\n'},\n",
       " {'text_field': 'I have successfully setup functionbeat on aws, but I am getting following license error, license manager stops and function stops:\\n2019-08-07T03:11:55.435Z\\tINFO\\t[functionbeat]\\tbeater/functionbeat.go:74\\tFunctionbeat is running\\n2019-08-07T03:11:55.435Z\\tINFO\\telasticsearch/client.go:170\\tElasticsearch url: http://10.56.0.100:9200\\n2019-08-07T03:11:55.435Z\\tINFO\\t[license-manager]\\tlicenser/manager.go:184\\tLicense manager started, retrieving initial license\\n2019-08-07T03:11:55.435Z\\tINFO\\t[functionbeat]\\tlicenser/manager.go:331\\tWaiting on synchronous license check\\n2019-08-07T03:11:55.435Z\\tINFO\\t[monitoring]\\tlog/log.go:118\\tStarting metrics logging every 30s\\n2019-08-07T03:11:55.987Z\\tINFO\\telasticsearch/client.go:743\\tAttempting to connect to Elasticsearch version 7.3.0\\n2019-08-07T03:11:56.026Z\\tINFO\\t[license-manager]\\tlicenser/manager.go:282\\tValid license retrieved\\t{\"license mode\": \"Open source\", \"type\": \"Open source\", \"status\": \"Active\"}\\n2019-08-07T03:11:56.026Z\\tINFO\\t[functionbeat]\\tlicenser/check.go:35\\tLicense is active for Basic\\n2019-08-07T03:11:56.026Z\\tINFO\\t[functionbeat]\\tlicenser/manager.go:339\\tLicense is valid, mode: Open source\\n2019-08-07T03:11:56.026Z\\tINFO\\t[license-manager]\\tlicenser/manager.go:211\\tLicense manager stopped\\n2019-08-07T03:11:56.026Z\\tINFO\\t[functionbeat]\\tbeater/functionbeat.go:86\\tFunctionbeat stopped running\\n2019-08-07T03:11:56.028Z\\tINFO\\t[monitoring]\\tlog/log.go:154\\tUptime: 639.743036ms\\n2019-08-07T03:11:56.028Z\\tINFO\\t[monitoring]\\tlog/log.go:131\\tStopping metrics logging.\\n2019-08-07T03:11:56.028Z\\tINFO\\tinstance/beat.go:431\\tfunctionbeat stopped.\\n2019-08-07T03:11:56.028Z\\tERROR\\tinstance/beat.go:877\\tExiting: invalid license\\nExiting: invalid license\\nWill appreciate quick response'},\n",
       " {'text_field': 'You are trying to connect to an Elasticsearch with OSS license. You need at least Basic license in ES for Functionbeat to run. You can download ES with basic license here: https://www.elastic.co/subscriptions'},\n",
       " {'text_field': 'Guys, when I use logstash to input syslog and keep the message to file, I found it seems logstash won\\'t keep syslog header.\\nmy configuration looks like this:\\ninput{\\n     syslog {\\n        port =&gt; 1522\\n     }\\n}\\noutput{\\n    file {\\n        path =&gt; \"/log/proxy/%{+yyyyMMdd}/%{host}/%{+HH}.log\"\\n        codec =&gt; line { format =&gt; \"%{message}\" }\\n    }\\n}\\n\\nand the raw data I captured via tcpdump looks like this:\\n..........      &lt;30&gt;Aug  7 08:47:12 blrmwg01 mwg: CEF:0|McAfee|Web Gateway|7.7.2.5.0|200|Proxy-Enable Web Cache|2|rt=Aug 07 2019 08:47:12 cat=Access Log dst=x ...\\n\\nbut when I checked the message from file, it looks like this:\\nCEF:0|McAfee|Web Gateway|7.7.2.5.0|200|Proxy-Enable Web Cache|2|rt=Aug 07 2019 08:47:12 cat=Access Log dst=x ...\\n\\nin brief, the syslog header is missing. So is it possible to keep the syslog header and save the whole message to file?'},\n",
       " {'text_field': 'Not a grok filter, but the grok_pattern option on the filter. Note the warning there about parsing the timestamp. You will have to add a grok that gets the timestamp out of the message (you can use the same grok pattern that the syslog filter defaults to) and remove the tag that the syslog filter adds.'},\n",
       " {'text_field': 'Hi All,\\nI don\\'t understand what\\'s wrong in below conf file.\\ninput {\\njdbc {\\njdbc_driver_library =&gt; \"/usr/share/logstash/myjars/mysql-connector-java-8.0.11.jar\"\\njdbc_driver_class =&gt; \"com.mysql.cj.jdbc.Driver\"\\njdbc_connection_string =&gt; \"jdbc:mysql://10.0.1.133:5500/alumini_factor\"\\njdbc_user =&gt; \"root\"\\njdbc_password =&gt; \"********\"\\nstatement =&gt; \"select * FROM profile\"\\nuse_column_value =&gt; true\\ntracking_column =&gt; id\\n}\\n}\\noutput {\\nelasticsearch {\\ndocument_id =&gt; \"%{id}\"\\nhosts =&gt; [\"http://localhost:9200\"]\\nindex =&gt; \"profiles\"\\ndocument_type =&gt; \"data\"\\n}\\n}\\nP.S:\\nElasticsearch version 7.0.1\\nLogstash version 7.3\\nMysql verion 5.7.26-0ubuntu0.16.04.1\\nThanks,\\nManideep'},\n",
       " {'text_field': 'Hi @guyboertje\\nThanks for the response.\\nI figured out the  error.\\nAs I am new to Linux, I did not understand the Logstash installation process.\\ni tried installing Logstash using APT given in the Logstash Docs.\\nLogstash was installed at /usr/share/logstash but the config folder was missing inside the Logstash.\\nSo, I externally copied the config folder inside the Logstash and tried to run it. It did not work.\\nSometime later, i realized that config folder from original installation was in /etc/config.\\nSo, I have two config folders one inside the Logstash and another in /etc/. (I am really not sure if this is the issue.)\\nThen, I deleted everything and downloaded tat.gz file and extracted to the location /usr/share/.\\nIt now has config folder. I ran the logstash with my conf file and it worked.\\nThanks,\\nManideep'},\n",
       " {'text_field': 'I\\'m looking at setting up Elasticsearch within Docker and am trying to figure out the best method for inserting new data. Currently, after running docker-compose up, I run a series of curl commands to set up the index and bulk import data:\\ncurl -XDELETE http://localhost:9200/my_index\\ncurl -XPUT http://localhost:9200/my_index -H \\'Content-Type: application/json\\' -d@/index.json\\ncurl -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/_bulk --data-binary \"@/bulk_import.json\"\\ncurl -XPOST \\'localhost:9200/my_index/_refresh\\'\\n\\nThis works fine, but I have to run these commands every time I restart the container (because the index and data are gone with it). I\\'m definitely missing something hopefully obvious, so would love if someone could point me in the right direction here.\\nEdit: My docker-compose.yml if it helps:\\nversion: \\'3\\'\\n\\nservices:\\n  es:\\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.1.1\\n    container_name: es\\n    environment:\\n      - discovery.type=single-node\\n    ports:\\n      - 9200:9200\\n      - 9300:9300\\n    volumes:\\n      - esdata1:/usr/share/elasticsearch/data\\n\\nvolumes:\\n    esdata1:\\n      driver: local'},\n",
       " {'text_field': \"Well, it depends how exactly you configure your named volumes in docker-compose. What docker-compose down -v deletes is, AFAIK, dockers metadata regarding the volumes, not (necessarily) the data itself. There are many solutions to the rather typical problem you are facing, but one easy solution is the following volumes configuration in your docker-compose file, assuming your elastic service uses esdata01 as the volume for data (ie. mounted to /usr/share/elasticsearch/data in the container):\\nvolumes:\\n  esdata01:\\n    driver: local\\n    driver_opts:\\n      type: none\\n      device: /reallybigdatadrive/elasticsearchdata/\\n      o: bind\\n\\nWhat this says is simply to bind an (already existing) directory (i'm assuming you are using *nix here, I have no idea how this works on MS windows) to the docker volume, which is then used by the elasticsearch service. Then, if you take down the volume, your data is still safe and sound in the directory /reallybigdatadrive/elasticsearchdata/, and can be refound by simply running docker-compose up\\nNB NB NB You mention that you want to let another machine continue work on the index. As long as you mount the data directory onto /reallybigdatadrive/elasticsearchdata/ on the second machine (or some other place, and modify the docker-compose file accordingly) this will work, however, you need to remember that disk IO to the index is usually a crucial bit for the performance of the cluster. Thus, you need to make certain that wherever you place the data is on a fast disk, and that you have plenty of network IO to and from the machines running the docker images.\\nAlso, do not start up two containers on two different machines, trying to use the same data for their indexes. You will be unhappy with the results. Honestly, the usual solution here, is to make certain that the nodes running the database (here: elasticsearch), have access to the data locally, and then communicate with elasticsearch through the REST interface or something similar - that practice is not confined to elasticsearch, it's generally a good principle from data mining...\"},\n",
       " {'text_field': '\\n  \\n    \\n    \\n    Watcher Index Actionで、ドキュメントを分けて出力する 日本語による質問・議論はこちら\\n  \\n  \\n    基礎的な質問で恐縮ですが、WatcherのIndex Actionを利用すると、デフォルトでは、集計結果が、一つのドキュメントにまとめられて出力されます。 \\nこれを、Watcherで引っかかったドキュメントが、そのまま別々の状態で出力されるように設定をしたいです。 \\nたとえば、 \\n①下記のような３つのデータを用意 \\n\\n*doc1 \\npc_name : A \\nfeature    : B \\n*doc2 \\npc_name : A \\nfeature    : C \\n*doc3 \\npc_name : B \\nfeature    : B \\n\\n②Watcherで、FeatureにBが含まれるときに、Indexを作成するように設定 \\n③doc1とdoc3が、一つのインデックスに、別々のドキュメントして出力 \\nという流れをつくりたいです \\n初歩的なミスなのかもしれませんが、、下記の通り、Transform句を使用して作成してみたのですが、「no requests added」というエラーが出てしまいました。 \\n{\\n  \"trigger\": {\\n    \"schedule\": {\\n      \"inte…\\n  \\n\\n\\n以前こちらで質問させていただき、無事にWatcher使用していたのですが、\\n一度ライセンスが切れ、今月再度ライセンスを更新して再びWatcherを動かしたところ、挙動がおかしくなりました。\\n\\n{\\n\"script\": {\\n\"script\": {\\n\"lang\": \"painless\",\\n\"source\": \"(doc[\\'login_date\\'].date.getMillis()/1000 - doc[\\'start_date\\'].date.getMillis()/1000) &gt; 172800\"\\n}\\n}\\n}\\n\\nこちらの指定が効いておらず、この条件に合致しないものがIndexに出力されてしまいます。\\nしかしなぜかAggregation部分を消すと、上記は効くようです。。\\n原因の特定がなかなかできず、教えていただけたら幸いです。'},\n",
       " {'text_field': 'ありがとうございます。\\nWatcher自体は変更を加えておりません\\n検証を続けたところ、どうやらデータ起因っぽそうです。\\nデータ修正して、なんとかできそうです。\\nありがとうございました。'},\n",
       " {'text_field': 'Hi All,\\nI am understanding the process of indexing. I have read this question and answer:\\nhttps://stackoverflow.com/questions/38248757/which-elasticsearch-node-is-better-configured-in-logstash-elasticsearch-output-p\\nWhen I use Logstash, I define the index for each document in the output.\\nBut why should we specify the data node of Elasticsearch cluster for the output in Logstash but not specify the master node which is specialized for indexing? Or is that the indexing is already done in Logstash so we can directly send them to data node?\\nShould the flow be like this?:\\nLogstash filtering and indexing -&gt; ES data node, or\\nLogstash filtering -&gt; ES master node indexing -&gt; ES data node for storage\\nThank you in advance'},\n",
       " {'text_field': 'When a new index is created this is done by the master node which distributes the shards across the nodes in the cluster. The data nodes receive bulk requests from Logstash and indexes this data into the shards they hold. The master node manages changes to the cluster state, e.g. moving shards in case of failure, but does not get involved in the processing of bulk requests or queries. It also monitors the state of the nodes in the cluster so failure can be detencted as quickly as possible, which is why it should not get overloaded.'},\n",
       " {'text_field': 'Hi,\\nES newbie here. Is it possible, say in an ingest pipeline, to store the document into one index and then take some of the fields and update a second index. For example an order document would be stored and a customer document in another index would have its order count and last_order date updated?\\nOr store some of the document fields in one index and other fields in a second index effectively creating 2 documents from one?'},\n",
       " {'text_field': 'hey,\\nan ingest pipeline can change a document, but in the end it will always only index a single document. I think it makes more sense to index two documents, using the bulk API, one and index operation and one an update operation.\\nAnother way could be to trigger a reindex operation to regularly query for changes in the last n minutes, which uses a script to only store parts of the documents in the other index.\\nMaybe (just guessing though) the new data frames feature might be worth a look as well.\\n--Alex'},\n",
       " {'text_field': 'I am running a two node ES cluster and a single Kibana instance using Docker Swarm,\\neverything worked fine before enabling SSL/TLS on the ES nodes.\\nI am using same certificate without hostname verification for all the ES nodes and everything works perfect for the ES nodes. The only problem is that Kibana is not able to connect to ES cluster.\\nI am using the same certificate for all the ES instances and for Kibana too.\\nFollowing is my docker-compose.yml-\\nversion: \"3.2\"\\nservices:\\n  elasticsearch:\\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0\\n    restart: always\\n    environment:\\n      - node.name={{.Node.Hostname}}\\n      - discovery.seed_hosts=elasticsearch\\n      - cluster.name=docker-cluster\\n      - cluster.initial_master_nodes=node1,node2\\n      - network.host=0.0.0.0\\n      - bootstrap.memory_lock=true\\n      - \"ES_JAVA_OPTS=-Xms3g -Xmx3g\"\\n      - \"ELASTIC_PASSWORD=myespassword\"\\n      - xpack.license.self_generated.type=basic\\n      - xpack.security.enabled=true\\n      - xpack.security.http.ssl.enabled=true\\n      - xpack.security.http.ssl.key=$CERTS_DIR/elasticsearch/elasticsearch.key\\n      - xpack.security.http.ssl.certificate_authorities=$CERTS_DIR/ca/ca.crt\\n      - xpack.security.http.ssl.certificate=$CERTS_DIR/elasticsearch/elasticsearch.crt\\n      - xpack.security.transport.ssl.enabled=true\\n      - xpack.security.transport.ssl.verification_mode=certificate\\n      - xpack.security.transport.ssl.certificate_authorities=$CERTS_DIR/ca/ca.crt\\n      - xpack.security.transport.ssl.certificate=$CERTS_DIR/elasticsearch/elasticsearch.crt\\n      - xpack.security.transport.ssl.key=$CERTS_DIR/elasticsearch/elasticsearch.key\\n    secrets:\\n      - source: es_ssl_key\\n        target: $CERTS_DIR/elasticsearch/elasticsearch.key\\n      - source: es_ssl_ca\\n        target: $CERTS_DIR/ca/ca.crt\\n      - source: es_ssl_crt\\n        target: $CERTS_DIR/elasticsearch/elasticsearch.crt\\n    ulimits:\\n      memlock:\\n        soft: -1\\n        hard: -1\\n    deploy:\\n      mode: replicated\\n      replicas: 1\\n    ports:\\n      - \"9200:9200\"\\n      - \"9300:9300\"\\n    volumes:\\n      - es_data:/usr/share/elasticsearch/data\\n\\n  kibana:\\n    image: docker.elastic.co/kibana/kibana:7.2.0\\n    restart: always\\n    environment:\\n      - \"SERVER_NAME=kibana\"\\n      - \"ELASTICSEARCH_HOSTS=https://elasticsearch:9200\"\\n      - \"XPACK_SECURITY_ENABLED=true\"\\n      - \"ELASTICSEARCH_USERNAME=kibana\"\\n      - \"ELASTICSEARCH_PASSWORD=mykbpassword\"\\n      - elasticsearch.ssl.certificateAuthorities=$CERTS_DIR/ca/ca.crt\\n      - elasticsearch.ssl.verificationMode=certificate\\n    secrets:\\n      - source: es_ssl_ca\\n        target: $CERTS_DIR/ca/ca.crt\\n    ports:\\n      - \"5601:5601\"\\n    depends_on:\\n      - elasticsearch\\n\\nsecrets:\\n  es_ssl_key:\\n    file: ./es_certs/elasticsearch/elasticsearch.key\\n  es_ssl_ca:\\n    file: ./es_certs/ca/ca.crt\\n  es_ssl_crt:\\n    file: ./es_certs/elasticsearch/elasticsearch.crt\\n\\nvolumes:\\n  es_data:\\n\\nI am using same certificate for all the ES nodes and Kibana generated using the following\\ncreate-certs.yml-\\nversion: \"2.2\"\\nservices:\\n  create_certs:\\n    container_name: create_certs\\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0\\n    command: &gt;\\n      bash -c \\'\\n        yum install -y -q -e 0 unzip;\\n        if [[ ! -f /certs/bundle.zip ]]; then\\n          bin/elasticsearch-certutil cert --silent --pem --in config/certificates/instances.yml -out /certs/bundle.zip;\\n          unzip /certs/bundle.zip -d /certs;\\n        fi;\\n        chown -R 1000:0 /certs  \\'\\n    user: \"0\"\\n    working_dir: /usr/share/elasticsearch\\n    volumes:\\n      - ./es_certs:/certs\\n      - .:/usr/share/elasticsearch/config/certificates\\n\\ninstances.yml\\ninstances:\\n  - name: elasticsearch\\n    dns:\\n      - elasticsearch\\n      - localhost\\n    ip:\\n      - 127.0.0.1\\n\\nI am able to successfully perform a curl call from inside of the Kibana container to the https Elasticsearch endpoint using the following command-\\ncurl --cacert ca.crt -u kibana:mykbpassword https://elasticsearch:9200\\nHowever normally Kibana cannot connect to the ES nodes and I receive the following logs-\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"error\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"Request error, retrying\\\\nGET https://elasticsearch:9200/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip =&gt; connect ECONNREFUSED 10.0.3.2:9200\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"Unable to revive connection: https://elasticsearch:9200/\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"No living connections\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"status\",\"plugin:xpack_main@7.2.0\",\"error\"],\"pid\":1,\"state\":\"red\",\"message\":\"Status changed from yellow to red - No Living connections\",\"prevState\":\"yellow\",\"prevMsg\":\"Waiting for Elasticsearch\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"status\",\"plugin:graph@7.2.0\",\"error\"],\"pid\":1,\"state\":\"red\",\"message\":\"Status changed from yellow to red - No Living connections\",\"prevState\":\"yellow\",\"prevMsg\":\"Waiting for Elasticsearch\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"status\",\"plugin:elasticsearch@7.2.0\",\"error\"],\"pid\":1,\"state\":\"red\",\"message\":\"Status changed from yellow to red - No Living connections\",\"prevState\":\"yellow\",\"prevMsg\":\"Waiting for Elasticsearch\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"error\",\"elasticsearch\",\"data\"],\"pid\":1,\"message\":\"Request error, retrying\\\\nGET https://elasticsearch:9200/_xpack =&gt; connect ECONNREFUSED 10.0.3.2:9200\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"warning\",\"elasticsearch\",\"data\"],\"pid\":1,\"message\":\"Unable to revive connection: https://elasticsearch:9200/\"}\\nes_stack_kibana.1.bl9aozgf1b0z@node1    | {\"type\":\"log\",\"@timestamp\":\"2019-08-07T09:46:12Z\",\"tags\":[\"warning\",\"elasticsearch\",\"data\"],\"pid\":1,\"message\":\"No living connections\"}'},\n",
       " {'text_field': '\\n\\n\\n saifat29:\\n\\nkibana:\\n  image: docker.elastic.co/kibana/kibana:7.2.0\\n  restart: always\\n  environment:\\n    - \"SERVER_NAME=kibana\" \\n    - \"ELASTICSEARCH_HOSTS=https://elasticsearch:9200\"\\n    - \"XPACK_SECURITY_ENABLED=true\"\\n    - \"ELASTICSEARCH_USERNAME=kibana\"\\n    - \"ELASTICSEARCH_PASSWORD=mykbpassword\"\\n    - elasticsearch.ssl.certificateAuthorities=$CERTS_DIR/ca/ca.crt\\n    - elasticsearch.ssl.verificationMode=certificate\\n\\n\\n\\nI believe your problem here is that you are trying to pass environment variables using a settings syntax.\\nFrom: Running Kibana on Docker | Kibana Guide [7.2] | Elastic\\n\\nFor compatibility with container orchestration systems, these environment variables are written in all capitals, with underscores as word separators. The helper translates these names to valid Kibana setting names.\\n\\nYour first 5 variables are in the correct format, but the last 2 have not been converted.'},\n",
       " {'text_field': 'Hi Team,\\nI have installed Elasticsearch 7.3.0 on Linux server and created one master node. On another server(Linux), i have installed Same Elasticsearch version and made it as data node. Also I have passed required configuration for connecting these servers. Everything is working fine.\\nNow I installed Kibana on Linux server (of master node) using below command -\\nwget https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.2.deb\\nand then\\nsudo dpkg -i kibana-7.3.0-amd64.deb\\nAnd edited below kibana.yml file -\\nserver.host: \"\"// my IP of the current server\\nelasticsearch.hosts: [\"\"] // URL of elasticsearch master node(which is on same server)\\n#server.port: 5601 // using default port.\\nNow when I am trying to access Kibana using http://myserverip:5601 then its not working and showing \"This site can\\'t be reached\".\\nCan anyone guide me what I am doing wrong or any step I am missing?\\nIf  any more detail needed, please let me know.'},\n",
       " {'text_field': 'I got the solution for this problem.\\nI was making mistake that on Linux, I need to uncomment port: 5601 (default port) from yml file and it works fine. This topic can be closed now.'},\n",
       " {'text_field': 'STATUS: Updating ...\\nHi everyone, i believe that have many people want to change logo kibana to owner company logo, like with me. After time  for searching and reading source code i found solution to change Logo, Favicon, Welcome logo login, Header logo, SelectSpace logo and Loading image\\n\\nLogin logo (welcome screen)\\nChange login in Select Space\\nChange login in Header\\nChange loading screen\\nChange favicon\\n\\n1. Login logo (welcome screen)\\nRequire : Logo must have size 80*80 pixel\\nOpen file login_page.js and remove original svg logo\\n\\n/usr/share/kibana/node_modules/x-pack/plugins/security/public/views/login/components/login_page/login_page.js\\n\\nInline remove line 54 change line 53 to\\nreact_1.default.createElement(\"span\", { className: \"loginWelcome__logo\" }),\\n\\nIt look like\\nreact_1.default.createElement(\"span\", { className: \"loginWelcome__logo\" }),\\n      react_1.default.createElement(eui_1.EuiTitle, { size: \"l\", className: \"loginWelcome__title\" },\\n\\nNext, we need edit css. Open file index.dark.css and index.light.css. Find class .loginWelcome__logo {}\\nRemove background and add background-image\\nbackground-image: url(url_your_logo_company);\\n\\nIt look like\\n.loginWelcome__logo {\\n  display: inline-block;\\n  width: 80px;\\n  height: 80px;\\n  line-height: 80px;\\n  text-align: center;\\n  background-image: url(https://i.imgur.com/OJVk1hn.png);\\n  border-radius: 100%;\\n  padding: 16px;\\n  -webkit-box-shadow: 0 6px 12px -1px rgba(0, 0, 0, 0.2), 0 4px 4px -1px rgba(0, 0, 0, 0.2), 0 2px 2px 0 rgba(0, 0, 0, 0.2);\\n          box-shadow: 0 6px 12px -1px rgba(0, 0, 0, 0.2), 0 4px 4px -1px rgba(0, 0, 0, 0.2), 0 2px 2px 0 rgba(0, 0, 0, 0.2);\\n  margin-bottom: 32px; }\\n  .loginWelcome__logo .euiIcon {\\n    vertical-align: baseline; }\\n\\nNote: Resize your logo to size 80*80 pixel.\\nMake change\\nTo make change, i need install or remove 1 plugin, to get kibana rebuild.\\nYou can try with :\\nInstall any plugin :\\n/usr/share/kibana/bin/kibana-plugin install https://github.com/wtakase/kibana-own-home/releases/download/v7.1.0/own_home-7.1.0.zip\\nsystemctl restart kibana\\n\\nWait in 10s, and type :\\n/usr/share/kibana/bin/kibana-plugin remove own_home\\nsystemctl restart kibana\\n\\n2. Change login in Select Space\\nRequire : Logo must have size 80*80 pixel\\nOpen file space_selector.js and remove original svg logo\\nvim /usr/share/kibana/node_modules/x-pack/plugins/spaces/public/views/space_selector/space_selector.js\\n\\nRemove line 82 and add \")\" to end of line 81, before comma. It look like\\nreact_2.default.createElement(\"span\", { className: \"spcSpaceSelector__logo\" }),\\nreact_2.default.createElement(eui_1.EuiTitle, { size: \"l\" },\\n    react_2.default.createElement(\"h1\", { tabIndex: 0, ref: this.setHeaderRef },\\n\\nNext, we need edit css. Open file  index.dark.css  and  index.light.css . Find class  .spcSpaceSelector__logo {} (line 127)\\nvim /usr/share/kibana/built_assets/css/plugins/spaces/index.dark.css\\nvim /usr/share/kibana/built_assets/css/plugins/spaces/index.dark.css\\n\\nRemove background-color and add background-image\\n.spcSpaceSelector__logo {\\n  margin-bottom: 32px;\\n  display: inline-block;\\n  width: 80px;\\n  height: 80px;\\n  line-height: 80px;\\n  text-align: center;\\n  background-image: url(\"https://i.imgur.com/OJVk1hn.png\");\\n  border-radius: 100%;\\n  padding: 16px;\\n  -webkit-box-shadow: 0 6px 12px -1px rgba(0, 0, 0, 0.2), 0 4px 4px -1px rgba(0, 0, 0, 0.2), 0 2px 2px 0 rgba(0, 0, 0, 0.2);\\n          box-shadow: 0 6px 12px -1px rgba(0, 0, 0, 0.2), 0 4px 4px -1px rgba(0, 0, 0, 0.2), 0 2px 2px 0 rgba(0, 0, 0, 0.2); }\\n  .spcSpaceSelector__logo .euiIcon {\\n    vertical-align: baseline; }\\n\\nNote : Resize your logo to size 80*80 pixel.\\nAfter that, you can make change (look in #1)\\n3. Change login in Header\\nRequire : Logo must have size 48*48 pixel\\nOpen file eui_theme_light.css and eui_theme_dark.css\\nvim /usr/share/kibana/node_modules/@elastic/eui/dist/eui_theme_light.css\\n\\nAnd\\nvim /usr/share/kibana/node_modules/@elastic/eui/dist/eui_theme_darkcss\\n\\nIn class .euiHeaderLogo__icon, change opacity to 0 (line 6248)\\n.euiHeaderLogo__icon {\\n  opacity: 0;\\n  position: relative;\\n  top: -2px; }\\n\\nIn class .euiHeaderLogo, add background-image. (change url_your_logo to your url)\\n.euiHeaderLogo {\\n  text-align: left;\\n  position: relative;\\n  height: 48px;\\n  line-height: 48px;\\n  min-width: 49px;\\n  padding: 0 13px 0 12px;\\n  display: inline-block;\\n  vertical-align: middle;\\n  background-image: url(\"url_your_logo\");\\n  white-space: nowrap; }\\n\\nIn class .euiHeaderLogo:focus, remove background\\n.euiHeaderLogo:focus {\\n    text-decoration: underline; }\\n\\n4. Change loading screen\\nwriting ......\\n5. Change favicon\\nwriting ......'},\n",
       " {'text_field': 'I updated the tatdat\\'s post. This is the method for 3, 4 and 5.\\nI\\'m based on a Kivana 7.x environment. The logo to be applied needs to be prepared in SVG format. There are many websites where Google converts image files to SVG format.\\n4. Change loading screen\\nThe loading screen has a base64 image path. Just convert the svg format to base64 and modify the code. For a site that converts SVG format to base64, see below.\\nhttps://base64.guru/converter/encode/image/svg\\nYou need to modify both files in the path below.\\nLogo image:\\n\\nkibana \\\\ src \\\\ legacy \\\\ ui \\\\ ui_render \\\\ views \\\\ chrome.pug\\n\\nLogo image and content at the bottom of the logo:\\n\\nkibana \\\\ src \\\\ legacy \\\\ ui \\\\ ui_render \\\\ views \\\\ ui.app.pug\\n\\nchrome.pug\\nBy modifying line 9 in the chrome.pug file, you can modify the title of the kibana web page.\\n\\ntitle Kibana\\n\\nTo change the loading logo, modify the\\n\\n.kibanaWelcomeLogo {\\n        width: 100%;\\n        height: 100%;\\n        background-repeat: no-repeat;\\n        background-size: contain;\\n        background-image: url(\"base64 information in SVG file\");\\n      }\\n\\n\\nui_app.pug\\nThe background-image: url of kibanaWelcomeLogo in the ui_app.pug file should also be changed to the base64 information of the SVG file above.\\n\\n.kibanaWelcomeLogo {\\n       background-image: url(\"base64 information in SVG file\");\\n       background-repeat: no-repeat;\\n       background-size: contain;\\n       width: 60px;\\n       height: 60px;\\n       margin: 10px 0px 10px 20px;\\n     }\\n\\n\\nThe text that appears at the bottom of the loading logo can be changed by modifying the code below.\\n.kibanaWelcomeView(id=\"kbn_loading_message\", style=\"display: none;\")\\n    .kibanaLoaderWrap\\n      .kibanaLoader\\n      .kibanaWelcomeLogoCircle\\n        .kibanaWelcomeLogo\\n    .kibanaWelcomeText(data-error-message=i18n(\\'common.ui.welcomeErrorMessage\\', { defaultMessage: \\'Kibana did not load properly. Check the server output for more information.\\' }))\\n      | #{i18n(\\'common.ui.welcomeMessage\\', { defaultMessage: \\'Your Message\\' })}\\n\\n5.Change favicon\\nThe favicon.ico file is in the following path:\\nOverwrite the favicon.ico you want to change.\\n\\nkibana \\\\ src \\\\ legacy \\\\ ui \\\\ public \\\\ assets \\\\ favicons\\n\\nThe image files in the folder are also created according to the size, and overwriting seems to be no problem. Clear your browser\\'s cache and run it to reflect the favicon.\\n3. Left Menu Header Logo\\nFor me, modifying the home button logo on the left menu did not work with tatdat\\'s suggested method.\\nFinally, we found the js path where the kibana logo was defined and solved it by modifying it. The kibana logo is defined in a file at the following path: It was very hard to find.\\n\\nkibana\\\\built_assets\\\\dlls\\\\icon.logo_kibana-js.bundle.dll.js\\n\\nJust insert the code from the SVG file into the js code, but it\\'s a little bit easier to apply.'},\n",
       " {'text_field': 'I am using Beats-&gt;ElasticSearch-&gt;Kibana for viewing the metrics. Now I need to monitor the webapplications that are deployed on a tomcat server. How can we achieve this? Please help.'},\n",
       " {'text_field': 'We use the following setting in haertbeat.yml to monitor our application - maybe this helps:\\n\\n- type: http\\n  enabled: true\\n  name: \"&lt;Application Name&gt;\"\\n  urls: [\"&lt;Application Url&gt;\"]\\n  ssl.certificate_authorities: [\"&lt;Root-CA PAth&gt;\"]\\n  check.request.method: GET\\n  check.response.status: 200\\n  schedule: \\'@every 60s\\'\\n'},\n",
       " {'text_field': 'Hi,\\nI\\'m trying to apply a tag based on the contents of a field. I\\'ve been trying it like this:\\nif [field.keyword] =~ /^(TEST|test)-.*$/ {\\n          mutate { add_tag =&gt; [ \"TEST\" ] } }\\n       }\\n\\nThe above does not work, the tag never gets applied even though the contents of the field looks like this:\\nTEST-somedata\\nor\\ntest-somedata\\nWhat is the issue with the regex above?'},\n",
       " {'text_field': 'The .keyword field does not exist until the data is ingested into elasticsearch. In logstash you should test\\nif [KundID-SiteID] =~ /^(TEST|test)-.*$/ {'},\n",
       " {'text_field': 'We set up authentication in our ELK cluster.\\nWe have an admin user with superuser role. I go to kibana, login as admin and I can login and see all the spaces.\\nHowever, when we click in a space, we get\\n[security_exception] action [cluster:admin/xpack/security/privilege/get] is unauthorized for user [_anonymous]\\n\\nI am logged in as admin, so why is it trying to use _anonymous to access that? '},\n",
       " {'text_field': 'Found the issue: I did not have elasticsearch.username nor password defined in my kibana.yml file, so it was trying to use _anonymous user.\\nAfter adding uisername and password there, everything works.\\nTopic can be closed'},\n",
       " {'text_field': 'Hi, I am using bulk api to inject pandas dataframe into elasticsearch index. I first converted dataframe to dict and then used bulk.\\nIn my .csv data file , I have a column name \"start_date\", I have converted it into datetime using to_datetime and it has some empty rows, when I used bulk. I got this error:\\n\\'type\\': \\'illegal_argument_exception\\', \\'reason\\': \\'cannot parse empty date\\'\\nAfter that I converted my empty rows i.e \\'\\' to pd.NaT using replace function, but still I am getting the same error.\\nPlease help how to resolve this issue, and also, Is NaT while injecting into elasticsearch has some issue to be taken care of?\\nThanks'},\n",
       " {'text_field': 'No. You must make certain that, for every row in your input data, one or the other of the following statements is true:\\n\\nThere is no key named \"start_date\" in the dictionary representing the row\\n\\nor\\n\\nThe value for the key \"start_date\" is a valid date, as defined by your mapping (defaults to, IIRC, ISO8601 or milliseconds since the epoch)\\n\\nIf any row in your indata does not fulfill one of these, your ingestion will fail.'},\n",
       " {'text_field': \"Hello,\\nI seem to be getting some strange behaviour in canvas line graphs. When I do an elastic SQL for 30 days my chart looks like this:\\n\\nMy sql looks like this:\\n\\nSELECT * FROM index WHERE QUERY('name:Splinter') AND date &gt; TODAY() - INTERVAL '30' DAYS\\n\"},\n",
       " {'text_field': 'Fixed this, noticed that the data wasnt being ordered by date.\\nThanks for your help'},\n",
       " {'text_field': \"Hi,\\nI'm wondering if there is a way to parse one single CSV log using two distinct logstash conf files assuming each conf file would have a CSV filter plugin with autodetect_column_names set to true.\\nAs I understand it, the option to autodetect column names drop the header event after reading it.\\nTherefore, I suppose this is why the second conf file never gets this event and I get values as column names.\\nIs there a way to keep this header event for the second configuration file or am I forced to not use a CSV filter in one of my conf files ?\"},\n",
       " {'text_field': 'Why would you want to parse the same CSV line twice? If they are running in the same pipeline then the second configuration file has access to all the fields that the first one parsed.'},\n",
       " {'text_field': 'Hi, I was trying to consolidate stack trace by filebeat as per instruction here\\nhttps://www.elastic.co/guide/en/beats/filebeat/7.0/multiline-examples.html\\nThis is my filebeat.yml settings\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n    - /home/myHomeFolder/log_example.log    \\n  fields:\\n    app: myapp\\n    multiline.pattern: \\'^Excep\\'\\n    multiline.negate: true\\n    multiline.match: after\\n\\n  name: filebeat-myapp-test\\n\\ntags: [\"\", \"env-test\"]\\n\\n# Optional fields that you can specify to add additional information to the\\n# output.\\nfields:\\n  env: test\\n  app: myapp\\n\\nThis is an example log file /home/myHomeFolder/log_example.log :\\n\\nException in thread \"main\" java.lang.NullPointerException\\n        at com.example.myproject.Book.getTitle(Book.java:16)\\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\\nException in thread \"main\" java.lang.NullPointerException\\n        at com.example.myproject.Book.getTitle(Book.java:16)\\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\\nException in thread \"main\" java.lang.NullPointerException\\n        at com.example.myproject.Book.getTitle(Book.java:16)\\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\\nException in thread \"main\" java.lang.NullPointerException\\n        at com.example.myproject.Book.getTitle(Book.java:16)\\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\\nException in thread \"main\" java.lang.NullPointerException\\n        at com.example.myproject.Book.getTitle(Book.java:16)\\n        at com.example.myproject.Author.getBookTitles(Author.java:25)\\n        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)\\n\\n\\nThis is an output of command\\nsudo /usr/share/filebeat/bin/filebeat -e -c /home/myHomeFolder/filebeat.yml\\n(part of the output) :\\n\\n. . .\\n{\\n\"@timestamp\": \"2019-08-07T14:29:13.559Z\",\\n\"@metadata\": {\\n\"beat\": \"filebeat\",\\n\"type\": \"_doc\",\\n\"version\": \"7.0.1\"\\n},\\n\"log\": {\\n\"file\": {\\n\"path\": \"/home/myHomeFolder/log_example.log\"\\n},\\n\"offset\": 2821\\n},\\n\"message\": \"Exception in thread \"main\" java.lang.NullPointerException\",\\n\"tags\": [\\n\"\",\\n\"env-test\"\\n],\\n\"input\": {\\n\"type\": \"log\"\\n},\\n\"fields\": {\\n\"env\": \"test\",\\n\"app\": \"myapp\",\\n\"multiline\": {\\n\"pattern\": \"^Excep\",\\n\"negate\": true,\\n\"match\": \"after\"\\n}\\n},\\n\"ecs\": {\\n\"version\": \"1.0.0\"\\n},\\n\"host\": {\\n\"name\": \"myapp-test\"\\n},\\n\"agent\": {\\n\"id\": \"0f1992b2-2dce-486c-a400-76cbe7e16a06\",\\n\"version\": \"7.0.1\",\\n\"type\": \"filebeat\",\\n\"ephemeral_id\": \"6a1ef472-992a-40ff-a448-04fa037e3bef\",\\n\"hostname\": \"myapp-test\"\\n}\\n}\\n{\\n\"@timestamp\": \"2019-08-07T14:29:13.559Z\",\\n\"@metadata\": {\\n\"beat\": \"filebeat\",\\n\"type\": \"_doc\",\\n\"version\": \"7.0.1\"\\n},\\n\"message\": \"        at com.example.myproject.Book.getTitle(Book.java:16)\",\\n\"tags\": [\\n\"\",\\n\"env-test\"\\n],\\n\"input\": {\\n\"type\": \"log\"\\n},\\n\"fields\": {\\n\"env\": \"test\",\\n\"app\": \"myapp\",\\n\"multiline\": {\\n\"pattern\": \"^Excep\",\\n\"negate\": true,\\n\"match\": \"after\"\\n}\\n},\\n\"agent\": {\\n\"id\": \"0f1992b2-2dce-486c-a400-76cbe7e16a06\",\\n\"version\": \"7.0.1\",\\n\"type\": \"filebeat\",\\n\"ephemeral_id\": \"6a1ef472-992a-40ff-a448-04fa037e3bef\",\\n\"hostname\": \"myapp-test\"\\n},\\n\"ecs\": {\\n\"version\": \"1.0.0\"\\n},\\n\"host\": {\\n\"name\": \"myapp-test\"\\n},\\n\"log\": {\\n\"offset\": 2879,\\n\"file\": {\\n\"path\": \"/home/myHomeFolder/log_example.log\"\\n}\\n}\\n}\\n{\\n\"@timestamp\": \"2019-08-07T14:29:13.559Z\",\\n\"@metadata\": {\\n\"beat\": \"filebeat\",\\n\"type\": \"_doc\",\\n\"version\": \"7.0.1\"\\n},\\n\"message\": \"        at com.example.myproject.Author.getBookTitles(Author.java:25)\",\\n. . .\\n\\nAs you can see, it splits every message separately(every line one by one) But I want to gather all this staff for the stack trace in a one message and than send to logstash server.\\nI tried to switch multiline.negate to true or false and no luck\\nCan anyone help with this?\\nThere is one error on filebeat starting\\n2019-08-07T17:28:23.554+0300 ERROR fileset/modules.go:125 Not loading modules. Module directory not found: /usr/share/filebeat/bin/module\\nand one warning\\n2019-08-07T17:28:23.555+0300 WARN beater/filebeat.go:357 Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning.\\nBut I believe it does not affect at all...\\nThanx in advance!'},\n",
       " {'text_field': \"Can you try moving your multiline options up a level?\\nI don't think it should be under the fields option as it should be on the same level according to the log input documentation:\\nhttps://www.elastic.co/guide/en/beats/filebeat/7.0/filebeat-input-log.html\\nEx.\\nfields:\\n  app: myapp\\nmultiline.pattern: '^Excep'\\nmultiline.negate: true\\nmultiline.match: after\\n\\nYou can also look at a reference yml here and see how everything is setup:\\nhttps://www.elastic.co/guide/en/beats/filebeat/7.0/filebeat-reference-yml.html\\nRegarding your multiline pattern, you can test it on the Go Playground, Filebeat has documentation on this at: https://www.elastic.co/guide/en/beats/filebeat/7.0/_test_your_regexp_pattern_for_multiline.html\\nAccording to the playground, your pattern seems to do what you want it to do so if you can get the multiline options to work everything should fall into place\"},\n",
       " {'text_field': 'Привет!\\nХотелось узнать, есть-ли какая-то информация о разработки в сторону словарей\\\\справочников?\\nК примеру, есть документ с ID категории, хотелось бы в поисковой выдаче или визуализации иметь не \"1\", \"3\", а \"Фрукты\", \"Игрушки\".\\nТакже это необходимо для бизнес-аналитики.'},\n",
       " {'text_field': 'На сколько я знаю, только на входе (во время индексации). Если я правильно вас понимаю, вас больше интересует выход (поиск).'},\n",
       " {'text_field': 'Im running a 5 node cluster in kubernetes. Each k8 host has 56 cores but when I query for node information I get these values:\\n    \"available_processors\" : 1,\\n    \"allocated_processors\" : 1\\n\\nIm using Java 12 so according to this the correct value is supposed to be passed to the container(s).\\nFurthermore - if I open a shell to a running container and run nproc I get the correct value (56).\\nHave I misconfigured something? Do I need to manually specify # of cores to the JVM?\\n\\n\\nSimilar issue to this posting\\n'},\n",
       " {'text_field': 'Hi, @ethrbunny\\nI have not changed any JVM arguments. I have set a limit cpu and it worked . There are some blogs on Internet that talk about this kind of issue. It depends on Java version and the proper application.\\nRegards '},\n",
       " {'text_field': 'Доброго дня уважаемые форумчане!\\nМоя схема передачи логов следующая:\\nApp servers -&gt; AWS SQS (очередь) -&gt; Logstash (input =&gt; sqs, outsput =&gt; elasticsearch) -&gt; AWS Elasticsearch\\nИногда Elasticsearch блокируется на запись (например, из-за недостатка места) и отсюда у меня возникают вопросы.\\nВопросы:\\n\\nЧто при этом происходит в Logstash? Сообщения попадают в in-memory очередь?\\nЕсть предел этих сообщений (размер RAM)?\\nТогда почему при заблокированной записи в Elasticsearch я вижу, что очередь SQS растёт, Logstash не падает\\nЕсли он не падает то может объем памяти выделенный под JVM heap заполняется и дальше Logstash перестаёт принимать сообщения?\\n\\nПомогите узнать правду. Благодарю заранее за ответы '},\n",
       " {'text_field': 'Я думаю, вы найдете ответы на эти вопросы тут\\nhttps://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html\\nhttps://www.elastic.co/guide/en/logstash/current/persistent-queues.html'},\n",
       " {'text_field': 'Greetings!\\nWe are using Elasticsearch rollups of metricbeat data using Kibana. Instead of @datetime we would like to see the date in UTC. I cannot find an option to do this in Kibana. We just need to convert the field \"@timestamp.date_histogram.timestamp\" in UTC format. I.e., we want \\'07-23-2019T19:34:00.000000\\' rather than \\'1563910440000\\'.\\nAny suggestions will be greatly appreciated.\\nI tried to modify the JSON configuration directly but Kibana will not allow it.\\nThis is the JSON configuration file for the rollup:\\n{\\n\"config\": {\\n\"id\": \"cpu_daily_rollup\",\\n\"index_pattern\": \"metricbeat-*\",\\n\"rollup_index\": \"cpu_daily\",\\n\"cron\": \"0 0 0 * * ?\",\\n\"groups\": {\\n\"date_histogram\": {\\n\"fixed_interval\": \"24h\",\\n\"field\": \"@timestamp\",\\n\"delay\": \"30m\",\\n\"time_zone\": \"UTC\"\\n},\\n\"histogram\": {\\n\"interval\": 5,\\n\"fields\": [\\n\"system.cpu.total.pct\"\\n]\\n},\\n\"terms\": {\\n\"fields\": [\\n\"system.cpu.total.pct\",\\n\"fields.oss.id\",\\n\"system.cpu.cores\"\\n]\\n}\\n},\\n\"metrics\": [\\n{\\n\"field\": \"system.cpu.total.pct\",\\n\"metrics\": [\\n\"avg\",\\n\"max\",\\n\"value_count\",\\n\"sum\",\\n\"min\"\\n]\\n},\\n{\\n\"field\": \"@timestamp\",\\n\"metrics\": [\\n\"value_count\"\\n]\\n}\\n],\\n\"timeout\": \"20s\",\\n\"page_size\": 1000\\n},\\n\"status\": {\\n\"job_state\": \"started\",\\n\"current_position\": {\\n\"@timestamp.date_histogram\": 1564963200000,\\n\"fields.oss.id.terms\": \"5a4fe62140a7078a7930ceac\",\\n\"system.cpu.cores.terms\": 2,\\n\"system.cpu.total.pct.histogram\": 0,\\n\"system.cpu.total.pct.terms\": 1.948\\n},\\n\"upgraded_doc_id\": true\\n},\\n\"stats\": {\\n\"pages_processed\": 17,\\n\"documents_processed\": 5424850,\\n\"rollups_indexed\": 15490,\\n\"trigger_count\": 1,\\n\"index_time_in_ms\": 4725,\\n\"index_total\": 16,\\n\"index_failures\": 0,\\n\"search_time_in_ms\": 47329,\\n\"search_total\": 17,\\n\"search_failures\": 0\\n}\\n}\\n\\nUTC.png640×988 23.4 KB\\n\\nThank you!'},\n",
       " {'text_field': \"Ah, sorry, I forgot - You cannot view rollup indices in Discover because discover is a tool to view individual documents. In rollup indices the original documents are not present anymore. If you want to see the count of documents over time, you can configure this kind of chart in Visualize based on the rollup index and everything will get formatted correctly.\\nIf you absolutely want to look at the the raw rolled up documents, you can create a standard index pattern that matches only the rolled up indices, then the rolled up document groups are shown as individual documents, which is probably more confusing than helpful. It's easier to stick with Visualizations and Dashboard to explore rolled up data.\"},\n",
       " {'text_field': 'Hello,\\nI can\\'t import the older logs into elasticsearch even though the following options have been specified into winlogbeat.yml.\\nwinlogbeat.event_logs:\\n  - name: Application\\n    ignore_older: 720h\\n  - name: System\\n    ignore_older: 720h\\n  - name: Security\\n    ignore_older: 720h\\n    tags: [\"security\", \"elastic1\"]\\nwinlogbeat.shutdown_timeout: 30s\\n\\nThe ILM settings are shown below:\\n# ILM and template settings\\nsetup.template.overwrite: true\\nsetup.ilm.enabled: true\\nsetup.ilm.rollover_alias: \"elastic1\"\\nsetup.ilm.pattern: \"{now/d}-000001\"\\nsetup.ilm.policy_file: \"ilm_policy.json\"\\nsetup.ilm.policy_name: \"elastic1\"\\nsetup.ilm.overwrite: true\\n\\nilm_policy.json\\n{\\n  \"policy\": {\\n    \"phases\": {\\n      \"hot\": {\\n        \"actions\": {\\n          \"rollover\": {\\n            \"max_age\": \"7d\",\\n            \"max_size\": \"10G\"\\n          }\\n        }\\n      },\\n      \"warm\": {\\n        \"min_age\": \"7d\",\\n        \"actions\": {\\n          \"forcemerge\": {\\n            \"max_num_segments\": 6\\n          },\\n          \"shrink\": {\\n            \"number_of_shards\": 6\\n          },\\n          \"allocate\": {\\n            \"number_of_replicas\": 1\\n          }\\n        }\\n      },\\n      \"cold\": {\\n        \"min_age\": \"30d\",\\n        \"actions\": {\\n          \"allocate\": {\\n            \"require\": {\\n              \"type\": \"cold\"\\n            }\\n          }\\n        }\\n      },\\n      \"delete\": {\\n        \"min_age\": \"90d\",\\n        \"actions\": {\\n          \"delete\": {}\\n        }\\n      }\\n    }\\n  }\\n}\\n'},\n",
       " {'text_field': 'Prior to ILM, logs from a past date, say 2019.01.01, would be placed into an index with the same name.  With ILM, they go into the current writing index.  But, the @timestamp should have the 2019.01.01 date.\\nAre you looking at the index name or the @timestamp?'},\n",
       " {'text_field': 'My environment: two ES clusters:  one ES 6.7 and one ES 7.1\\nMy app need to use High Level REST Client to access both cluster at the same time.\\nWhen I am using 7.x REST CLIENT with 7.X ES library. When accessing 6.x ES, I got Ccs_minimize_roundtrips error when using 7.X high level REST API to access 6.x index\\nWhen I am using 6.x REST CLient with 7.X ES, when accessing 7.x ES,I got java.lang.ClassNotFoundException: org.elasticsearch.search.aggregations.metrics.cardinality.ParsedCardinality since I have a Cardinality Aggregation.\\nWhen I am using both 6.x REST Client and ES lib, when access 7.x ES, some of the aggregation result on 7.X ES is not correct.\\nAny idea how to solve this issue? Thanks'},\n",
       " {'text_field': 'With 6.8.3 HLRC and 6.8.3 ES lib. I was able to access 6.x and 7.x ES with correct TotalHit() and no Cardinality class not found error.\\nThanks for all the help.'},\n",
       " {'text_field': 'Hello All,\\nI am getting an error which suggests that my if-statement is trying to do a &gt; (greater-than) expression comparison against a null value. When I comment out the if-statement everything works as expected.\\nHere is the\\nfilter {\\n if [netflow][fw_ext_event] != \\'\\' {\\n\\n   mutate { convert =&gt; {\"[netflow][fw_ext_event]\" =&gt; \"integer\" }}\\n\\n   ######## This does not work######\\n   #if [netflow][fw_ext_event] &gt; 2000 {\\n   #  mutate { add_field =&gt; { \"netflow.fw_ext_event_name\" =&gt; \"flowDeleted\"} }\\n   #}\\n   ########\\n\\n   if [netflow][fw_ext_event] == 1001 {\\n   #} else if [netflow][fw_ext_event] == 1001 {\\n     mutate { add_field =&gt; { \"netflow.fw_ext_event_name\" =&gt; \"deniedByIngressACL\"} }\\n   } else if [netflow][fw_ext_event] == 1002 {\\n     mutate { add_field =&gt; { \"netflow.fw_ext_event_name\" =&gt; \"deniedByEgressACL\"} }\\n   } else if [netflow][fw_ext_event] == 1003 {\\n     mutate { add_field =&gt; { \"netflow.fw_ext_event_name\" =&gt; \"deniedICMP\"} }\\n   } else if [netflow][fw_ext_event] == 1004 {\\n     mutate { add_field =&gt; { \"netflow.fw_ext_event_name\" =&gt; \"deniedNonSYNPacket\"} }\\n   }\\n\\n }\\n}\\n\\nHere is the error message:\\njava.lang.NullPointerException: null\\n[ERROR][org.logstash.execution.WorkerLoop] Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.\\njava.lang.NullPointerException: null\\n\\nnetflow.fw_ext_event is an integer, but I still convert it to an integer as a test to validate that I am not doing a greater-than comparison against a text field.\\n\\nCan anyone please provide some insight into why I am getting the null error?\\nThank you!!'},\n",
       " {'text_field': \"I think the problem is that the field [netflow][fw_ext_event] does not exist. I notice that when you add the name you use a period in the name and do not add a field to the netflow object. Should you be referring to netflow.fw_ext_event?\\nif [netflow][fw_ext_event] != '' {\\n\\nIf the field does not exist then the left hand side is nil, which is not equal to an empty string. The normal way to test for existence is just\\nif [netflow][fw_ext_event] {\"},\n",
       " {'text_field': 'I\\'m unable to open kibana as elastic-search is irresponsive as I made changes to elasticsearch.yml for SSL authentication.\\nOn dashboard on webpage i get \"Cannot connect to the Elasticsearch cluster\".\\nMy elasticsearch.yml looks like\\n\\nxpack.security.http.ssl.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.enabled: true\\nxpack.ssl.verification_mode: none\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.http.ssl.verification_mode: certificate\\nxpack.security.http.ssl.key: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.key\\nxpack.security.http.ssl.certificate: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.crt\\nxpack.security.http.ssl.certificate_authorities: [ \"/usr/share/ca-certificates/mozilla/DigiCert_Trusted_Root_G4.crt\" ]\\nxpack.security.transport.ssl.key: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.key\\nxpack.security.transport.ssl.certificate: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.crt\\nxpack.security.transport.ssl.certificate_authorities: [ \"/usr/share/ca-certificates/mozilla/DigiCert_Trusted_Root_G4.crt\" ]\\n\\nerror i get is also\\n\\ncurl failed to verify the legitimacy of the server and therefore could not\\nestablish a secure connection to it. To learn more about this situation and\\nhow to fix it, please visit the web page mentioned above.\\nlabuser@illumni8:/root/elasticsearch-7.1.1/config$ curl -v \\'https://10.27.0.4:9200\\'\\n\\nExpire in 0 ms for 6 (transfer 0x55e27d9cf5c0)\\nTrying 10.27.0.4...\\nTCP_NODELAY set\\nExpire in 200 ms for 4 (transfer 0x55e27d9cf5c0)\\nConnected to 10.27.0.4 (10.27.0.4) port 9200 (#0)\\nALPN, offering h2\\nALPN, offering http/1.1\\nsuccessfully set certificate verify locations:\\nCAfile: none\\nCApath: /etc/ssl/certs\\nTLSv1.3 (OUT), TLS handshake, Client hello (1):\\nTLSv1.3 (IN), TLS handshake, Server hello (2):\\nTLSv1.2 (IN), TLS handshake, Certificate (11):\\nTLSv1.2 (OUT), TLS alert, unknown CA (560):\\nSSL certificate problem: unable to get local issuer certificate\\nClosing connection 0\\ncurl: (60) SSL certificate problem: unable to get local issuer certificate\\nMore details here: https://curl.haxx.se/docs/sslcerts.html\\n\\n\\ncurl failed to verify the legitimacy of the server and therefore could not\\nestablish a secure connection to it. To learn more about this situation and\\nhow to fix it, please visit the web page mentioned above.\\n\\nOn elasticsearch logs , I get\\n[2019-08-07T23:02:29,687][WARN ][o.e.h.AbstractHttpServerTransport] [illumni8] caught exception while handling client http traffic, closing connection Netty4HttpChannel{localAddress=0.0.0.0/0.0.0.0:9200, remoteAddress=/10.27.0.4:42440}\\n\\n[2019-08-07T23:02:28,702][WARN ][o.e.h.AbstractHttpServerTransport] [illumni8] caught exception while handling client http traffic, closing connection Netty4HttpChannel{localAddress=0.0.0.0/0.0.0.0:9200, remoteAddress=/127.0.0.1:41492}\\nio.netty.handler.codec.DecoderException: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 48454144202f20485454502f312e310d0a417574686f72697a6174696f6e3a2042617369632064584e6c636a707759584e7a643239795a413d3d0d0a486f73743a203132372e302e302e313a393230300d0a436f6e74656e742d4c656e6774683a20300d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a0d0a\\n\\nThank-you.'},\n",
       " {'text_field': 'Problem solved\\nwith\\nelasticsearch.hosts: [\"http://10.27.0.4:9200\"]\\nwas https,  i got confused as most examples https://www.elastic.co/guide/en/kibana/current/configuring-tls.html\\nit mentioned https which was misleading since i m using all in one ELK is one single host.\\nAlso, I\\'m not aware of any config on elasticsearch side to have it receive traffic as \"https\"?'},\n",
       " {'text_field': 'Hello,\\nI keep on getting the following error\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T02:52:45Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"No living connections\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T02:52:45Z\",\"tags\":[\"warning\",\"task_manager\"],\"pid\":1,\"message\":\"PollError No Living connections\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T02:52:46Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"Unable to revive connection: http://192.168.86.2:9200/\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T02:52:46Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"No living connections\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T02:52:48Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"Unable to revive connection: http://192.168.86.2:9200/\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T02:52:48Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"No living connections\"}\\n\\n\\nkibana.yml file\\nxpack.infra.enabled: true\\nxpack.logstash.enabled: true\\nxpack.canvas.enabled: true\\nxpack.spaces.enabled: true\\nxpack.apm.enabled: true\\nxpack.security.enabled: true\\nxpack.reporting.enabled: true\\nxpack.ml.enabled: true\\nserver.port: 5601\\nserver.host: \"0.0.0.0\"\\nelasticsearch.hosts: [\"http://192.168.86.2:9200\"]\\n\\n\\nversion: \\'2.3\\'\\nservices:\\n#### ELK\\n## Elasticsearch service\\n  elasticsearch:\\n    container_name: elasticsearch\\n    restart: always\\n    environment:\\n     - bootstrap.memory_lock=true\\n     - \"ES_JAVA_OPTS=-Xms2g -Xmx2g\"\\n     - ES_TMPDIR=/tmp\\n    cap_add:\\n     - IPC_LOCK\\n    ulimits:\\n      memlock:\\n        soft: -1\\n        hard: -1\\n      nofile:\\n        soft: 65536\\n        hard: 65536\\n    mem_limit: 6g\\n    ports:\\n     - \"127.0.0.1:64298:9200\"\\n    image: \"iukea/develasticsearch:4\"\\n    volumes:\\n     - /data:/data\\n\\n## Kibana service\\n  kibana:\\n    container_name: kibana\\n    restart: always\\n    network_mode: \"host\"\\n    ports:\\n     - \"127.0.0.1:64296:5601\"\\n    image: \"iukea/devkibana:11\"\\n\\n## Logstash service\\n  logstash:\\n    container_name: logstash\\n    restart: always\\n    env_file:\\n     - /opt/ThreatSpy/etc/compose/elk_environment\\n    image: \"iukea/logstash:4019\"\\n    volumes:\\n     - /data:/data\\n    ports:\\n     - \"6990:6990\"\\n\\n## Elasticsearch-head service\\n  head:\\n    container_name: head\\n    restart: always\\n    ports:\\n     - \"127.0.0.1:64302:9100\"\\n    image: \"dtagdevsec/head:1811\"\\n    read_only: false\\n\\n\\nany advice?'},\n",
       " {'text_field': 'well i am a dummy\\nelasticsearch.hosts: [\"http://127.0.0.1:64298\"]\\ndid the trick lol'},\n",
       " {'text_field': 'Hi, i am producing a valid json document from get-aduser username | convertto-json | out-file file.json\\nI am then sending that file to ES using filebeat, however in ES it is ingesting the document, one document per line, but because the json document is multiline it\\'s obviously not what i need.\\nHow do i get the entire json object as a single document in ES rather then a document per line?\\nFollow on question would be, say i want to do get-aduser -filter * and output that to a json file, and send that in, does the same rule apply?\\nfile.json\\n{\\n\"GivenName\":  \"Joe\",\\n\"Surname\":  \"Bloggs\",\\n\"UserPrincipalName\":  \"job@domain.com\",\\n\"Enabled\":  true,\\n\"SamAccountName\":  \"job\",\\n\"SID\":  {\\n\"BinaryLength\":  28,\\n\"AccountDomainSid\":  {\\n\"BinaryLength\":  24,\\n\"AccountDomainSid\":  \"S-1-5-21-nnnn-nnnn-nnnn\",\\n\"Value\":  \"S-1-5-21-nnnn-nnnn-nnnn\"\\n},\\n\"Value\":  \"S-1-5-21-nnnn-nnnn-nnnn-nnnn\"\\n},\\n\"DistinguishedName\":  \"CN=Joe Bloggs,DC=domain,DC=com\",\\n\"Name\":  \"Joe Bloggs\",\\n\"ObjectClass\":  \"user\",\\n\"ObjectGuid\":  \"nnnn-nnnn-nnnn-nnnn-nnnn\"\\n }\\n\\nFilebeat.yml\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n    - \"D:\\\\\\\\dev\\\\\\\\ADUsers.json\"\\n  processors:\\n     - decode_json_fields:\\n       fields: [\\'message\\']'},\n",
       " {'text_field': '\\n\\n\\n ajhstn:\\n\\nQuestion: In a foreach loop through 100s, 1000s of users, doing a POST each iteration, is that a supported or recommended way, or should i persist with Filebeat?\\n\\n\\nIt\\'s preferable to send multiple documents in one HTTP request, using the _bulk API, which is what filebeat does.\\n\\n\\n\\n ajhstn:\\n\\n$u = Get-ADUser bob | Select-Object DistinguishedName, Enabled, GivenName, ObjectClass, ObjectGUID, SamAccountName, Surname, UserPrincipalName @{ timestamp = (Get-Date); message= $u } | ConvertTo-Json -Compress | Out-File .\\\\adusers.json -Force\\n\\n\\n\\nI think what you want, is to append the timestamp to the set of available properties selected from the ADUser object, then serialize this to JSON? If so, you can use a calculated property for this. Something like\\n# guess you want the same timestamp for all retrieved objects?\\n$timestamp = (Get-Date).ToUniversalTime().ToString(\"u\")\\n\\nGet-ADUser bob | \\nSelect-Object DistinguishedName, Enabled, GivenName, `\\n    ObjectClass, ObjectGUID, SamAccountName, Surname, `\\n    UserPrincipalName, @{Name = \\'@timestamp\\'; Expression = { $timestamp }} | \\nForEach-Object { $_ | ConvertTo-Json -Compress } |\\nOut-File .\\\\adusers.json\\n'},\n",
       " {'text_field': 'Hi all,\\nIs there a best way to restore an index like restoring a relational database?\\nAs I know, in Oracle, restoring a backup is overwriting the existing database and replace with the backup,\\nand the behavior of _restore API cannot overwrite the index since it will warn \"an open index with same name already exists in the cluster\", so I have to define \"rename_pattern\" and \"rename_replacement\".\\nIs there a best way to restore an index like restoring a relational database?\\nThanks in advance'},\n",
       " {'text_field': 'You can close or delete the index before restoring from a snapshot, or else you can restore to a new name and use an alias to redirect traffic to the new index.'},\n",
       " {'text_field': 'Hey guys,\\nI\\'m facing a problem with my elasticsearch-output in Logstash. What I\\'m trying to archieve is indexing my logs, sent by filebeat, via logstash in elasticsearch. My logs are in json-codec and I\\'m using version 6.2.3. for Filebeat, LS and ES. When Logstash tries to index my loglines I\\'m getting this error:\\n[2019-08-08T11:05:03,798][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"application_log-08.08.2019\", :_type=&gt;\"doc\", :_routing=&gt;nil}, #&lt;LogStash::Event:0x6c6a3dae&gt;], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"application_log-08.08.2019\", \"_type\"=&gt;\"doc\", \"_id\"=&gt;\"bA15cGwBBfwccSsXdQAy\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"failed to parse [vae_message.ergebnis]\", \"caused_by\"=&gt;{\"type\"=&gt;\"illegal_state_exception\", \"reason\"=&gt;\"Can\\'t get text on a START_OBJECT at 1:1043\"}}}}}\\nHere\\'s the original logline:\\n{\"@timestamp\":\"2019-08-08T11:04:27.677\",\"level\":\"INFO\",\"modul\":\"__main__\",\"process\":\"12291\",\"vae_message\":{\"action\": \"VAE-NN abfragen\", \"parameter\": {\"corrid\": \"MyCorrelationId\", \"uuid\": \"d19bcd68-e389-4414-989b-2ba5a85e274a\", \"volltext_len\": 83}, \"ergebnis\": {\"vorgangsart\": \"Rechnung\", \"points\": 0.7539469585414332, \"error_text\": \"\"}}}\\nHere\\'s the logline Logstashs receives from Filebeat:\\n{\"message\":\"{\\\\\"@timestamp\\\\\":\\\\\"2019-08-08T11:04:28.390\\\\\",\\\\\"level\\\\\":\\\\\"INFO\\\\\",\\\\\"modul\\\\\":\\\\\"__main__\\\\\",\\\\\"process\\\\\":\\\\\"12291\\\\\",\\\\\"vae_message\\\\\":{\\\\\"action\\\\\": \\\\\"VAE-NN abfragen\\\\\", \\\\\"parameter\\\\\": {\\\\\"corrid\\\\\": \\\\\"MyCorrelationId\\\\\", \\\\\"uuid\\\\\": \\\\\"d19bcd68-e389-4414-989b-2ba5a85e274a\\\\\", \\\\\"volltext_len\\\\\": 83}, \\\\\"result\\\\\": {\\\\\"vorgangsart\\\\\": \\\\\"Rechnung\\\\\", \\\\\"points\\\\\": 0.7539469585414332, \\\\\"error_text\\\\\": \\\\\"\\\\\"}}}\",\"@version\":\"1\",\"type\":\"application_log\",\"process\":\"12291\",\"source\":\"/vae-service/predict-main-log.txt\",\"offset\":48673,\"tags\":[\"HDLT\",\"MS-VAE\",\"beats_input_codec_plain_applied\"],\"level\":\"INFO\",\"vae_message\":{\"result\":{\"error_text\":\"\",\"points\":0.7539469585414332,\"vorgangsart\":\"Rechnung\"},\"parameter\":{\"corrid\":\"MyCorrelationId\",\"volltext_len\":83,\"uuid\":\"d19bcd68-e389-4414-989b-2ba5a85e274a\"},\"action\":\"VAE-NN abfragen\"},\"modul\":\"__main__\",\"@timestamp\":\"2019-08-08T09:04:28.390Z\",\"beat\":{\"version\":\"6.2.3\",\"hostname\":\"MM01\",\"name\":\"MM01\"},\"host\":\"localhost\"}\\nMy Logstash-Configuration looks like this:\\ninput {\\n    beats {\\n      host =&gt; \"192.168.xxx.xxx\"\\n      port =&gt; \"5044\"\\n      type =&gt; \"application_log\"\\n    }\\n}\\n\\nfilter {\\n    json {\\n        source =&gt; \"message\"\\n    }\\n}\\n\\noutput {\\n    file {\\n        path =&gt; \"/var/log/logstash.output.log\"    \\n    }\\n\\n    elasticsearch {\\n        hosts =&gt; [\"192.168.xxx.xxx:9200\"]\\n        index =&gt; [\"application_log-%{+dd.MM.YYYY}\"]\\n    } \\n}\\n\\nCan anybody help me please?'},\n",
       " {'text_field': \"What does the mapping of your ES index look like? It seems like it is expecting “ergebnis” to be text, but it is an object. So you'd either have to change it to a string in Logstash or recreate your index with the field ergebnis as a nested datatype.\"},\n",
       " {'text_field': \"Hi,\\nIs it possible to disable elastic apm agent on a running server without causing any server downtime?\\nI don't know how elastic is going to behave in one of my environments and would like the ability to turn it off without causing any server downtime if possible, I checked the docs but couldn't find anything relating to this.\\nThanks!\"},\n",
       " {'text_field': 'Yes, you can set active to false at runtime via the elasticapm.properties or by setting the corresponding system property.'},\n",
       " {'text_field': 'Hi,\\nProblem: I am trying to put an alert on for every time a log is not sent.\\nI have tried to stimulate my watch, but I get the following error message back:\\n\"[parse_exception] could not read search request. unexpected object field [query]\".\\nThis is what my console looks like:\\n{\\n\"trigger\": {\\n\"schedule\": {\\n\"interval\": \"1m\"\\n}\\n},\\n\"input\": {\\n\"search\": {\\n\"request\": {\\n\"query\": {\\n\"query_string\": {\\n\"default_field\": \"content\",\\n\"query\": \"(forward_to.keyword) AND (expo)\"\\n}\\n},\\n\"indices\": \"edmetric*\"\\n}\\n}\\n},\\n\"condition\": {\\n\"compare\": {\\n\"ctx.payload.hits.total\": {\\n\"gte\": 1\\n}\\n}\\n},\\n\"actions\": {\\n\"log\": {\\n\"logging\": {\\n\"text\": \"The number of {{ctx.payload.hits.total}} logs were not sent to Expo\"\\n}\\n}\\n}\\n}\\nThank you in advance!'},\n",
       " {'text_field': 'Hi @ntran, thanks for your question.\\nIt seems like you have a little typo in your script - the request object should contain a body property, not a query property\\n{\\n  \"trigger\": {\\n    \"schedule\": {\\n      \"interval\": \"1m\"\\n    }\\n  },\\n  \"input\": {\\n    \"search\": {\\n      \"request\": {\\n        \"body\": {\\n          \"query_string\": {\\n            \"default_field\": \"content\",\\n            \"query\": \"(forward_to.keyword) AND (expo)\"\\n          }\\n        },\\n        \"indices\": \"edmetric*\"\\n      }\\n    }\\n  },\\n  \"condition\": {\\n    \"compare\": {\\n      \"ctx.payload.hits.total\": {\\n        \"gte\": 1\\n      }\\n    }\\n  },\\n  \"actions\": {\\n    \"log\": {\\n      \"logging\": {\\n        \"text\": \"The number of {{ctx.payload.hits.total}} logs were not sent to Expo\"\\n      }\\n    }\\n  }\\n}\\n\\n\\nis generated successfully for me.'},\n",
       " {'text_field': 'Good day, dear forum users!\\nMy logs transfer scheme is as follows:\\nApp servers -&gt; AWS SQS (queue) -&gt; Logstash (input =&gt; sqs, output =&gt; elasticsearch) -&gt; AWS Elasticsearch\\nSometimes Elasticsearch is blocked for writing (for example, due to lack of space) and here I have questions.\\nQuestions:\\n\\nWhat happens in Logstash? Do messages get in the in-memory Logstash queue?\\nIs there a limit to these messages (RAM size/etc)?\\nWhy with a blocked Elasticsearch writing, I see that the SQS queue is growing, and Logstash does not fail?\\nIf it does not fall, then maybe the amount of memory allocated for the JVM heap is filled and then Logstash stops receiving messages?\\n\\nHelp find out the truth. Thank you in advance for your answers.'},\n",
       " {'text_field': 'By default, Logstash uses in-memory bounded queues between pipeline stages (inputs → pipeline workers) to buffer events. The size of these in-memory queues is fixed and not configurable.\\nIf an output is blocked then back-pressure will stop the pipelines and then block the inputs. Once the output starts writing again the pipelines will start processing events and the inputs will continue reading.'},\n",
       " {'text_field': 'Hello, since i deployed an ELK 7.2 stack ( 4 days ago )  the auditbeat shards are failing with 3 out of 4 shards   when i try to use the defaulf [[Auditbeat Auditd] Overview ECS] dashboard ( or any of the other auditbeat default dashboards).Which leaves me with accessible events only older than 3 days  Everything is green in the cluster.\\nIncoming events are indexed correctly but when i execute a query the shards fails, from a data node:\\n{\\n  \"type\": \"server\",\\n  \"timestamp\": \"2019-08-08T09:53:03,254+0000\",\\n  \"level\": \"DEBUG\",\\n  \"component\": \"o.e.a.s.TransportSearchAction\",\\n  \"cluster.name\": \"elkelasticsearch_name\",\\n  \"node.name\": \"elkelasticsearch-data-1\",\\n  \"cluster.uuid\": \"uzybupYsRiuyxlorl0gVfQ\",\\n  \"node.id\": \"wkLzxc7pSeuhpJqFwAhnsQ\",\\n  \"message\": \"[auditbeat-2019.08.07][0], node[OM0otphDQ1yK3zRlqJhhKA], [R], s[STARTED], a[id=aAg3q3uXSjSS63M2r3VASQ]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[auditbeat-*], indice\\nsOptions=IndicesOptions[ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_aliases_to_multiple_indices=true, forbid_closed_indices=true, ignore_al\\niases=false, ignore_throttled=true], types=[], routing=\\'null\\', preference=\\'null\\', requestCache=null, scroll=null, maxConcurrentShardRequests=0, batchedReduceSize=512, preFilterShardSize=128, allowPartialSearch\\nResults=true, localClusterAlias=null, getOrCreateAbsoluteStartMillis=-1, ccsMinimizeRoundtrips=true, source={\\\\\"size\\\\\":0,\\\\\"timeout\\\\\":\\\\\"30000ms\\\\\",\\\\\"query\\\\\":{\\\\\"bool\\\\\":{\\\\\"must\\\\\":[{\\\\\"range\\\\\":{\\\\\"@timestamp\\\\\":{\\\\\"from\\n\\\\\":\\\\\"2019-08-08T09:52:02.844Z\\\\\",\\\\\"to\\\\\":\\\\\"2019-08-08T09:53:02.844Z\\\\\",\\\\\"include_lower\\\\\":true,\\\\\"include_upper\\\\\":true,\\\\\"format\\\\\":\\\\\"strict_date_optional_time\\\\\",\\\\\"boost\\\\\":1.0}}},{\\\\\"query_string\\\\\":{\\\\\"query\\\\\":\\\\\"event.\\nmodule:auditd\\\\\",\\\\\"fields\\\\\":[],\\\\\"type\\\\\":\\\\\"best_fields\\\\\",\\\\\"default_operator\\\\\":\\\\\"or\\\\\",\\\\\"max_determinized_states\\\\\":10000,\\\\\"enable_position_increments\\\\\":true,\\\\\"fuzziness\\\\\":\\\\\"AUTO\\\\\",\\\\\"fuzzy_prefix_length\\\\\":0,\\\\\"fuzzy\\n_max_expansions\\\\\":50,\\\\\"phrase_slop\\\\\":0,\\\\\"analyze_wildcard\\\\\":true,\\\\\"escape\\\\\":false,\\\\\"auto_generate_synonyms_phrase_query\\\\\":true,\\\\\"fuzzy_transpositions\\\\\":true,\\\\\"boost\\\\\":1.0}}],\\\\\"filter\\\\\":[{\\\\\"match_all\\\\\":{\\\\\"boost\\n\\\\\":1.0}}],\\\\\"adjust_pure_negative\\\\\":true,\\\\\"boost\\\\\":1.0}},\\\\\"aggregations\\\\\":{\\\\\"61ca57f1-469d-11e7-af02-69e470af7417\\\\\":{\\\\\"meta\\\\\":{\\\\\"timeField\\\\\":\\\\\"@timestamp\\\\\",\\\\\"intervalString\\\\\":\\\\\"1s\\\\\",\\\\\"bucketSize\\\\\":1,\\\\\"seriesId\\\\\\n\":\\\\\"61ca57f1-469d-11e7-af02-69e470af7417\\\\\"},\\\\\"terms\\\\\":{\\\\\"field\\\\\":\\\\\"event.action\\\\\",\\\\\"size\\\\\":10,\\\\\"min_doc_count\\\\\":1,\\\\\"shard_min_doc_count\\\\\":0,\\\\\"show_term_doc_count_error\\\\\":false,\\\\\"order\\\\\":[{\\\\\"_count\\\\\":\\\\\"desc\\\\\"},\\n{\\\\\"_key\\\\\":\\\\\"asc\\\\\"}]},\\\\\"aggregations\\\\\":{\\\\\"timeseries\\\\\":{\\\\\"date_histogram\\\\\":{\\\\\"field\\\\\":\\\\\"@timestamp\\\\\",\\\\\"time_zone\\\\\":\\\\\"Europe\\\\\",\\\\\"interval\\\\\":\\\\\"1s\\\\\",\\\\\"offset\\\\\":0,\\\\\"order\\\\\":{\\\\\"_key\\\\\":\\\\\"asc\\\\\"},\\\\\"keyed\\\\\":false,\\\\\"min_\\ndoc_count\\\\\":0,\\\\\"extended_bounds\\\\\":{\\\\\"min\\\\\":1565257922844,\\\\\"max\\\\\":1565257982844}},\\\\\"aggregations\\\\\":{\\\\\"6b9fb2d0-c1bc-11e7-938f-ab0645b6c431\\\\\":{\\\\\"bucket_script\\\\\":{\\\\\"buckets_path\\\\\":{\\\\\"count\\\\\":\\\\\"_count\\\\\"},\\\\\"script\\\\\\n\":{\\\\\"source\\\\\":\\\\\"count * 1\\\\\",\\\\\"lang\\\\\":\\\\\"expression\\\\\"},\\\\\"gap_policy\\\\\":\\\\\"skip\\\\\"}}}}}}}}}] lastShard [true]\",\\n  \"stacktrace\": [\\n    \"org.elasticsearch.transport.RemoteTransportException: [elkelasticsearch-data-0][10.224.3.247:9300][indices:data/read/search[phase/query]]\",\\n    \"Caused by: java.lang.IllegalArgumentException: Fielddata is disabled on text fields by default. Set fielddata=true on [event.action] in order to load fielddata in memory by uninverting the inverted index.\\n Note that this can however use significant memory. Alternatively use a keyword field instead.\",\\n    \"at org.elasticsearch.index.mapper.TextFieldMapper$TextFieldType.fielddataBuilder(TextFieldMapper.java:711) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.index.fielddata.IndexFieldDataService.getForField(IndexFieldDataService.java:116) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.index.query.QueryShardContext.getForField(QueryShardContext.java:179) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.aggregations.support.ValuesSourceConfig.resolve(ValuesSourceConfig.java:95) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.aggregations.support.ValuesSourceAggregationBuilder.resolveConfig(ValuesSourceAggregationBuilder.java:321) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.aggregations.support.ValuesSourceAggregationBuilder.doBuild(ValuesSourceAggregationBuilder.java:314) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.aggregations.support.ValuesSourceAggregationBuilder.doBuild(ValuesSourceAggregationBuilder.java:39) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.aggregations.AbstractAggregationBuilder.build(AbstractAggregationBuilder.java:139) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.aggregations.AggregatorFactories$Builder.build(AggregatorFactories.java:332) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.SearchService.parseSource(SearchService.java:789) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.SearchService.createContext(SearchService.java:591) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:550) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:353) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.SearchService.lambda$executeQueryPhase$1(SearchService.java:340) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.action.ActionListener.lambda$map$2(ActionListener.java:145) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.search.SearchService$2.doRun(SearchService.java:1052) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:44) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:758) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.2.0.jar:7.2.0]\",\\n    \"at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\",\\n    \"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\",\\n    \"at java.lang.Thread.run(Thread.java:835) [?:?]\"\\n  ]\\n}\\n\\n'},\n",
       " {'text_field': 'It looks like the index auditbeat-2019.08.07 has the field event.action as  a text field rather than a keyword field. Is that an old or new index? If it is a new index, and has been set up using the ./auditbeat setup command it should be a keyword field - so I suspect something went wrong there.\\nIf you want to see data from it in the dashboard you can either enable fielddata (docs), or reindex the data with the correct data type for that field.'},\n",
       " {'text_field': 'Hy,\\nIs the answer to this post Relationship between refresh &amp; flush  still valid ?\\nid \"Yes that is expected, a refresh calls a flush and writes files to the filesystem\"'},\n",
       " {'text_field': 'yes, that is the high level overview.'},\n",
       " {'text_field': 'Hi,\\nDuring the course \"Elasticsearch Engineer II\", it is said :\\n\"A search request is distributed to the shards and on each\\nshard it is performed sequentially over the segments\"\\nDoes that means that if tow requests arrive on a shard, the second waits until the first one finishes to request all segments ?'},\n",
       " {'text_field': 'less segments is usually a faster search, this is why merging happens also in the background. forcemerging mostly makes sense if you have data that gets written once and then never is written again (no deletes, no update, no indexes).'},\n",
       " {'text_field': 'I have the following json code, it has a nested fields and I\\'m trying to make it flat but I couldn\\'t!\\nHere is the source json file\\n{\\n    \"body\": {\\n        \"BlockName\": \"SA\",\\n        \"input\": {\\n            \"type\": \"array\",\\n            \"identification\": \"iValue\",\\n            \"configs\": {},\\n            \"information\": {\\n                \"location\": \"Users\\\\\\\\files\\\\\\\\\",\\n                \"client_id\": \"abd\"\\n            },\\n            \"value\": {\\n                \"info\": [\\n                    \"Apple\",\\n                    \"MSFT\",\\n                    \"AVB\"\\n                ],\\n                \"domains\": [],\\n                \"uniqueId\": \"1108738857225224193\",\\n                \"restul_set\": [\\n                    0.4112226366996765,\\n                    0.34098902344703674,\\n                    0.24778836965560913\\n                ],\\n                \"timestamp\": 1554940800000,\\n                \"text\": \"Test Text\"\\n            }\\n        }\\n    },\\n    \"result\": 0.06\\n}\\n\\nhere is what I\\'m looking for:\\n{\\n    \"body.BlockName\": \"SA\",\\n    \"body.input.type\": \"array\",\\n    \"body.input.identification\": \"iValue\",\\n    \"body.input.configs\": {},\\n    \"body.input.information.location\": \"Users\\\\\\\\files\\\\\\\\\",\\n    \"body.input.information.client_id\": \"abd\",    \\n    \"body.input.value.info\": [\\n        \"Apple\",\\n        \"MSFT\",\\n        \"AVB\"\\n    ],\\n    \"body.input.value.domains\": [],\\n    \"body.input.value.uniqueId\": \"1108738857225224193\",\\n    \"body.input.value.restul_set\": [\\n        0.4112226366996765,\\n        0.34098902344703674,\\n        0.24778836965560913\\n    ],\\n    \"body.input.value.timestamp\": 1554940800000,\\n    \"body.input.value.text\": \"Test Text\",\\n    \"result\": 0.06\\n}\\n\\nand here is the logstash config file:\\ninput {\\n exec{\\n    command =&gt; \"cat /usr/share/logstash/data/pipelineOutput.json\"\\n    codec =&gt; json\\n    interval =&gt; 60\\n  }\\n}\\nfilter {\\n\\n  split {     \\n    field =&gt; \"[body]\"\\n  }\\n\\n}\\noutput {\\n stdout {\\n    codec =&gt; json\\n }\\n}\\n\\nany ideas?'},\n",
       " {'text_field': 'You will need a ruby script. Create a file called flattenJSON.rb that contains\\ndef register(params)\\n    @field = params[\\'field\\']\\nend\\n\\ndef flatten(object, name, event)\\n    if object\\n        if object.kind_of?(Hash) and object != {}\\n            object.each { |k, v| flatten(v, \"#{name}.#{k}\", event) }\\n        else\\n            event.set(name, object)\\n        end\\n    end\\nend\\n\\ndef filter(event)\\n    o = event.get(@field)\\n    if o\\n        flatten(o, @field, event)\\n    end\\n    event.remove(@field)\\n    [event]\\nend\\n\\nand then call it using\\n    ruby {\\n        path =&gt; \"/home/user/flattenJSON.rb\"\\n        script_params =&gt; { \"field\" =&gt; \"body\" }\\n    }'},\n",
       " {'text_field': \"Hi,\\nI am getting the below error while trying to ingest data to ES (ECE) from hadoop (HIVE) query.\\n===============\\n2019-08-08 13:55:24,957 INFO [main]: httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(444)) - Retrying request\\n2019-08-08 13:55:24,958 ERROR [main]: rest.NetworkClient (NetworkClient.java:execute(147)) - Node [d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443] failed (java.net.ConnectException: Connection refused (Connection refused)); no other nodes left - aborting...\\n2019-08-08 13:55:24,959 ERROR [main]: CliDriver (SessionState.java:printError(1089)) - Failed with exception java.io.IOException:org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\\njava.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:520)\\n\\nat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:427)\\n\\nat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146)\\n\\nat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1773)\\n\\nat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:237)\\n\\nat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:169)\\n\\nat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:380)\\n\\nat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:740)\\n\\nat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:685)\\n\\nat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)\\n\\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\nat java.lang.reflect.Method.invoke(Method.java:498)\\n\\nat org.apache.hadoop.util.RunJar.run(RunJar.java:233)\\n\\nat org.apache.hadoop.util.RunJar.main(RunJar.java:148)\\n\\nCaused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\\nat org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340)\\n\\nat org.elasticsearch.hadoop.hive.HiveUtils.init(HiveUtils.java:197)\\n\\nat org.elasticsearch.hadoop.hive.EsHiveInputFormat.getSplits(EsHiveInputFormat.java:112)\\n\\nat org.elasticsearch.hadoop.hive.EsHiveInputFormat.getSplits(EsHiveInputFormat.java:51)\\n\\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:371)\\n\\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:303)\\n\\nat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:458)\\n\\n... 15 more\\n\\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443]]\\nat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:152)\\n\\nat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:424)\\n\\nat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388)\\n\\nat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392)\\n\\nat org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168)\\n\\nat org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:735)\\n\\nat org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330)\\n\\n... 21 more\\n\\n2019-08-08 13:55:24,959 INFO [main]: exec.TableScanOperator (Operator.java:close(616)) - Closing operator TS[0]\\n2019-08-08 13:55:24,959 INFO [main]: exec.SelectOperator (Operator.java:close(616)) - Closing operator SEL[1]\\n2019-08-08 13:55:24,959 INFO [main]: exec.ListSinkOperator (Operator.java:close(616)) - Closing operator OP[3]\\n2019-08-08 13:55:24,963 INFO [main]: CliDriver (SessionState.java:printInfo(1066)) - Time taken: 0.051 seconds\\n2019-08-08 13:55:24,963 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(149)) - \\n2019-08-08 13:55:24,963 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - &lt;/PERFLOG method=releaseLocks start=1565272524963 end=1565272524963 duration=0 from=org.apache.hadoop.hive.ql.Driver&gt;\\nBelow configuration is working fine for ES as independent component but failing in ECE.\\n====================\\n'es.index.read.missing.as.empty'='true',\\n'es.mapping.date.rich'='false',\\n'es.mapping.names'='date:@timestamp',\\n'es.net.http.auth.pass'='&lt;es_cluster_pwd&gt;',\\n'es.net.http.auth.user'='&lt;es_cluster_id&gt;',\\n'es.net.ssl'='true',\\n'es.net.ssl.cert.allow.self.signed'='true',\\n'es.net.ssl.keystore.location'='file:///etc/ssl/certs/1.jks',\\n'es.net.ssl.keystore.pass'='&lt;jks_pwd&gt;',\\n'es.net.ssl.keystore.type'='jks',\\n'es.net.ssl.protocol'='SSL',\\n'es.nodes.wan.only'='true',\\n'es.nodes'='d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443',\\n'es.query'='?q=*',\\n'es.resource'='users/user-events'\"},\n",
       " {'text_field': \"Hi James,\\nThanks for your response. I have fixed the issue by configuring below in the table properties.\\n'es.index.read.missing.as.empty'='true',\\n'es.mapping.date.rich'='false',\\n'es.mapping.names'='date:@timestamp',\\n'es.net.http.auth.pass'='&lt;ES_PWD&gt;',\\n'es.net.http.auth.user'='&lt;ES_USERNAME&gt;',\\n'es.net.ssl'='true',\\n'es.net.ssl.cert.allow.self.signed'='true',\\n'es.net.ssl.keystore.location'='&lt;JKS_FILE_PATH&gt;',\\n'es.net.ssl.keystore.pass'='&lt;KEYSTORE_PWD&gt;',\\n'es.net.ssl.keystore.type'='jks',\\n'es.net.ssl.truststore.location'='&lt;JKS_FILE_PATH&gt;',\\n'es.net.ssl.truststore.pass'='&lt;KEYSTORE_PWD&gt;',\\n'es.net.ssl.truststore.type'='jks',\\n'es.net.ssl.protocol'='SSL',\\n'es.nodes.wan.only'='true',\\n'es.nodes'='&lt;ES_CLUSTER_ID&gt;:9243',\\n'es.query'='?q=*',\\n'es.resource'='&lt;INDEX_NAME&gt;/&lt;DOCUMENT_TYPE&gt;'\\nRegards,\\nRijash\"},\n",
       " {'text_field': 'Is GC pauses still an issue with data nodes having more than 32GB of memory allocated to Java ?'},\n",
       " {'text_field': 'Roughly yes, but the reference docs give more context to that recommendation.'},\n",
       " {'text_field': \"I am trying to take the records to create user account, delete user account and modify user account.\\nI have also been asked to take audit events on files and folders, this send it to logstash.\\nI have already installed winlogbeat and it is already sending data to logstash and this to elasticsearch but I do not know which event corresponds to each of the aforementioned actions, my knowledge of log log storage in windows is null.\\nI don't know if anyone has had the same problem, thank you very much.\"},\n",
       " {'text_field': \"We have started building a module for the Security event log that will apply categorization and normalization based on Elastic Common Schema (ECS). You can try out the module or look at its source code to see what it does.\\n\\nhttps://www.elastic.co/guide/en/beats/winlogbeat/master/winlogbeat-module-security.html\\nhttps://www.elastic.co/guide/en/ecs/current/ecs-reference.html\\n\\nWe're still working on developing the categories that will go into ECS.\"},\n",
       " {'text_field': 'We are building a 3-node elastic cluster v7.1.0 (statefulset), and single-node Kibana (deployment) on Kuberenetes using basic license. Was wondering how can we implement native authentication?'},\n",
       " {'text_field': 'as Elasticsearch is using the java security manager, only a certain set of directories are allowed to be read. One of those is the config directory where you should store your certificates. Others are log/data directories, any other directory cannot be read. The reason for this behaviour is to reduce the impact if anyone finds a way to read arbitrary files using Elasticsearch, as only elasticsearch specific files could be read then.'},\n",
       " {'text_field': 'Hi\\nIs it possible to customize the machine learning algorithms?\\nI mean, is it possible to use my own algorithms?\\nThnak you'},\n",
       " {'text_field': 'Just Google \"elastic ML source code\" it\\'ll be one of the first results'},\n",
       " {'text_field': 'Elasticsearch Serviceについて質問させて頂きます。\\nDeployment環境\\n\\nAWS Asia Pacific(Tokyo)\\nI/O Optimized\\n\\n質問事項\\nCuratorを導入し、cron起動を行いたいです。\\nElasticserch ServiceにおけるCuratorの導入手順について教えてください。\\nCurator公式サイトを見てもわかりませんでした…\\ncron起動をするために、Elasticserch ServiceのOSに接続たいのですが、接続手順について教えてください。\\n例えばTeratermで接続する場合以下情報が必要になると思います。\\nこの情報はどこを参照すれば良いでしょうか。\\n\\n接続先EC2のグローバルIPアドレス or ホスト名\\nユーザ名（Amazon Linuxの場合デフォルトは ec2-user です）\\n秘密鍵（デフォルトでは ****.pem という形式です）\\n\\n初歩的な質問で恐縮ですが、分かる方がいらっしゃいましたらご回答をお願いいたします。'},\n",
       " {'text_field': 'Elasticsearch Serviceに関する質問についてはElasticsearch ServiceのHelpから問い合わせてください。問い合わせに対応してくれます。\\nまた、マネージドサービスなので、OSには接続できないです。クラウドのコンソールからのみ操作ができます。'},\n",
       " {'text_field': 'Hi all,\\nI’ve recently brought up my first Kibana server, and it looks like the HTTP server is running but not actually sending any HTML…?\\nThe details:  My Kibana is the Docker container version, and I spun it us using this tutorial.  The commands I specifically used to pull then spin up the container are:\\ndocker pull docker.elastic.co/kibana/kibana:7.3.0\\n\\ndocker run --link MyElasticsearch:elasticsearch \\\\\\n   -p 5601:5601 \\\\\\n   --net mgt_network \\\\\\n   --name MyKibana \\\\\\n   -e ELASTICSEARCH_HOSTS=\"http://192.168.3.8:9200\" \\\\\\n   8bcee4a4f79d\\n\\n...where...\\n-- \"MyElasticsearch\" is the Docker name of my Elasticsearch container, already running\\n-- ELASTICSEARCH_HOSTS is an environment variable that I think I have to set…?  Not sure\\n-- \"8bcee4a4f79d\" is the Kibana docker image’s UUID\\nThe Kibana container spins up fine and from it, I can ping my host machine, my Elasticsearch container, everywhere.\\nBut then when point my browser to my Kibana on TCP 5601, I pull down a blank webpage.  And when I test within the Kibana container itself, I also get a blank page.  Check this out, this is me on the Kibana\\'s command line:\\nbash-4.2$ curl localhost:5601 &gt; /tmp/webpage\\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\n                                 Dload  Upload   Total   Spent    Left  Speed\\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\\nbash-4.2$ ls -l /tmp\\ntotal 8\\n-rw-r--r-- 1 kibana kibana    0 Aug  8 20:21 webpage\\nbash-4.2$\\n\\nSoooo…  The online documentation says that once I spin up Kibana, I should be able to websurf to my Kibana and start configuring it.  But is there something else I have to do to spur the web service into action?  Any suggestions on how to troubleshoot this?\\nAre there any log files which I should consult?  Below are the “docker logs” output for my Kibana container, captured while I was trying to do sample websurfs to TCP 5601:\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:12:59Z\",\"tags\":[\"listening\",\"info\"],\"pid\":1,\"message\":\"Server running at http://0:5601\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:12:59Z\",\"tags\":[\"info\",\"http\",\"server\",\"Kibana\"],\"pid\":1,\"message\":\"http server running\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:12:59Z\",\"tags\":[\"status\",\"plugin:spaces@7.3.0\",\"info\"],\"pid\":1,\"state\":\"green\",\"message\":\"Status changed from yellow to green - Ready\",\"prevState\":\"yellow\",\"prevMsg\":\"Waiting for Elasticsarch\"}\\n{\"type\":\"response\",\"@timestamp\":\"2019-08-08T20:13:35Z\",\"tags\":[],\"pid\":1,\"method\":\"get\",\"statusCode\":302,\"req\":{\"url\":\"/\",\"method\":\"get\",\"headers\":{\"user-agent\":\"curl/7.29.0\",\"host\":\"localhost:5601\",\"accept\":\"*/*\"},\"remoteAddress:\"127.0.0.1\",\"userAgent\":\"127.0.0.1\"},\"res\":{\"statusCode\":302,\"responseTime\":22,\"contentLength\":9},\"message\":\"GET / 302 22ms - 9.0B\"}\\n{\"type\":\"response\",\"@timestamp\":\"2019-08-08T20:21:29Z\",\"tags\":[],\"pid\":1,\"method\":\"get\",\"statusCode\":302,\"req\":{\"url\":\"/\",\"method\":\"get\",\"headers\":{\"user-agent\":\"curl/7.29.0\",\"host\":\"localhost:5601\",\"accept\":\"*/*\"},\"remoteAddress:\"127.0.0.1\",\"userAgent\":\"127.0.0.1\"},\"res\":{\"statusCode\":302,\"responseTime\":5,\"contentLength\":9},\"message\":\"GET / 302 5ms - 9.0B\"}\\n{\"type\":\"response\",\"@timestamp\":\"2019-08-08T20:21:45Z\",\"tags\":[],\"pid\":1,\"method\":\"get\",\"statusCode\":302,\"req\":{\"url\":\"/\",\"method\":\"get\",\"headers\":{\"user-agent\":\"curl/7.29.0\",\"host\":\"localhost:5601\",\"accept\":\"*/*\"},\"remoteAddress:\"127.0.0.1\",\"userAgent\":\"127.0.0.1\"},\"res\":{\"statusCode\":302,\"responseTime\":5,\"contentLength\":9},\"message\":\"GET / 302 5ms - 9.0B\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:25:15Z\",\"tags\":[\"info\",\"plugins-system\"],\"pid\":1,\"message\":\"Stopping all plugins.\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:25:16Z\",\"tags\":[\"info\",\"plugins\",\"translations\"],\"pid\":1,\"message\":\"Stopping plugin\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:27:12Z\",\"tags\":[\"info\",\"plugins-system\"],\"pid\":1,\"message\":\"Setting up [1] plugins: [translations]\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:27:12Z\",\"tags\":[\"info\",\"plugins\",\"translations\"],\"pid\":1,\"message\":\"Setting up plugin\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:27:12Z\",\"tags\":[\"info\",\"plugins-system\"],\"pid\":1,\"message\":\"Starting [1] plugins: [translations]\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:27:37Z\",\"tags\":[\"info\",\"plugins-system\"],\"pid\":1,\"message\":\"Setting up [1] plugins: [translations]\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:27:37Z\",\"tags\":[\"info\",\"plugins\",\"translations\"],\"pid\":1,\"message\":\"Setting up plugin\"}\\n{\"type\":\"log\",\"@timestamp\":\"2019-08-08T20:27:37Z\",\"tags\":[\"info\",\"plugins-system\"],\"pid\":1,\"message\":\"Starting [1] plugins: [translations]\"}'},\n",
       " {'text_field': \"Thanks Tiagocosta,\\nSo I went to look at my Kibana server after seeing your note.  Now the portal is available and there is no trouble surfing to it.  Well, the response is pretty slow, but that's probably because the host system is overtaxed.  Not sure why the Kibana portal was blank yesterday... but I'm not about to look a gift horse in the mouth.  Thanks for the help  \"},\n",
       " {'text_field': 'Hello everyone,\\nI\\'m trying to insert data into ElasticSearch using RestHighLevelClient and BulkProcessor. The code I\\'m using for the BulkProcessor is the following.\\n    BulkProcessor getBulkProcessor(RestHighLevelClient restHighLevelClient) {\\n        BiConsumer&lt;BulkRequest, ActionListener&lt;BulkResponse&gt;&gt; consumer = (request, bulkListener) -&gt; {\\n            try {\\n                restHighLevelClient.bulk(request, RequestOptions.DEFAULT);\\n            } catch (IOException e) {\\n                final String message = \"Failed to create BulkProcessor for ElasticSearch\";\\n                throw new RuntimeException(message, e);\\n            }\\n        };\\n\\n        return BulkProcessor.builder(consumer, getListener()).setConcurrentRequests(0).build();\\n    }\\n\\n    BulkProcessor.Listener getListener() {\\n        return new BulkProcessor.Listener() {\\n            @Override\\n            public void beforeBulk(long executionId, BulkRequest request) {\\n                int numberOfActions = request.numberOfActions();\\n                System.out.println(String.format(\"Executing bulk %d with %d requests\", executionId, numberOfActions));\\n            }\\n\\n            @Override\\n            public void afterBulk(long executionId, BulkRequest request, BulkResponse bulkResponse) {\\n                if (bulkResponse.hasFailures()) {\\n                    for (BulkItemResponse bulkItemResponse : bulkResponse) {\\n                        if (bulkItemResponse.isFailed()) {\\n                            BulkItemResponse.Failure failure = bulkItemResponse.getFailure();\\n                            throw new RuntimeException(String.format(\"Adding document %s to ElasticSearch failed\", failure.getId()),\\n                                                       failure.getCause());\\n                        }\\n                    }\\n                    System.out.println(String.format(\"Bulk %d executed with failures\", executionId));\\n                } else {\\n                    System.out.println(String.format(\"Bulk %d completed in %d milliseconds\", executionId, bulkResponse.getTook().getMillis()));\\n                }\\n            }\\n\\n            @Override\\n            public void afterBulk(long executionId, BulkRequest request, Throwable failure) {\\n                System.out.println(String.format(\"Failed to execute bulk %s\", failure));\\n                throw new RuntimeException(failure);\\n            }\\n        };\\n    }\\n\\nthen\\npublic static void main(String[] args) {\\n        BulkProcessor bulkProcessor = getBulkProcessor(/* pass RestHighLevelClient there */);\\n        DocWriteRequest docWriteRequest = /* logic to have a DocWriteRequest */;\\n        bulkProcessor.add(docWriteRequest);\\n        bulkProcessor.flush();\\n        bulkProcessor.close();\\n        return null;\\n}\\n\\nMy problem is the following :\\nMy ElasticSearch instance successfully add a new document. But my BulkProcessor doesn\\'t seems to receive a response and AfterBulk is never executed (hence the program doesn\\'t terminate because I add elements in a sync manner).\\nDoes something seems to be wrong with what I\\'m doing or am I missing something ?\\nThanks for you help !'},\n",
       " {'text_field': 'Changing bulk to bulkASync in BiConsumer solved my problem.'},\n",
       " {'text_field': 'Hi All,\\nI have a problem on groking jenkins console log, i have tested the filter instructions using input type file and it\\'s OK but when i implement this configuration using a tcp input type the results is not the same.\\nthe content of input file is:\\n002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Connexion] Summary | Status : Passed\\n002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Creation_Modele] Summary | Status : Passed\\n002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Ajout_Document_Errone_Circuit] Summary | Status : Passed\\nlogstash.conf:\\ninput {\\nfile {\\npath =&gt; [\"c:/aaa.txt\"]\\nstart_position =&gt; \"beginning\"\\n}\\n}\\nfilter {\\ngrok {\\nmatch =&gt; {\\n\"message\" =&gt; \"%{WORD:scenario} : %{WORD:action} %{DATA:feature} %{DATA:action} : %{WORD:status}\"\\n}\\nremove_field =&gt; [\"action\"]\\nadd_field =&gt; {\"date\" =&gt; \"%{+YYYY.MM.dd}\"}\\n}\\n}\\nKibana results OK:\\n{  \"path\" =&gt; \"c:/aaa.txt\",\\n\"status\" =&gt; \"Passed\",\\n\"feature\" =&gt; \"[connexion]\",\\n\"date\" =&gt; \"2019.08.08\",\\n\"message\" =&gt; \"\"002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Connexion] Summary | Status : Passed\",\\\\r\",\\n\"scenario\" =&gt; \"002_SeL_Scenario_Realisation_Circuit_Depuis_Modele\",\\n}\\n{\\n\"path\" =&gt; \"c:/aaa.txt\",\\n\"status\" =&gt; \"Passed\",\\n\"feature\" =&gt; \"[Creation_Modele]\",\\n\"date\" =&gt; \"2019.08.08\",\\n\"message\" =&gt; \"\"002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Creation_Modele] Summary | Status : Passed\",\\\\r\",\\n\"scenario\" =&gt; \"002_SeL_Scenario_Realisation_Circuit_Depuis_Modele\",\\n}\\nThe problem is there, when i implement that using tcp input, i expect the same result but is not true:\\njenkins console logs:\\nDémarré par l\\'utilisateur Jenkins\\nConstruction à distance sur windowsServer2 in workspace C:\\\\Automatisation\\\\workspace\\\\Report\\n[WS-CLEANUP] Deleting project workspace...\\nC:\\\\Automatisation\\\\workspace\\\\Report&gt;exit 0\\n[Report] $ cmd /c call C:\\\\Users\\\\ADMINI~1.WIN\\\\AppData\\\\Local\\\\Temp\\\\2\\\\jenkins8691771370815427256.bat\\nC:\\\\Automatisation\\\\workspace\\\\Report&gt;C:/Automatisation/affichage.bat\\n002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Connexion] Summary | Status : Passed\\n002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Creation_Modele] Summary | Status : Passed\\nlogstash conf:\\ninput { tcp{port =xxxx }}\\nfilter {\\njson{\\nsources =&gt; \"message\"}\\ngrok {\\nmatch =&gt; {\\n\"message\" =&gt; \"%{WORD:scenario} : %{WORD:action} %{DATA:feature} %{DATA:action} : %{WORD:status}\"\\n}\\nremove_field =&gt; [\"action\"]\\n}\\n}\\nkibana result:\\n{\\nmessage =&gt; \"..... Démarré par l\\'utilisateur  ....\"\\n\"status\" =&gt; [\\n[ 0] \"Passed\",\\n[ 1] \"Passed\",\\n],\\n\"feature\" =&gt; [\\n[ 0] \"[Connexion]\",\\n[ 1] \"[Creation_Modele]\",\\n],\\n\"scenario\" =&gt; [\\n[ 0] \"002_SeL_Scenario_Realisation_Circuit_Depuis_Modele\",\\n[ 1] \"002_SeL_Scenario_Realisation_Circuit_Depuis_Modele\",\\n],\\n}\\ni dont have scenario , feature and status in separated document, i have one document containing table of values.\\nWhen i remove json{ ..} part i have one document result contains the grok result of the first line  (002_SeL_Scenario_Realisation_Circuit_Depuis_Modele : Action1 [Connexion] Summary | Status : Passed)  the following lines ares not goked.\\ncan someone help me please, thnx in advance.'},\n",
       " {'text_field': 'Hi all,\\nThe problem is resolved, i had grok my log as following:\\nfilter {\\njson{source =&gt; \"message\"}\\nsplit{ field =&gt; \"message\"}\\ngrok {\\nmatch =&gt; {\\n\"message\" =&gt; \"%{WORD:scenario} : %{WORD:action} %{DATA:feature} %{DATA:action} : %{WORD:status}\"\\n}\\nremove_field =&gt; [\"action\"]\\nadd_field =&gt; {\"date\" =&gt; \"%{+YYYY.MM.dd}\"}\\nremove_field =&gt; [\"data\"]\\n}'},\n",
       " {'text_field': 'Hi,\\nI had installed ELK on a CentOS7 machine and then make my ELK work with more than 10GB data. After 2 weeks I stopped Logstash and clean all data and indices at Kibana. It took 3 more weeks until I didn\\'t start it again with more memory space on the machine. Now I\\'m working with a test for my new grok. The think is that it does not work fine. Logstash works correctly but, I can\\'t find my indices at Kibana.\\nHere you have my .conf file:\\n\\ninput {\\nfile {\\npath =&gt; \"/home/admin/envoirments/mytests/02/messages.csv*\"\\nsincedb_path =&gt; \"/dev/null\"\\nmode =&gt; \"read\"\\nignore_older =&gt; \"37 d\"\\nfile_completed_action =&gt; \"delete\"\\n}\\n}\\nfilter {\\ngrok {\\nmatch =&gt; { \"message\" =&gt; \"I\\'M NOT GONA SHOW MY GROK CAUSE IS INNECESARY AND CONTAINS WORK INFORMATION.\"}\\n}\\n}\\noutput {\\nelasticsearch {\\nhosts =&gt; [\"127.0.0.1:9200\"]\\nindex =&gt; [\"mytest_02\"]\\n}\\n}\\n\\nAnd here my Logstash logs:\\n\\n[2019-08-09T12:05:09,926][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n[2019-08-09T12:06:37,669][WARN ][logstash.runner          ] SIGTERM received. Shutting down.\\n[2019-08-09T12:06:37,826][INFO ][filewatch.observingread  ] QUIT - closing all files and shutting down.\\n[2019-08-09T12:06:38,677][INFO ][logstash.javapipeline    ] Pipeline terminated {\"pipeline.id\"=&gt;\"main\"}\\n[2019-08-09T12:06:38,930][INFO ][logstash.runner          ] Logstash shut down.\\n[2019-08-09T12:07:02,721][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"7.2.0\"}\\n[2019-08-09T12:07:09,604][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[http://127.0.0.1:9200/]}}\\n[2019-08-09T12:07:09,837][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=&gt;\"http://127.0.0.1:9200/\"}\\n[2019-08-09T12:07:09,889][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=&gt;7}\\n[2019-08-09T12:07:09,893][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the `type` event field won\\'t be used to determine the document _type {:es_version=&gt;7}\\n[2019-08-09T12:07:09,924][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=&gt;\"LogStash::Outputs::ElasticSearch\", :hosts=&gt;[\"//127.0.0.1:9200\"]}\\n[2019-08-09T12:07:10,035][INFO ][logstash.outputs.elasticsearch] Using default mapping template\\n[2019-08-09T12:07:10,150][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=&gt;{\"index_patterns\"=&gt;\"logstash-*\", \"version\"=&gt;60001, \"settings\"=&gt;{\"index.refresh_interval\"=&gt;\"5s\", \"number_of_shards\"=&gt;1}, \"mappings\"=&gt;{\"dynamic_templates\"=&gt;[{\"message_field\"=&gt;{\"path_match\"=&gt;\"message\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false}}}, {\"string_fields\"=&gt;{\"match\"=&gt;\"*\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false, \"fields\"=&gt;{\"keyword\"=&gt;{\"type\"=&gt;\"keyword\", \"ignore_above\"=&gt;256}}}}}], \"properties\"=&gt;{\"@timestamp\"=&gt;{\"type\"=&gt;\"date\"}, \"@version\"=&gt;{\"type\"=&gt;\"keyword\"}, \"geoip\"=&gt;{\"dynamic\"=&gt;true, \"properties\"=&gt;{\"ip\"=&gt;{\"type\"=&gt;\"ip\"}, \"location\"=&gt;{\"type\"=&gt;\"geo_point\"}, \"latitude\"=&gt;{\"type\"=&gt;\"half_float\"}, \"longitude\"=&gt;{\"type\"=&gt;\"half_float\"}}}}}}}\\n[2019-08-09T12:07:10,324][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.\\n[2019-08-09T12:07:10,329][INFO ][logstash.javapipeline    ] Starting pipeline {:pipeline_id=&gt;\"main\", \"pipeline.workers\"=&gt;2, \"pipeline.batch.size\"=&gt;125, \"pipeline.batch.delay\"=&gt;50, \"pipeline.max_inflight\"=&gt;250, :thread=&gt;\"#&lt;Thread:0x45e78d92 run&gt;\"}\\n[2019-08-09T12:07:10,946][INFO ][logstash.javapipeline    ] Pipeline started {\"pipeline.id\"=&gt;\"main\"}\\n[2019-08-09T12:07:11,034][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]}\\n[2019-08-09T12:07:11,208][INFO ][filewatch.observingread  ] START, creating Discoverer, Watch with file and sincedb collections\\n[2019-08-09T12:07:11,648][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n\\n\\nI read many people that changing the sincedb_path =&gt; \"NUL\" they could solve their problems as happened here and here (both thanks to Badger). I could not solve my problem this way.\\nAny recomendation?'},\n",
       " {'text_field': \"SOLVED!!!!\\nI have to apologize for this stupid issue. When I had this issue I did chmod 777 to all file I wanted to work with but the problem wasn't solved, so I thought it wasn't permissions problem. Today, I tried to reinstall Logstash and Elasticsearch and didn't solve it, so, while I was crying I though I could give permissions 755 to all directories like:\\n\\nchmod 755 /home\\nchmod 755 /home/admin/\\nchmod 755 /home/admin/envoirments/\\nchmod 755 /home/admin/envoirments/tests/\\nchmod 755 /home/admin/envoirments/tests/03\\nchmod 755 /home/admin/envoirments/tests/03/*\\n\\nAnd this solved the issue, so my conclusion is:\\nIf there is a problem with Logstash to read the files because of permissions, the issue seems to be alerted by: [org.logstash.instrument.metrics.gauge.LazyDelegatingGauge]. I don't know if this is the best way of, but I think that It could be interesting to alert this easy issue with another text (at log).\\nAnyway, thank you Badger for your help and hope this could be useful for someone.\"},\n",
       " {'text_field': 'Hi Everybody\\nI\\'m not much of a programmer, but I\\'m tempted to try to learn to submit some changes to winlogbeat, but would be interested in finding out if I\\'m doing this right.  Newer versions of sysmon added event_id 22, which is a DNS query by a specific process.\\nI\\'ve modified the winlogbeat-sysmon.js file to rename fields based on https://www.elastic.co/guide/en/beats/packetbeat/current/exported-fields-dns.html.  Here\\'s what I\\'ve changed winlogbeat\\\\sysmon\\\\config\\\\winlogbeat-sysmon.js, added:\\n    var event22 = new processor.Chain()\\n    .Add(parseUtcTime)\\n    .Convert({\\n        fields: [\\n            {from: \"winlog.event_data.UtcTime\", to: \"@timestamp\"},\\n            {from: \"winlog.event_data.ProcessGuid\", to: \"process.entity_id\"},\\n            {from: \"winlog.event_data.ProcessId\", to: \"process.pid\", type: \"long\"},\\n            {from: \"winlog.event_data.Image\", to: \"process.executable\"},\\n\\t\\t\\t{from: \"winlog.event_data.QueryName\", to: \"dns.question.name\"},\\n\\t\\t\\t{from: \"winlog.event_data.QueryResults\", to: \"dns.answers\"},\\n        ],\\n        mode: \"rename\",\\n        ignore_missing: true,\\n        fail_on_error: false,\\n    })\\n    .Add(setProcessNameUsingExe)\\n    .Add(removeEmptyEventData)\\n    .Build();\\n-snip-\\n\\t// Event ID 22 - Dns Query\\n\\t22: event22.Run,\\n\\nDoes this look good?  I\\'m not sure if this complies with the ECS format.  I\\'d also like to get this contributed to the github repo, but that might take too much time for me.'},\n",
       " {'text_field': \"Hi Ian, support for event ID 22 was added in https://github.com/elastic/beats/pull/12960. This hasn't been released yet. It uses the fields proposed in https://github.com/elastic/ecs/pull/438.\"},\n",
       " {'text_field': 'Hello,\\nWhat\\'s the trick to get the Kibana Code Application (7.2.1) working with private GitLab repositories? The documentation is rather sparse.\\nRelevant Kibana configuration:\\nxpack.code.ui.enabled: true\\nxpack.code.security.gitHostWhitelist: [ \"github.com\", \"gitlab.com\", \"bitbucket.org\", \"gitbox.apache.org\", \"eclipse.org\", \"acme.com\", \"mydomain\" ]\\nxpack.code.security.gitProtocolWhitelist: [ \"https\", \"git\" ]\\n\\n~\\nCreated /usr/share/kibana/data/code/credentials folder and generated an SSH key with:\\nsudo ssh-keygen -t rsa -b 4096\\nRestarted kibana service. When I try to import a git repository such as\\n\"git@gitlab.mydomain:group/configuration/repo.git\"\\nfrom our on-premise GitLab, I get error: \"Git url protocol is not whitelisted.\"\\nGrtz\\nWillem'},\n",
       " {'text_field': 'And the solution was:\\nxpack.code.security.gitProtocolWhitelist: [ \"https\", \"ssh\" ]\\ninstead of:\\nxpack.code.security.gitProtocolWhitelist: [ \"https\", \"git\" ]'},\n",
       " {'text_field': 'Kibana version: 6.8.1\\nElasticsearch version: 6.8.1\\nAPM Server version: 6.8.1\\nAPM Agent language and version: JS RUM  \"@elastic/apm-rum\": \"^4.3.1\",\\nOriginal install method and version:\\nYarn 1.7.0\\nNode 8.14.0\\nWebpack 4\\nSymfony Webpack Encore 0.26\\nIs there anything special in your setup?\\nI have multiple endpoints in my webpack, so I am trying to upload multiple sourcemaps. For example:\\napp.js\\nvendor.js\\nhome.js\\nsearch.js\\nDescription of the problem including expected versus actual behavior. Please include screenshots (if relevant):\\nI have followed these two guides.\\nhttps://www.elastic.co/guide/en/apm/agent/js-base/4.x/sourcemap.html\\nhttps://www.elastic.co/guide/en/apm/server/6.8/sourcemap-api.html\\'\\nWhen I try to upload a single sourcemap file from cURL I get a 202 Accepted, but the exptected sourcemap index is not created and cannot be found in Kibana (apm-(version)-sourcemaps). The error is also not using the sourcemap.\\ncurl -v -X POST http://apm:8200/assets/v1/sourcemaps   -F service_name=\"test-js\"   -F service_version=\"314d1f2\"   -F bundle_filepath=\"http://www.local.test/build/amp/search.314d1f2.js\"   -F sourcemap=@/var/www/html/symfony/web/build/amp/search.314d1f2.js.map\\nNote: Unnecessary use of -X or --request, POST is already inferred.\\n*   Trying 172.18.0.7...\\n* TCP_NODELAY set\\n* Connected to apm (172.18.0.7) port 8200 (#0)\\n&gt; POST /assets/v1/sourcemaps HTTP/1.1\\n&gt; Host: apm:8200\\n&gt; User-Agent: curl/7.61.1\\n&gt; Accept: */*\\n&gt; Content-Length: 159441\\n&gt; Content-Type: multipart/form-data; boundary=------------------------d63c0681f0af8df2\\n&gt; Expect: 100-continue\\n&gt;\\n&lt; HTTP/1.1 100 Continue\\n&lt; HTTP/1.1 202 Accepted\\n&lt; Date: Fri, 09 Aug 2019 20:13:43 GMT\\n&lt; Content-Length: 0\\n&lt;\\n* Connection #0 to host apm left intact\\n\\nHere is the relevant JSON produced by the APM error caused by a JS error:\\n\\n{\\n      \"exception\": {\\n        \"message\": \"Uncaught TypeError: Cannot read property \\'currentIndex\\' of undefined\",\\n        \"type\": \"TypeError\",\\n        \"stacktrace\": [\\n          {\\n            \"line\": {\\n              \"number\": 1,\\n              \"column\": 8703\\n            },\\n            \"sourcemap\": {\\n              \"updated\": false,\\n              \"error\": \"No Sourcemap available for Service Name: \\'test-js\\', Service Version: \\'314d1f2\\' and Path: \\'http://www.local.test/build/amp/search.314d1f2.js\\'.\"\\n            },\\n            \"filename\": \"build/amp/search.314d1f2.js\",\\n            \"abs_path\": \"http://www.local.test/build/amp/search.314d1f2.js\",\\n            \"function\": \"V\",\\n            \"library_frame\": false,\\n            \"exclude_from_grouping\": false\\n          },\\n          {\\n            \"abs_path\": \"http://www.local.test/build/amp/search.314d1f2.js\",\\n            \"function\": \"&lt;anonymous&gt;\",\\n            \"library_frame\": false,\\n            \"exclude_from_grouping\": false,\\n            \"line\": {\\n              \"number\": 1,\\n              \"column\": 6002\\n            },\\n            \"sourcemap\": {\\n              \"updated\": false,\\n              \"error\": \"No Sourcemap available for Service Name: \\'test-js\\', Service Version: \\'314d1f2\\' and Path: \\'http://www.local.test/build/amp/search.314d1f2.js\\'.\"\\n            },\\n            \"filename\": \"build/amp/search.314d1f2.js\"\\n          }\\n        ]\\n      },\\n      \"culprit\": \"build/amp/search.314d1f2.js\"\\n    },\\n\\n\\nAccording to this docs: https://www.elastic.co/guide/en/apm/server/current/sourcemap-indices.html#sourcemap-example there should be a index:\\n\\nSource maps are stored in separate indices of the format  apm-[version]-sourcemap .\\n\\nBut this is not the case, even though the file is successfully uploaded:\\n\\nIn addition to the cURL command, I have tried following the guide\\'s advice and using Node to upload the sourcemaps.\\nconsole.log(\\'Uploading sourcemaps!\\');\\nvar request = require(\\'request\\');\\nvar path = require(\\'path\\');\\nvar fs = require(\\'fs\\');\\nvar git = require(\\'git-rev-sync\\')\\nvar serviceVersion = git.short(\\'./../\\');\\nvar directory = \\'./web/build/amp\\';\\nvar directoryPath = path.join(__dirname, directory);\\n\\n//GET MAPPED FILENAMES\\nvar urls = [];\\nvar files = fs.readdirSync(directoryPath);\\nfiles.forEach(function(file) {\\n    if (file.includes(serviceVersion) &amp;&amp; file.includes(\\'.map\\')) {\\n        urls.push({\\n            map: file,\\n            url: file.replace(\\'.map\\', \\'\\')\\n        });\\n    }\\n});\\n\\n//SUBMIT FILES TO ELASTIC APM\\nurls.forEach(function(url) {\\n    var formData = {\\n        sourcemap: fs.createReadStream(directoryPath + \\'/\\' + url.map),\\n        service_version: serviceVersion,\\n        bundle_filepath: \\'http://www.local.test/build/amp/\\'+url.url,\\n        service_name: \\'test-js\\'\\n    }\\n    request.post({url: \\'http://apm:8200/assets/v1/sourcemaps\\',formData: formData}, function (err, resp, body) {\\n        if (err) {\\n            console.log(\\'Error while uploading sourcemaps!\\', err)\\n        } else {\\n            console.log(\\'Sourcemap uploaded: http://www.local.test/build/amp/\\'+url.url)\\n        }\\n    })\\n});\\n\\nThe results for this file are as follows, which indicates successful upload.\\n\\nSourcemap uploaded: http://www.local.test/build/amp/vendor.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/log.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/local.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/images.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/runtime.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/blog.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/apm.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/app.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/search.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/videos.314d1f2.js\\nSourcemap uploaded: http://www.local.test/build/amp/home.314d1f2.js\\n\\n\\nI am using these two docker images:\\nimage: docker.elastic.co/elasticsearch/elasticsearch:6.8.1\\nimage: docker.elastic.co/apm/apm-server:6.8.1\\nI don\\'t know where to go from here, or how to further debug why my sourcemaps aren\\'t taking effect even on successful upload.\\nThank you!'},\n",
       " {'text_field': 'Hey @Emirii, welcome to the group.  Based on the screenshot of the available apm indices, it appears all APM data is directed to apm-%{[beat.version]}-%{+yyyy.MM.dd}  index.  I expect the following query will return the sourcemaps you have uploaded, can you confirm?\\nGET apm-*/_search\\n{\\n  \"query\": {\\n    \"term\": {\\n      \"processor.event\": \"sourcemap\"\\n    }\\n  }\\n}\\n\\nIf so, dId you intend for that to be the case?  The recommended index configuration in 6.8.1 for apm-server.yml is:\\noutput.elasticsearch\\n    - index: \"apm-%{[beat.version]}-sourcemap\"                                                                                                                                                                                                \\n      when.contains:\\n        processor.event: \"sourcemap\"\\n\\n    - index: \"apm-%{[beat.version]}-error-%{+yyyy.MM.dd}\"\\n      when.contains:\\n        processor.event: \"error\"\\n\\n    - index: \"apm-%{[beat.version]}-transaction-%{+yyyy.MM.dd}\"\\n      when.contains:\\n        processor.event: \"transaction\"\\n\\n    - index: \"apm-%{[beat.version]}-span-%{+yyyy.MM.dd}\"\\n      when.contains:\\n        processor.event: \"span\"\\n\\n    - index: \"apm-%{[beat.version]}-metric-%{+yyyy.MM.dd}\"\\n      when.contains:\\n        processor.event: \"metric\"\\n\\n    - index: \"apm-%{[beat.version]}-onboarding-%{+yyyy.MM.dd}\"\\n      when.contains:\\n        processor.event: \"onboarding\"\\n\\nIf it is intended, you should configure apm-server to look for sourcemaps in apm-*, replacing the default of apm-*-sourcemap.  This can be done in apm-server.yml with apm-server.rum.source_mapping.index_pattern.'},\n",
       " {'text_field': 'I am new to logstash and have been trying to parse the below json with no luck.\\n{\\n\"stat\": \"user\",\\n\"Ref\": \"USER:[5000,John Smith,3A37332D2659554F9CCE0CD754185269]\",\\n\"statistics\": [\\n    \"0\",\\n    \"\",\\n    \"\",\\n    \"0\",\\n    0,\\n    0,\\n    1,\\n    \"\",\\n    0,\\n    \"\",\\n    \"0\",\\n    1,\\n    \"IDLE\",\\n    \"5017\",\\n    1505\\n]\\n\\n}\\nThe main thing I am trying to do is put each statistic into its own field,  each line under statistic represents a metric from the source system.\\nexample output:\\n  statistic.Status =&gt; \"IDLE\"\\n  statistic.Duration =&gt; \"1505\"\\n\\nCan anyone point me in the right direction to get started with this?'},\n",
       " {'text_field': '    mutate { join =&gt; { \"statistics\" =&gt; \",\" } }\\n    csv { source =&gt; \"statistics\" autogenerate_column_names =&gt; true }\\n\\nObviously you do not actually want autogenerate_column_names, you would use the columns option.'},\n",
       " {'text_field': 'Hi guys please verify whether this is the proper way to use grok ?\\nIf \"i am happy: true\" in [message] {\\ngrok { match =&gt; \"message\" , \"%{GREEDYDATA:myfield\"}\\n}\\nMy intension is to create a new field based on spesific string to be selected in message for visualization later.\\nAny help is really really appreciated as im still new with this elk! Thanks guys'},\n",
       " {'text_field': '\\n\\n\\n Bob94:\\n\\nmutate { addfield , \"myfield\"=&gt; %[message]}\\n\\n\\nYou could do that but the syntax is\\nmutate { add_field =&gt; { \"myfield\" =&gt; \"%{[message]}\" } }'},\n",
       " {'text_field': 'Hello - can you spot any problem with this processor.  when i .\\\\winlogbeat.exe test config i get a good Config OK as expected, however when events that i feel should match the processor occur, the field is not being added.\\nPlease note NONE of these 4 add fields seem to work.  I have other processors above and beneath this code that works fine..\\n    - add_fields:\\n        when.equals.winlog.event_id: \"6005\"\\n        fields:\\n          winlog.event_data.Info: \"Machine starting\"\\n        target: \"\"\\n    - add_fields:\\n        when.equals.winlog.event_id: \"6006\"\\n        fields:\\n          winlog.event_data.Info: \"Machine stopping\"\\n        target: \"\"\\n    - add_fields:\\n        when:\\n          and:\\n            - equals.winlog.event_id: \"1074\"\\n            - equals.winlog.event_data.param5: \"restart\"\\n        fields:\\n          winlog.event_data.Info: \"Machine will restart\"\\n        target: \"\"\\n    - add_fields:\\n        when:\\n          and:\\n            - equals.winlog.event_id: \"1074\"\\n            - equals.winlog.event_data.param5: \"power off\"\\n        fields:\\n          winlog.event_data.Info: \"Machine will shutdown\"\\n        target: \"\"'},\n",
       " {'text_field': 'Fixed, i changed strings to the correct data type and it worked, eg \"6005\" should be 6005, namely a number not a string.'},\n",
       " {'text_field': 'Hi,\\nWe have a requirement wherein we want to combine two fields into one field during reindex.\\nFor Example:\\nOriginal Index:\\nfield1: firstname\\nfield2: lastname\\nAfter reindex:\\nName: firstname lastname\\nI have checked some documents on reindex and _update_by_query but I am not able to find my requirement.Can this be done using reindex? or Is there any other way?\\nThanks,\\nNikhil'},\n",
       " {'text_field': '@Nikhil04 here is a simple example of what I am trying to say. Downside is that the target index will have to create a mapping for the two fields, but you don\\'t have to store them in the document. As I wrote before, the fields can be removed in the reindex script. Here is the example:\\nPUT index_4\\n{\\n  \"mappings\": {\\n    \"properties\": {\\n      \"firstName\":{\\n        \"type\":\"text\"\\n      },\\n      \"lastName\":{\\n        \"type\":\"text\"\\n      }\\n    }\\n  }\\n}\\n\\nPUT index_4/_doc/1\\n{\\n  \"firstName\": \"A\",\\n  \"secondName\": \"B\"\\n}\\n\\nPUT index_5\\n{\\n  \"mappings\": {\\n    \"properties\": {\\n      \"name\":{\\n        \"type\":\"text\"\\n      }\\n    }\\n  }\\n}\\n\\nPOST _reindex\\n{\\n  \"source\": {\\n    \"index\": \"index_4\"\\n  },\\n  \"dest\": {\\n    \"index\": \"index_5\"\\n  },\\n  \"script\": {\\n    \"source\": \"ctx._source.name=ctx._source.firstName + \\' \\' + ctx._source.secondName\"\\n  }\\n}\\n\\nGET index_5/_search'},\n",
       " {'text_field': 'I\\'m running ELK ver 7.1 with kibana.yml that looks like\\n\\nserver.port: 5601\\nserver.host: 0.0.0.0\\nelasticsearch.hosts: [\"https://illumin8.inboxbiz.com:9200\"]\\nelasticsearch.username: \"elastic\"\\nelasticsearch.password: \"elk#123\"\\nserver.ssl.enabled: true\\nserver.ssl.key: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.key\\nserver.ssl.certificate: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.crt\\nelasticsearch.ssl.certificate: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.crt\\nelasticsearch.ssl.key: /root/elasticsearch-7.1.1/config/cert/wildcard_inboxbiz_com.key\\n\\nand elasticsearch.yml is:-\\n\\nhttp.port: 9200\\nnetwork.host: 10.27.0.4\\nnetwork.publish_host: 10.27.0.4\\ndiscovery.seed_hosts: [\"illumin8\"]\\ncluster.initial_master_nodes: [\"illumin8\"]\\nxpack.security.enabled: true\\n#xpack.security.http.ssl.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.key: cert/wildcard_inboxbiz_com.key\\nxpack.security.transport.ssl.certificate: cert/wildcard_inboxbiz_com.crt\\nxpack.security.transport.ssl.certificate_authorities: [ \"cert/DigiCertCA2.pem\" ]\\n\\nerror on elastic-search cluster is\\n\\nUsing REST wrapper from plugin org.elasticsearch.xpack.security.Security\\n[2019-08-10T08:55:08,952][INFO ][o.e.d.DiscoveryModule    ] [illumin8] using discovery type [zen] and seed hosts providers [settings]\\n[2019-08-10T08:55:10,009][INFO ][o.e.n.Node               ] [illumin8] initialized\\n[2019-08-10T08:55:10,010][INFO ][o.e.n.Node               ] [illumin8] starting ...\\n[2019-08-10T08:55:10,143][INFO ][o.e.t.TransportService   ] [illumin8] publish_address {10.27.0.4:9300}, bound_addresses {10.27.0.4:9300}\\n[2019-08-10T08:55:10,151][INFO ][o.e.b.BootstrapChecks    ] [illumin8] bound or publishing to a non-loopback address, enforcing bootstrap checks\\n[2019-08-10T08:55:10,159][INFO ][o.e.c.c.Coordinator      ] [illumin8] cluster UUID [YB1-EZrBRju-sm1h_74oUA]\\n[2019-08-10T08:55:10,430][INFO ][o.e.c.s.MasterService    ] [illumin8] elected-as-master ([1] nodes joined)[{illumin8}{tJl3X-bPRp62gGp_Gtj8UQ}{ug6JR                    nnhQgKiHE03PebdMg}{10.27.0.4}{10.27.0.4:9300}{ml.machine_memory=4058152960, xpack.installed=true, ml.max_open_jobs=20} elect leader, BECOME_MASTER                    TASK_, FINISH_ELECTION], term: 123, version: 3436, reason: master node changed {previous , current [{illumin8}{tJl3X-bPRp62gGp_Gtj8UQ}{ug6JRnnhQ                    gKiHE03PebdMg}{10.27.0.4}{10.27.0.4:9300}{ml.machine_memory=4058152960, xpack.installed=true, ml.max_open_jobs=20}]}\\n[2019-08-10T08:55:12,205][INFO ][o.e.c.s.ClusterApplierService] [illumin8] master node changed {previous , current [{illumin8}{tJl3X-bPRp62gGp_Gtj                    8UQ}{ug6JRnnhQgKiHE03PebdMg}{10.27.0.4}{10.27.0.4:9300}{ml.machine_memory=4058152960, xpack.installed=true, ml.max_open_jobs=20}]}, term: 123, versi                    on: 3436, reason: Publication{term=123, version=3436}\\n[2019-08-10T08:55:12,266][INFO ][o.e.h.AbstractHttpServerTransport] [illumin8] publish_address {10.27.0.4:9200}, bound_addresses {10.27.0.4:9200}\\n[2019-08-10T08:55:12,267][INFO ][o.e.n.Node               ] [illumin8] started\\n[2019-08-10T08:55:12,600][INFO ][o.e.c.s.ClusterSettings  ] [illumin8] updating [xpack.monitoring.collection.enabled] from [false] to [true]\\n[2019-08-10T08:55:13,194][INFO ][o.e.l.LicenseService     ] [illumin8] license [8265ae0a-a6fa-4283-88d6-40ec8f08cc3a] mode [basic] - valid\\n[2019-08-10T08:55:13,205][INFO ][o.e.g.GatewayService     ] [illumin8] recovered [46] indices into cluster_state\\n[2019-08-10T08:55:15,046][INFO ][o.e.x.w.WatcherService   ] [illumin8] reloading watcher, reason [new local watcher shard allocation ids], cancelled                     [0] queued tasks\\n[2019-08-10T08:55:24,020][INFO ][o.e.c.r.a.AllocationService] [illumin8] Cluster health status changed from [RED] to [YELLOW] (reason: [shards start\\n'},\n",
       " {'text_field': 'Problem solved .....\\nwith\\nelasticsearch.hosts: [\"http://10.27.0.4:9200\"]\\nwas https,  i got confused as most examples https://www.elastic.co/guide/en/kibana/current/configuring-tls.html\\nit mentioned https which was misleading since i m using all in one ELK is one single host.\\nAlso, I\\'m not aware of any config on elasticsearch side to have it receive traffic as \"https\"?'},\n",
       " {'text_field': 'Hi\\nWe use a graylog with 10 elastic node backend.\\nES version: elasticsearch-6.6.0 (same problem with prev versions also)\\nSnapshot repo: local \"fs\" repo, NFS mounted to a local folder\\nOS: CentOS 7\\n10 elastic nodes in one cluster, no set roles.\\nThe problem:\\nWe run snapshots every night from a cron job. Before the job, it delete the old ones. So we have a constant disk usage. We make snapshots per index.\\nBut if we update a host OS (ES version not changes), and reboot it (eg. kernel update) the night snapshot jobs starts, and it eat all space, make all snapshots from the begining. So write the data to the disk again.\\nIf we don\\'t update nodes, it can run months without any problem.\\nES starts automatically, but the NFS mounts with hand. So after the restart the ES starts with empty repo. We also tried to restart ES after the mount.\\nI tried to check logs, but I didn\\'t see any errors, but there is a lot of logs, so I\\'m not sure, I\\'m right.\\nThe restarted nodes\\' logs are empty at the time when the snapshots starts.\\nHave you got any idea where to start the debugging? Or have you seen same error before?\\nThanks, Macko'},\n",
       " {'text_field': \"\\n\\n\\n macko003:\\n\\nEXCEPT the restarted server's indices (where one replica shard of an index is on the restarted server), where the elastic start a Full snapshot instead an incremental one.\\n\\n\\nOk, I think I understand a little better.\\n\\n\\n\\n macko003:\\n\\nES starts automatically, but the NFS mounts with hand. So after the restart the ES starts with empty repo. We also tried to restart ES after the mount.\\n\\n\\nThis seems bad. I'm pretty sure strange things will happen if you start Elasticsearch before mounting the repository.\\nCan you try to verify your repository before starting the snapshot? I.e. run POST /_snapshot/$REPOSITORY_NAME/_verify first and check for success.\"},\n",
       " {'text_field': 'I\\'m not sure if this is a version issue, I found a \"plugin\" executable in the bin directory that seems to be the same but I\\'m concerned that all the docs reference logstash-plugin and I cannot seem to find it in a basic / general install.'},\n",
       " {'text_field': 'I believe that in early versions logstash-plugin was just called plugin, so, yes, I think it is a version issue.'},\n",
       " {'text_field': 'I am using Filebeat for supported log formats and using the default index settings and mappings etc..  This is great..\\nNow i also want to send a custom JSON log file, also using Filebeat, but want to send it into it\\'s own new index, i cannot work out how to do this.\\nI am using Elastic Cloud, so i cannot use the indices property in Elasticsearch output.\\nfilebeat.yml\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n    - \"path/to/customlog.json\"\\n\\ncloud.id: \"${CLOUD_ID}\"\\ncloud.auth: \"${ES_PWD}\"'},\n",
       " {'text_field': \"You can still populate the output.elasticsearch.X settings. When using cloud, only the hostname setting will not be useable.\\nBy default ILM will be enabled. Which Beats + ES versions have you running?\\nThe settings output.elasticsearch.index and output.elasticsearch.indices get disabled if ILM is enabled, because ILM requires you to have a write alias.\\nI think it is a good opinion to separate 'setup' and actually running beats. This also allows you to create different users for indexing and setup, and be more strict on permissions for users wantng to index (do not allow them to create/change any resource in the cluster, besides indexing). For some related discussion have a look at this github issue: https://github.com/elastic/beats/issues/10241\\nThe input you have configured does not enable json parsing. Is this on purpose? If you want to enable json parsing (or in general when using another index), you should prepare and install an index template for your new index.\\nHaving some details on what your end result should be, I can maybe give you some more instructions/hints:\\n\\nExactly which Elasticsearch/Beats are you using?\\nWhat would be the index names?\\nDoes your json log follow ECS, or is there a chance of mapping conflicts?\\nDo you want to use ILM for all indices, or not at all?\\n\\ndifferent policies?\\n\\n\\nDo you make use of security (users, roles) ?\\n\"},\n",
       " {'text_field': 'Hi,\\nI am pretty new to ES: I have to create an index which is dot seperated something like,\\n// minions.dev.contents/minion02.retro/noise:2019-08-01T19:42:45.000Z\\nand the contents is\\n//\\n{\\n\"noise\" : \"25\",\\n\"temperature\" : 33,\\n\"pressure\" : 112\\n}\\nwhere noise:2019-08-01Txxxxx is the ID of the document.\\nI am trying to do a SQL search\\nlocalhost:9200/_sql?format=txt\\n{\\n\"query\": \"SELECT * FROM \\'minions.dev.contents\\' WHERE noise &lt; 25\"\\n}\\nI am getting an input mismatch exception\\nmismatched input \\'\\'minions.dev.contents\\'\\' expecting {\\'(\\',\\nI am wondering if this is allowed, is there a better practice for storing groups and subgroup and have unique documents based on timestamp ?!\\nsome tips would be helpful\\nthanks\\nCandy'},\n",
       " {'text_field': 'The issue is, in fact, with how you name the index. When reserved characters are used in the name of the index, you should use double quotes. Single quotes and double quotes have different meaning in SQL. More about this here.\\nSo, your request should look like \"query\": \"SELECT * FROM \\\\\"minions.dev.contents\\\\\" WHERE noise &lt; 25\".'},\n",
       " {'text_field': 'Hi,\\nI have a single node elasticsearch cluster called nuxio.\\nSince a few days ago I suddenly also have a Standalone Cluster in my monitoring tab.\\nElasticsearch, Kibana and Logstash are showing up under the nuxio cluster.\\nBut all Beats (filebeat and winlogbeat) show up under the Standalone Cluster, befor they also showed up under nuxio.\\nWhere did this cluster come from and how do I get rid of it?\\n\\nimage.png1299×627 27 KB\\n'},\n",
       " {'text_field': 'Hi @stephan13360,\\nWelcome to the Elastic community forums!\\nI assume you are running Beats 7.2.0 or higher. Please see the discussion in this post, particularly my explanation in this comment and my proposed fix in this pull request.\\nHope that helps,\\nShaunak'},\n",
       " {'text_field': 'Hi,\\nI am trying to import a CSV file to elasticsearch using logstash, but with no success.\\nWindows 10\\nLogstash 7.3.0/Elasticsearch 7.3.0/Kibana 7.3.0\\n// Config File is as below\\ninput\\n{\\nfile\\n{\\npath =&gt; [\"E:\\\\Softwares\\\\ES\\\\UU_Telemetry.csv\"]\\nstart_position =&gt; \"beginning\"\\nsincedb_path =&gt; \"NUL\"\\n}\\n}\\nfilter\\n{\\ncsv\\n{\\nseparator =&gt; \",\"\\ncolumns =&gt; [\"Site\", \"Process_Area\", \"Asset\", \"Description\", \"Alarms\", \"Alarm_Priority\", \"date_created\"]\\n}\\n mutate {convert =&gt; [\"Alarm_Priority\", \"integer\"]}\\n\\n}\\noutput\\n{\\nelasticsearch\\n{\\nhosts =&gt; [\"localhost:9200\"]\\nindex =&gt; \"UU_Telemetry\"\\n}\\nstdout {}\\n}\\n//config file end\\n//Output @ power shell\\nPS E:\\\\Softwares\\\\ES&gt; .\\\\logstash-7.3.0\\\\bin\\\\logstash -f .\\\\logstash-UU_Tele.conf\\nThread.exclusive is deprecated, use Thread::Mutex\\nSending Logstash logs to E:/Softwares/ES/logstash-7.3.0/logs which is now configured via log4j2.properties\\n[2019-08-11T22:16:05,585][WARN ][logstash.config.source.multilocal] Ignoring the \\'pipelines.yml\\' file because modules or command line options are specified\\n[2019-08-11T22:16:05,599][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"7.3.0\"}\\n[2019-08-11T22:16:06,743][INFO ][org.reflections.Reflections] Reflections took 32 ms to scan 1 urls, producing 19 keys and 39 values\\n[2019-08-11T22:16:08,780][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;, :added=&gt;[http://localhost:9200/]}}\\n[2019-08-11T22:16:09,009][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=&gt;\"http://localhost:9200/\"}\\n[2019-08-11T22:16:09,052][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=&gt;7}\\n[2019-08-11T22:16:09,055][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the type event field won\\'t be used to determine the document _type {:es_version=&gt;7}\\n[2019-08-11T22:16:09,078][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=&gt;\"LogStash::Outputs::ElasticSearch\", :hosts=&gt;[\"//localhost:9200\"]}\\n[2019-08-11T22:16:09,142][INFO ][logstash.outputs.elasticsearch] Using default mapping template\\n[2019-08-11T22:16:09,183][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.\\n[2019-08-11T22:16:09,187][INFO ][logstash.javapipeline    ] Starting pipeline {:pipeline_id=&gt;\"main\", \"pipeline.workers\"=&gt;8, \"pipeline.batch.size\"=&gt;125, \"pipeline.batch.delay\"=&gt;50, \"pipeline.max_inflight\"=&gt;1000, :thread=&gt;\"#&lt;Thread:0x5e61ec0a run&gt;\"}\\n[2019-08-11T22:16:09,227][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=&gt;{\"index_patterns\"=&gt;\"logstash-\", \"version\"=&gt;60001, \"settings\"=&gt;{\"index.refresh_interval\"=&gt;\"5s\", \"number_of_shards\"=&gt;1}, \"mappings\"=&gt;{\"dynamic_templates\"=&gt;[{\"message_field\"=&gt;{\"path_match\"=&gt;\"message\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false}}}, {\"string_fields\"=&gt;{\"match\"=&gt;\"\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false, \"fields\"=&gt;{\"keyword\"=&gt;{\"type\"=&gt;\"keyword\", \"ignore_above\"=&gt;256}}}}}], \"properties\"=&gt;{\"@timestamp\"=&gt;{\"type\"=&gt;\"date\"}, \"@version\"=&gt;{\"type\"=&gt;\"keyword\"}, \"geoip\"=&gt;{\"dynamic\"=&gt;true, \"properties\"=&gt;{\"ip\"=&gt;{\"type\"=&gt;\"ip\"}, \"location\"=&gt;{\"type\"=&gt;\"geo_point\"}, \"latitude\"=&gt;{\"type\"=&gt;\"half_float\"}, \"longitude\"=&gt;{\"type\"=&gt;\"half_float\"}}}}}}}\\n[2019-08-11T22:16:09,809][INFO ][logstash.javapipeline    ] Pipeline started {\"pipeline.id\"=&gt;\"main\"}\\n[2019-08-11T22:16:09,877][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;}\\n[2019-08-11T22:16:09,880][INFO ][filewatch.observingtail  ] START, creating Discoverer, Watch with file and sincedb collections\\n[2019-08-11T22:16:10,230][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n//END\\nPlease help\\nThanks in advance'},\n",
       " {'text_field': 'Hi thanks for try Logstash on windows\\n\\n\\n\\n aahtysh2525:\\n\\npath =&gt; [\"E:\\\\Softwares\\\\ES\\\\UU_Telemetry.csv\"]\\n\\n\\nTry\\npath =&gt; [\"E:/Softwares/ES/UU_Telemetry.csv\"]\\nAlso in the future if you use the &lt;/&gt; button at the top of the editor it will format your code correctly and make it easier for others to help.\\nHope this helps...'},\n",
       " {'text_field': 'I some have source with generator Here\\n    generator {\\n    lines =&gt; [\\n      \\'{\\n        \"macAddr\": \"d880397a3b0f\",\\n        \"sensors\": [\\n          {\"sensor-id\": \"s01\", \"sensor-val\": 31},\\n          {\"sensor-id\": \"s02\", \"sensor-val\": 32},\\n          {\"sensor-id\": \"s03\", \"sensor-val\": 33}\\n        ],\\n        \"@timestamp\": \"2019-08-08T12:23:34Z\"\\n      }\\'\\n    ]\\n    count =&gt; 1\\n    codec =&gt; json { }\\n  }\\n\\nand Filter is here\\njdbc_static {\\n    loaders =&gt; [\\n      {\\n        id =&gt; \"remote-device\"\\n        query =&gt; \"select deviceId, deviceName, spotId, spotName, locGbCode, locGbName, ipAddr, macAddr from rs_temp_device_t\"\\n        local_table =&gt; \"localDevice\"\\n      },\\n      {\\n        id =&gt; \"remote-sensor\"\\n        query =&gt; \"select deviceId, sensorId, sensorName, sensorUnit from rs_temp_device_sensor_t\"\\n        local_table =&gt; \"localSensor\"\\n      }\\n    ]\\n\\n    local_db_objects =&gt; [\\n      {\\n        name =&gt; \"localDevice\"\\n        index_columns =&gt; [\"deviceId\", \"macAddr\"]\\n        columns =&gt; [\\n          [\"deviceId\", \"varchar(10)\"],\\n          [\"deviceName\", \"varchar(20)\"],\\n          [\"spotId\", \"varchar(20)\"],\\n          [\"spotName\", \"varchar(20)\"],\\n          [\"locGbCode\", \"varchar(20)\"],\\n          [\"locGbName\", \"varchar(20)\"],\\n          [\"ipAddr\", \"varchar(20)\"],\\n          [\"macAddr\", \"varchar(20)\"]\\n        ]\\n      },\\n      {\\n        name =&gt; \"localSensor\"\\n        index_columns =&gt; [\"deviceId\", \"sensorId\"]\\n        columns =&gt; [\\n          [\"deviceId\", \"varchar(10)\"],\\n          [\"sensorId\", \"varchar(20)\"],\\n          [\"sensorName\", \"varchar(20)\"],\\n          [\"sensorUnit\", \"varchar(20)\"]\\n        ]\\n      }\\n    ]\\n\\n    local_lookups =&gt; [\\n        {\\n          query =&gt; \"select deviceId, deviceName, spotId, spotName, locGbCode, locGbName, ipAddr from localDevice where macAddr = :macAddr\"\\n          parameters =&gt; { macAddr =&gt; \"[macAddr]\" }\\n          target =&gt; \"device\"\\n        },\\n        {\\n          query =&gt; \"select sensorName, sensorUnit from localSensor where sensorId = :sensorId\"\\n          parameters =&gt; { sensorId =&gt; \"[ #sensors &gt; sensor-id# ]\"}\\n          target =&gt; \"sensor\"\\n        }\\n    ]\\n\\n    add_field =&gt; { deviceId =&gt; \"%{[device][0][deviceid]}\" }\\n    add_field =&gt; { deviceName =&gt; \"%{[device][0][devicename]}\" }\\n    add_field =&gt; { spotId =&gt; \"%{[device][0][spotid]}\" }\\n    add_field =&gt; { spotName =&gt; \"%{[device][0][spotname]}\" }\\n    add_field =&gt; { locGbCode =&gt; \"%{[device][0][locgbcode]}\" }\\n    add_field =&gt; { locGbName =&gt; \"%{[device][0][locgbname]}\" }\\n    add_field =&gt; { ipAddr =&gt; \"%{[device][0][ipaddr]}\" }\\n\\n    add_field =&gt; { sensorName =&gt; \"%{[sensor][0][sensorname]}\" }\\n    add_field =&gt; { sensorUnit =&gt; \"%{[sensor][0][sensorunit]}\" }\\n\\n    remove_field =&gt; [\"device\", \"sensor\"]\\n    # {jdbc_settings....}\\n}\\n\\ncan I lookup sensors(nested list) name and unit data from localSensor table with individual sensor-id?'},\n",
       " {'text_field': 'There are a few ideas to consider.\\n-- using split\\n\\nUse beats parallelism to distribute work across 10 (example) LS pipelines. Only one of the pipelines needs the loaders and local_db_objects defined as the jdbc_static local DB is single instance JVM wide and will be initialised once when the pipelines are compiled. Each pipeline\\'s jdbc_static filter will need the local_lookups defined.\\nStore the 1 sensor for 1 device documents in a separate elasticsearch index.\\nUse elasticsearch aggregations to build a new index holding documents, each containing all sensors for one device. Watcher can do this.\\n\\n-- not using split.\\n\\nThink about using a single second query (target =&gt; \"sensors\") that uses the deviceId, added by the first query to return all sensor rows as a multiple element array for that device then use add_field to do this (or a ruby filter to loop through the [sensors] field array value):\\n\\n    add_field =&gt; { \"[sensor-name-1]\" =&gt; \"%{[sensors][0][sensorname]}\" }\\n    add_field =&gt; { \"[sensor-unit-1]\" =&gt; \"%{[sensors][0][sensorunit]}\" }\\n    add_field =&gt; { \"[sensor-name-2]\" =&gt; \"%{[sensors][1][sensorname]}\" }\\n    add_field =&gt; { \"[sensor-unit-2]\" =&gt; \"%{[sensors][1][sensorunit]}\" }\\n...\\n    add_field =&gt; { \"[sensor-name-10]\" =&gt; \"%{[sensors][9][sensorname]}\" }\\n    add_field =&gt; { \"[sensor-unit-10]\" =&gt; \"%{[sensors][9][sensorunit]}\" }\\n\\nActually, because add_field is a multi-valued Hash data data type you can use just one declaration.\\nI modified my test config not to use split:\\ninput {\\n  generator {\\n    lines =&gt; [\\n      \\'{\\n        \"mac_addr\": \"d880397a3b0f\",\\n        \"sensors\": [\\n          {\"sensor_id\": \"s01\", \"sensor_val\": 31},\\n          {\"sensor_id\": \"s02\", \"sensor_val\": 32},\\n          {\"sensor_id\": \"s03\", \"sensor_val\": 33}\\n        ],\\n        \"@timestamp\": \"2019-08-08T12:23:34Z\"\\n      }\\'\\n    ]\\n    count =&gt; 1\\n    codec =&gt; json { }\\n  }\\n}\\n\\nfilter {\\n  # split { field =&gt; \"[sensors]\" }\\n  jdbc_static {\\n    loaders =&gt; [\\n            {\\n        id =&gt; \"remote-device\"\\n        query =&gt; \\'select \"device_id\", \"device_name\", \"spot_id\", \"spot_name\", \"loc_gb_code\", \"loc_gb_name\", \"ip_addr\", \"mac_addr\" from rs_temp_device_t\\'\\n        local_table =&gt; \"localDevice\"\\n      },\\n      {\\n        id =&gt; \"remote-sensor\"\\n        query =&gt; \\'select \"device_id\", \"sensor_id\", \"sensor_name\", \"sensor_unit\" from rs_temp_device_sensor_t\\'\\n        local_table =&gt; \"localSensor\"\\n      }\\n    ]\\n    local_db_objects =&gt; [\\n      {\\n        name =&gt; \"localDevice\"\\n        index_columns =&gt; [\"device_id\", \"mac_addr\"]\\n        columns =&gt; [\\n          [\"device_id\", \"varchar(20)\"],\\n          [\"device_name\", \"varchar(20)\"],\\n          [\"spot_id\", \"varchar(20)\"],\\n          [\"spot_name\", \"varchar(20)\"],\\n          [\"loc_gb_code\", \"varchar(20)\"],\\n          [\"loc_gb_name\", \"varchar(20)\"],\\n          [\"ip_addr\", \"varchar(20)\"],\\n          [\"mac_addr\", \"varchar(20)\"]\\n        ]\\n      },\\n      {\\n        name =&gt; \"localSensor\"\\n        index_columns =&gt; [\"device_id\", \"sensor_id\"]\\n        columns =&gt; [\\n          [\"device_id\", \"varchar(20)\"],\\n          [\"sensor_id\", \"varchar(20)\"],\\n          [\"sensor_name\", \"varchar(20)\"],\\n          [\"sensor_unit\", \"varchar(20)\"]\\n        ]\\n      }\\n    ]\\n    local_lookups =&gt; [\\n      {\\n        query =&gt; \"SELECT device_id, device_name, spot_id\\n        , spot_name, loc_gb_code, loc_gb_name, ip_addr FROM localDevice WHERE mac_addr = :macaddr\"\\n        parameters =&gt; {\\n          \"macaddr\" =&gt; \"[mac_addr]\"\\n        }\\n        target =&gt; \"device\"\\n      },\\n      {\\n        query =&gt; \"SELECT sensor_name, sensor_unit FROM localSensor WHERE device_id = :devid\"\\n        parameters =&gt; {\\n          \"devid\" =&gt; \"[device][0][device_id]\"\\n        }\\n        target =&gt; \"sensor_info\"\\n      }\\n    ]\\n    add_field =&gt; {\\n        \"[sensors][0][sensor_name]\" =&gt; \"[sensor_info][0][sensor_name]\"\\n        \"[sensors][1][sensor_name]\" =&gt; \"[sensor_info][1][sensor_name]\"\\n        \"[sensors][2][sensor_name]\" =&gt; \"[sensor_info][2][sensor_name]\"\\n        \"[sensors][0][sensor_unit]\" =&gt; \"[sensor_info][0][sensor_unit]\"\\n        \"[sensors][1][sensor_unit]\" =&gt; \"[sensor_info][1][sensor_unit]\"\\n        \"[sensors][2][sensor_unit]\" =&gt; \"[sensor_info][2][sensor_unit]\"\\n      }\\n    remove_field =&gt; [\"sensor_info\"]\\n    staging_directory =&gt; \"/elastic/tmp/logstash-6.3.2/data/jdbc_static/import_data\"\\n    jdbc_user =&gt; \"logstash\"\\n    jdbc_password =&gt; \"logstash??\"\\n    jdbc_driver_class =&gt; \"org.postgresql.Driver\"\\n    jdbc_driver_library =&gt; \"/elastic/tmp/postgresql-42.1.4.jar\"\\n    jdbc_connection_string =&gt; \"jdbc:postgresql://localhost:5432/ls_test_2\"\\n  }\\n}\\noutput {\\n  stdout {\\n    codec =&gt; rubydebug {metadata =&gt; true}\\n  }\\n}\\n\\nResults:\\n{\\n      \"sequence\" =&gt; 0,\\n       \"sensors\" =&gt; [\\n        [0] {\\n              \"sensor_id\" =&gt; \"s01\",\\n             \"sensor_val\" =&gt; 31,\\n            \"sensor_unit\" =&gt; \"[sensor_info][0][sensor_unit]\",\\n            \"sensor_name\" =&gt; \"[sensor_info][0][sensor_name]\"\\n        },\\n        [1] {\\n              \"sensor_id\" =&gt; \"s02\",\\n             \"sensor_val\" =&gt; 32,\\n            \"sensor_unit\" =&gt; \"[sensor_info][1][sensor_unit]\",\\n            \"sensor_name\" =&gt; \"[sensor_info][1][sensor_name]\"\\n        },\\n        [2] {\\n              \"sensor_id\" =&gt; \"s03\",\\n             \"sensor_val\" =&gt; 33,\\n            \"sensor_unit\" =&gt; \"[sensor_info][2][sensor_unit]\",\\n            \"sensor_name\" =&gt; \"[sensor_info][2][sensor_name]\"\\n        }\\n    ],\\n    \"@timestamp\" =&gt; 2019-08-08T12:23:34.000Z,\\n      \"mac_addr\" =&gt; \"d880397a3b0f\",\\n      \"@version\" =&gt; \"1\",\\n          \"host\" =&gt; \"Elastics-MacBook-Pro.local\",\\n        \"device\" =&gt; [\\n        [0] {\\n            \"device_name\" =&gt; \"Device 1\",\\n            \"loc_gb_code\" =&gt; \"code01\",\\n              \"device_id\" =&gt; \"dev01\",\\n            \"loc_gb_name\" =&gt; \"Code 1\",\\n                \"spot_id\" =&gt; \"spot01\",\\n                \"ip_addr\" =&gt; \"1.1.1.1\",\\n              \"spot_name\" =&gt; \"Spot 1\"\\n        }\\n    ]\\n}\\n\\nNOTE: You can also use the ruby filter to do the add_field in case there is no 1 to 1 correlation, for example [sensors][0][sensor_id] is not \"s01\".'},\n",
       " {'text_field': 'Hello Community,\\ni\\'m still pretty new to Elastic-Stack.\\nI got a Cluster of 1 Master and 2 Nodes set up, and already got all my Beat-Agents deployed on the servers. Now i set up beat nodes on my Cluster aswell, to monitor any activity on it aswell (I use elk as a Security Setup). My Problem is that the fields which are filtered now are rudimentary.\\nThe field \"message\" holds a few informations i actually want to filter by. How do i get my ES to put certain values in new fields? I found the fields.yml file, but i\\'m not sure how to use it, just write stuff somewhere? In which Syntax? Do i have to watch out for something?\\nInformation in the message field looks like :\\ntype=CRYPTO_KEY_USER msg=audit(1111111111.111:1111111): pid=111111 uid=0 auid=1111111111 ses=111111111 subj=system:\\n\\nLet\\'s say in this example i want the Information\\ntype=CRYPTO_KEY_USER\\nseperated into an extra field, by which i can Filter in Kibana\\nThanks in advance!\\nMo'},\n",
       " {'text_field': 'Filebeat just collects log lines and sends each log line as an event. You need to parse the logs.\\nYou can use filebeat modules, if you find one that matches the logs/service you want to monitor: https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-modules.html\\nOtherwise you need to parse the logs yourself. Using an Ingest Node pipeline (Elasticsearch feature), you can create a pipeline in Elasticsearch doing the parsing for you (e.g. dissect, grok, or key value parser). You can also try to apply the parsing in filebeat using the dissect processor.'},\n",
       " {'text_field': 'I have a form where user can select their favorite food. Options like below -\\nOptions\\nVegetables\\n\\t- Tomato\\n\\t- Pumpkin\\n\\t- Carrot\\nFruits\\n\\t- Apple\\n\\t- Banana\\n\\t- Orange\\nMeats and Poultry\\n \\t- Lean meats\\n \\t\\t- Beef\\n \\t\\t- Lamb\\n \\t\\t- Pork\\n\\t- Poultry\\n\\t\\t- Chicken\\n\\t\\t- Turkey\\n\\t- Fish and Seafood\\n\\t\\t- Fish\\n\\t\\t- Prawns\\n\\t\\t- Lobster\\n\\t- Eggs\\n\\t\\t- Chicken eggs\\n \\t\\t- Duck eggs\\n\\nUsers can select any number of option from any level.\\nWhat should be the ideal Engine Schema in App Search and how to manage search query in API?\\nReference from: Hierarchical Facets Guide | Swiftype Documentation\\nThank you.'},\n",
       " {'text_field': 'Abhishek -- thank  you!\\nI appreciate the detail.\\nThe Hierarchical Facets guide mentions this at the end:\\n\\nIf we want the facet menu to include the correct counts for all dimensions, like so:\\nCalifornia (8)\\n  World Heritage Site (6)\\n    Open (4)\\n    Closed (2)\\nAlaska (5)\\n\\nWe would use multiple queries:\\n\\nRequest  dimension1  facets.\\nRequest  dimension2  facets, filter on  dimension1 .\\nRequest  dimension3  facets, filter on  dimension1  and  dimension2 .\\n\\n\\nYour flow looks like so:\\n\\nRequest dimension1 facets.\\nRequest dimension2 facets, filter on dimension1.\\n...\\n\\nIt is returning the correct facet counts for dimension2.\\nThe filter will not impact the facet -- it will impact the result set.\\nSo, anytime you are faceting on dimension2, it will return all of dimension2.\\n\\nRemember that you have full control over the schema.\\nFor example, you could break up dimension2 and establish separate hierarchies.\\n\\nVegetables are a 2 dimensional hierarchy.\\nFruits are a 2 dimensional hierarchy.\\nMeats and Poultry is a 3 dimensional hierarchy.\\n\\nIf your schema were more \"broken apart\", such as this:\\n\"id\": text,\\n\"email\": text,\\n\"dimension1\": text,\\n\"dimension2_veg\": text\\n\"dimension2_fruit\": text,\\n\"dimension2_meats\": text,\\n\"dimension3\":  ...\\n\\nYou would be able to...\\n\\nmore precisely apply single queries\\napply multiple queries when you need to represent multiple dimensions\\n\\nRequest 1:\\n{\\n  \"query\": \"\",\\n  \"facets\": {\\n    \"dimension1\": [{ \"type\": \"value\"}]\\n  }\\n}\\n\\nWill return the count of Vegetables/Fruits/Meats&amp;Poultry:\\nVegetables (8)\\nFruits (5)\\nMeats &amp; Poultry (5)\\n\\nRequest 2:\\n{\\n  \"query\": \"\",\\n  \"facets\": {\\n    \"dimension2_veg\": [{ \"type\": \"value\"}]\\n  }\\n}\\n\\nWill return:\\nVegetables (8)\\n   Etc (1)\\n   Etc (2)\\nFruits (5)\\nMeats &amp; Poultry (5)\\n\\nRequest 3:\\n{\\n  \"query\": \"\",\\n  \"facets\": {\\n    \"dimension2_fruits\": [{ \"type\": \"value\"}]\\n  }\\n}\\n\\nWill return:\\nVegetables (x)\\n   Etc (x)\\n   Etc (x)\\nFruits (x)\\n   Etc (x)\\n   Etc (x)\\nMeats &amp; Poultry (x)\\n\\n\"Dimension\" is just an example placeholder that can be replaced with your own symbols. \\nThis isn\\'t a \"correct answer\" for your case, but a demonstration that there is great flexibility.\\nAnother thing that might help is our open source library, Search UI.\\nSearch UI can help build search from a great set of foundational components, like faceting.\\nI hope this is helpful,\\nKellen'},\n",
       " {'text_field': 'Hello,\\nI am trying to crated new field in indices by recalculate duration based on response and request timestamps.\\nI have this document:\\n{\\n    \"request\" : {\\n      \"time\" : \"2019-08-04T20:02:15.459Z\"\\n    },\\n    \"response\" : {\\n      \"time\" : \"2019-08-04T20:04:03.009Z\"\\n    }\\n  }\\n\\nMapping is following:\\n{\\n  \"my_index\" : {\\n    \"mappings\" : {\\n      \"properties\" : {\\n        \"request\" : {\\n          \"properties\" : {\\n            \"time\" : {\\n              \"type\" : \"date\"\\n            }\\n          }\\n        },\\n        \"response\" : {\\n          \"properties\" : {\\n            \"time\" : {\\n              \"type\" : \"date\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nI am trying to run this script:\\nPOST my_index/_update_by_query\\n{\\n    \"script\" : {\\n    \"inline\": \"ctx._source.duration = (new SimpleDateFormat(\\\\\"yyyy-MM-dd\\'T\\'HH:mm:ss.SSSZ\\\\\").parse(ctx._source.response.time).getTime() - new SimpleDateFormat(\\\\\"yyyy-MM-dd\\'T\\'HH:mm:ss.SSSZ\\\\\").parse(ctx._source.request.time).getTime())\"\\n  },\\n  \"query\": { \"match_all\": {} }\\n}\\n\\nbut getting error:\\n\"script\": \"ctx._source.duration = (new SimpleDateFormat(\\\\\"yyyy-MM-dd\\'T\\'HH:mm:ss.SSSZ\\\\\").parse(ctx._source.response.time).getTime() - new SimpleDateFormat(\\\\\"yyyy-MM-dd\\'T\\'HH:mm:ss.SSSZ\\\\\").parse(ctx._source.request.time).getTime())\",\\n\"lang\": \"painless\",\\n\"caused_by\": {\\n  \"type\": \"null_pointer_exception\",\\n  \"reason\": null\\n}\\n\\nI do not see there any problem or did I overlooked something?\\nThank you for help\\nCheers, Reddy'},\n",
       " {'text_field': 'how about this\\nDELETE test\\n\\nPUT test/_doc/1?refresh\\n{\\n  \"request\": {\\n    \"time\": \"2019-08-04T20:02:15.459Z\"\\n  },\\n  \"response\": {\\n    \"time\": \"2019-08-04T20:04:03.009Z\"\\n  }\\n}\\n\\nPOST test/_update_by_query\\n{\\n    \"script\" : {\\n      \"lang\" : \"painless\",\\n    \"source\": \"ctx._source.duration_in_ms = ZonedDateTime.parse(ctx._source.response.time).toInstant().toEpochMilli() - ZonedDateTime.parse(ctx._source.request.time).toInstant().toEpochMilli()\"\\n  }\\n}\\n\\nGET test/_doc/1\\n'},\n",
       " {'text_field': 'Hello Team,\\nI am using Kibana Custom plugin and in that I am showing results in EUITable. But the time field is showing datetime in UTC but in Kibana time field shows based on the local/browser time.\\nCould you please help me understand how to achieve the same functionality in custom plugin.\\nKibana version : 6.7\\nThank you for your help and support.\\nAditya'},\n",
       " {'text_field': 'Hello Team,\\nI was able to achieve this by setting datType property in EUIBasicTable column.\\nCould you please help me understand how can we set the date format.\\nThank you for your help and support\\nAditya'},\n",
       " {'text_field': 'Hi,\\nI was using metricbeat 6.5.4 in my dev system before I upgraded to 7.3.0.\\nMetricbeat is not pushing directly elasticsearch, but is the probes it takes to filesystem. So filesystem is my first buffer layer for metricbeats events. Then these logs are fetched by filebeat, shipped to redis where they are fetched by logstash, which is pushing to elasticsearch.\\nFilebeat is simply adding a field \"logType\", and using this logType as redis key.\\nThe logstash configuration is quite trivial:\\ninput\\n{\\n\\tredis\\n\\t{\\n\\t\\tdata_type =&gt; \"list\"\\n\\t\\tdb \\t\\t\\t\\t=&gt; \"${REDIS_DB}\"\\n\\t\\thost \\t\\t\\t=&gt; \"${REDIS_HOST}\"\\n\\t\\tport\\t\\t\\t=&gt; \"${REDIS_PORT}\"\\n\\t\\tkey \\t\\t\\t=&gt; \"metricbeat\"\\n\\t}\\n}\\n\\nfilter\\n{\\n\\n\\tjson\\n\\t{\\n\\t\\tid =&gt; \"json\"\\n\\t\\tsource =&gt; \"message\"\\n\\t}\\n\\t\\n\\t# delete message if no _jsonparsefailure\\n\\tif (\"_jsonparsefailure\" not in [tags])\\n\\t{\\n\\t\\tmutate\\n\\t\\t{\\n\\t\\t\\tremove_field =&gt; [\\'message\\']\\n\\t\\t}\\n\\t}\\n}\\n\\n###\\n# this filter is just setting the index name and setting the index rotation time\\n###\\nfilter\\n{\\n  mutate\\n  {\\n    # as suffix you can use following options: SUFFIX_WEEKLY, SUFFIX_DAILY, SUFFIX_MONTHLY\\n    # !!! NOT USING PREFIX plx_ FOR METRICBEAT !!!!\\n    add_field =&gt; { \"[@metadata][indexName]\" =&gt; \"%{[logType]}-${SUFFIX_WEEKLY}\" }\\n  }\\n}\\n\\n\\noutput\\n{\\n\\telasticsearch\\n\\t{\\n\\t\\thosts \\t\\t=&gt; [\"${ES_HOST}:${ES_PORT}\"]\\n\\t\\tssl \\t\\t\\t=&gt; \"${USE_ES_SSL}\"\\n\\t\\tcacert\\t\\t=&gt; \"${ES_CA_CERT_PATH}\"\\n\\n\\t\\t# credentials are fetched from envrionment or logstash-keystore\\n\\n\\t\\tuser\\t\\t\\t=&gt; \"${LOGSTASH_USER}\"\\n\\t\\tpassword\\t=&gt; \"${LOGSTASH_PASSWORD}\"\\n\\n\\t\\tindex\\t\\t\\t=&gt; \"%{[@metadata][indexName]}\"\\n\\t}\\n}\\n\\nAfter Upgrading I exported metricbeat 7.3.0 index template and added it to elasticsearch via Kibana\\'s DevTools.\\nI now have these templates:\\n.ml-meta                    [.ml-meta]                    0          7030099\\n.monitoring-alerts-7        [.monitoring-alerts-7]        0          7000199\\n.ml-config                  [.ml-config]                  0          7030099\\n.watches                    [.watches*]                   2147483647 \\n.ml-notifications           [.ml-notifications]           0          7030099\\n.data-frame-notifications-1 [.data-frame-notifications-*] 0          7030099\\n.watch-history-10           [.watcher-history-10*]        2147483647 \\n.ml-anomalies-              [.ml-anomalies-*]             0          7030099\\nmetricbeat-7.3.0            [metricbeat-*]                1          \\n.watch-history-9            [.watcher-history-9*]         2147483647 \\n.logstash-management        [.logstash]                   0          \\n.kibana_task_manager        [.kibana_task_manager]        0          7030099\\n.management-beats           [.management-beats]           0          70000\\n.ml-state                   [.ml-state*]                  0          7030099\\n.monitoring-kibana          [.monitoring-kibana-7-*]      0          7000199\\nplx                         [plx*]                        0          \\n.monitoring-beats           [.monitoring-beats-7-*]       0          7000199\\n.monitoring-es              [.monitoring-es-7-*]          0          7000199\\nlogstash                    [logstash-*]                  0          60001\\n.monitoring-logstash        [.monitoring-logstash-7-*]    0          7000199\\n.triggered_watches          [.triggered_watches*]         2147483647 \\n.data-frame-internal-1      [.data-frame-internal-1]      0          7030099\\n\\nI also deleted all metricbeat indices, so that there shouldn\\'t be any type mismatch. All metricbeat instances are updated to 7.3.0, so no cross version.\\nmetricbeat-Data is getting stuck in logstash:\\n[2019-08-12T12:36:33,468][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"metricbeat-2019.w33\", :_type=&gt;\"_doc\", :routing=&gt;nil}, #&lt;LogStash::Event:0x6f1e9013&gt;], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"metricbeat-2019.w33\", \"_type\"=&gt;\"_doc\", \"_id\"=&gt;\"xIfThWwBNHp2V5M6a9Pb\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"object mapping for [source] tried to parse field [source] as object, but found a concrete value\"}}}}\\nAny Idea what is the cause of this trouble?\\nThanks, Andreas'},\n",
       " {'text_field': 'Great, after a lot of searching I found the root cause.\\nDon\\'t know for which module, but metricbeat is declaring an object source in the mapping:\\n\\nimage.png529×681 25.5 KB\\n\\nBut I am using filebeat for shipping the saved metricbeat logfile. Filebeat itself is creating a field source as string. That filebeat field is conflicting with the optional source object of metricbeat.\\nSo the issue should only occur if I use filebeat to ship metricbeat probe logs!\\nSo I came to the following solution:\\nFor metrics I do not really have a benefit if I know, which logfile was read.  So get rid of it.\\nIn the json parsing\\nI changed my filter for parsing the json message to the following:\\nfilter\\n{\\n\\n\\t# filebeat is exporting a field source as string, metricbeat has a source as object in mapping. This seems to collide.\\n\\t# We don\\'t need metricbeat source file, so we delete the filebeat field BEFORE parsing the json message\\n\\n\\tmutate\\n\\t{\\n\\t\\tremove_field =&gt; [ \"source\" ]\\n\\t}\\n\\n\\tjson\\n\\t{\\n\\t\\tid =&gt; \"json\"\\n\\t\\tsource =&gt; \"message\"\\n\\t}\\n\\n\\n\\t# delete message if no _jsonparsefailure\\n\\tif (\"_jsonparsefailure\" not in [tags])\\n\\t{\\n\\t\\tmutate\\n\\t\\t{\\n\\t\\t\\tremove_field =&gt; [\\'message\\']\\n\\t\\t}\\n\\t}\\n}\\n\\n-&gt; Maybe it helps someone else \\nPS: renamed the title to be closer to the root cause.\\nPPS: before upgrading  I missed to add metricbeat mapping templates. So I think that was the point why I did not face the issue in lower version.'},\n",
       " {'text_field': \"Hello,\\nI am injesting some java logs via filebeat. I see the logs being sent to logstash however on the stdout filter I don't see anything being processed by the logstash. But logstash does not throw any errors either. Logstash config can be found in here: https://pastebin.com/5asGqvYk\\nAny idea how to debug this? I've tried taking tcpdump on the logstash side but the information is not really human readable so I don't know if the data sent to logstash from filebeat is correct or not.\\nThank you in advance\"},\n",
       " {'text_field': 'Great, I found out that the conditionals in the output were wrong. Than you for helping to fix this.'},\n",
       " {'text_field': \"Hi,\\nI am having an es database of products (tires and rims) in my ecommerce solution. All my tires are marked with a quality such as good, great, best. The reason why tires are marked with this term is to bundle rims with different tires in packages based on quality.\\nI have an external API which tells me which tires SKU's that is fitting the the rim selected. I would like to search the database for tires matching the list of tire SKU's returned by the external API, but I only want to select 1 tire out of all the three quality terms, as I am only showcasing 3 packages.\\nI want to filter tires that are unavailable obviously initially.\\nIs it possible to have elasticsearch return only 1 result from each quality term?\"},\n",
       " {'text_field': 'take a look at field collapsing, that might solve this particular use-case'},\n",
       " {'text_field': 'If I disable the xpack.telemetry.enabled setting I can\\'t create or edit roles in the kibana ui.\\nThe /app/kibana#/management/security/roles site is accessable,\\nbut /app/kibana#/management/security/roles/edit is not.\\nTested with Version 7.3.0 on Windows 10.\\nNo Warning or Error logs.\\nElasticsearch\\'s roles API works fine.\\nSteps:\\nDownload and extract es and kibana zips.\\nelasticsearch.yml:\\nxpack.security.enabled: true\\nStart es\\nelasticsearch-setup-passwords auto\\nkibana.yml:\\nelasticsearch.username: \"kibana\"\\nelasticsearch.password: \"ThePwd\"\\nxpack.telemetry.enabled: false\\nStart kibana\\nLogin -&gt; Management -&gt; Roles\\nClick on \"Create Role\" -&gt; Nothing happens'},\n",
       " {'text_field': 'Able to replicate on 7.3 and master by Brandon. @Philos - its a bug. Can you please file it https://github.com/elastic/kibana/issues/new?template=Bug_report.md\\nNever mind, I already filed it: https://github.com/elastic/kibana/issues/43208 our team will look into it.\\nThank you\\nRashmi'},\n",
       " {'text_field': \"Hi All,\\ni have a log which contains the message as below\\nmessage ProName = 'Core:Worthiness:Price'\\nas of now i'm using the %{CISCO_REASON}= %{QS:proname} grok to extact it, but getting the whole Core:Worthiness:Price as the output, how can i separate each of them like Core:Worthiness:Price to Core,Worthiness,Price\\nThanks\"},\n",
       " {'text_field': 'If you want that string split into an array use a mutate+split filter.'},\n",
       " {'text_field': 'Hi ,\\nI am running elasticdiagnostic tool and facing this issue ,Can someone help?\\nsupport-diagnostics-7.0.8]$ ./diagnostics.sh --host ***** --port 9200 -u elastic -p --ssl --noVerify\\nUsing /usr/bin/java as Java Runtime\\nUsing -Xms256m -Xmx2000m  for options.\\n10:20:47 INFO  Processing diagnosticInputs...\\nPassword:\\n10:20:50 INFO  Creating temp directory: /tmp/support-diagnostics-7.0.8/diagnostics\\n10:20:50 INFO  Created temp directory: /tmp/support-diagnostics-7.0.8/diagnostics\\n10:20:50 INFO  Configuring log file.\\n10:20:50 INFO  Writing diagnostic manifest.\\n10:20:50 INFO  Checking for diagnostic version updates.\\n10:20:50 INFO  Getting Elasticsearch Version.\\n10:20:50 INFO  Log close complete.\\nException in thread \"main\" java.lang.RuntimeException: DiagnosticService runtime error\\nat com.elastic.support.diagnostics.chain.DiagnosticChainExec.runDiagnostic(DiagnosticChainExec.java:22)\\nat com.elastic.support.diagnostics.DiagnosticService.exec(DiagnosticService.java:59)\\nat com.elastic.support.diagnostics.DiagnosticApp.main(DiagnosticApp.java:31)'},\n",
       " {'text_field': 'this looks a little bit like this one at the support diagnostics repo. That commit has not yet seen a release though.\\nYou can however clone the github release and run mvn clean package, which will result in a package in the target/ directory.\\nI will also ask if there is the possibility of another release.\\nIf it still does not work, feel free to open an issue in the respective github repo.'},\n",
       " {'text_field': 'Hi,\\nI have a problem with an error invalid byte sequence in UTF-8. Logstash 7.3.0.\\nconf below:\\ninput{\\njdbc{\\njdbc_driver_library =&gt; \"C:\\\\sqljdbc42.jar\"\\njdbc_driver_class =&gt; \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\\njdbc_connection_string =&gt; \"jdbc:sqlserver://MYSERVER;integratedsecurity=true;databaseName=Db;\"\\njdbc_user =&gt; \"user\"\\njdbc_password =&gt; \"\"\\njdbc_validate_connection =&gt; true\\njdbc_pool_timeout =&gt; 10\\njdbc_default_timezone =&gt; \"Etc/UTC\"\\nconnection_retry_attempts =&gt; 3\\nconnection_retry_attempts_wait_time =&gt; 2\\nlowercase_column_names =&gt; false\\nstatement_filepath =&gt; \"C:\\\\sql_1.sql\"\\nschedule =&gt; \"3 * * * * *\"\\ncodec =&gt; line { charset =&gt; \"UTF-8\" }\\n}\\n}\\noutput {\\nelasticsearch {\\nhosts =&gt; [\"http://localhost:9200\"]\\nindex =&gt; \"my_pattern\"\\n}\\n}\\nAn error:\\n{ 2048 rufus-scheduler intercepted an error:\\n2048   job:\\n2048     Rufus::Scheduler::CronJob \"3 * * * * *\" {}\\n2048   error:\\n2048     2048\\n2048     ArgumentError\\n2048     invalid byte sequence in UTF-8\\n2048       org/jruby/RubyString.java:4790:in partition\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/dataset/sql.rb:600:inplaceholder_literal_string_sql_append\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/sql.rb:112:in to_s_append\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/dataset/sql.rb:1236:inliteral_expression_append\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/dataset/sql.rb:89:in literal_append\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/dataset/sql.rb:263:inliteral\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/dataset/sql.rb:1581:in static_sql\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/dataset/sql.rb:236:inselect_sql\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/adapters/utils/emulate_offset_with_row_number.rb:45:in select_sql\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/adapters/shared/mssql.rb:664:inselect_sql\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/sequel-5.22.0/lib/sequel/dataset/actions.rb:152:in each\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/jdbc.rb:257:inperform_query\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/jdbc.rb:229:in execute_statement\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/inputs/jdbc.rb:277:inexecute_query\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/inputs/jdbc.rb:258:in block in run\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:234:indo_call\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:258:in do_trigger\\' 2048 C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:300:inblock in start_work_thread\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:299:in block in start_work_thread\\' 2048 org/jruby/RubyKernel.java:1425:inloop\\'\\n2048       C:/Elastic/Logstash/7.3.0/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:289:in `block in start_work_thread\\'\\n2048   tz:\\n2048     ENV[\\'TZ\\']:\\n2048     Time.now: 2019-08-12 17:39:03 +0200\\n2048   scheduler:\\n2048     object_id: 2012\\n2048     opts:\\n2048       {:max_work_threads=&gt;1}\\n2048       frequency: 0.3\\n2048       scheduler_lock: #Rufus::Scheduler::NullLock:0x7116ed8d\\n2048       trigger_lock: #Rufus::Scheduler::NullLock:0x7ef55089\\n2048     uptime: 4250.077 (1h10m50s77)\\n2048     down?: false\\n2048     threads: 2\\n2048       thread: #Thread:0x46af4b51\\n2048       thread_key: rufus_scheduler_2012\\n2048       work_threads: 1\\n2048         active: 1\\n2048         vacant: 0\\n2048         max_work_threads: 1\\n2048       mutexes: {}\\n2048     jobs: 1\\n2048       at_jobs: 0\\n2048       in_jobs: 0\\n2048       every_jobs: 0\\n2048       interval_jobs: 0\\n2048       cron_jobs: 1\\n2048     running_jobs: 1\\n2048     work_queue: 0\\n} 2048 .\\nWhat is missing?'},\n",
       " {'text_field': 'The jdbc input does not use a codec because the data it is getting from jdbc is already structured.\\nFrom the backtrace, I can see that the error occurs when the SQL statement is being built before it get sent to the server for execution, i.e. on the way up not on the way down.\\nThe most likely problem is with the statement_filepath =&gt; \"C:\\\\sql_1.sql\", the actual file is assumed to be UTF-8 - is it in a different encoding?'},\n",
       " {'text_field': 'Hello,\\nI am running logstash 6.8\\nI am taking all Src_Ip addresses and doing a reverse DNS lookup through the DNS plugin in logstash.\\nLogstash YML\\n  if [src_ip]  {\\n     \\n     \\n      mutate {\\n        add_field =&gt; {\"DNS_Name\" =&gt; \"%{src_ip}\"}\\n      }\\n      dns {\\n        reverse =&gt; [\"DNS_Name\"]\\n        action =&gt; \"replace\"\\n      }\\n  }\\n\\nis there any way for if the IP address does not resolve back to a hostname for that field \"DNS_Name\" to be blank?\\nexample dropping the field names DNS field name from 31.184.249.177 because it is not resolving to anything.\\n'},\n",
       " {'text_field': 'dns {\\n    reverse =&gt; [\"DNS_Name\"]\\n    action =&gt; \"replace\"\\n}\\nif [src_ip] == [DNS_Name] {\\n    mutate { replace =&gt; { \"DNS_Name\" =&gt; \"\" } }\\n}'},\n",
       " {'text_field': 'Hi guys,\\nI\\'m trying to figure out why my filebeat is not sending information to logstach. Instead, it\\'s sending directly to elastic.\\nWhen I go to my kibana there is only the filebeat index patter. When I access the elastich utr :http://172.26.73.113:9200/logstash-*/_search?pretty, the following result is showed (no results):\\n{\\n\\n\"took\" : 0,\\n\"timed_out\" : false,\\n\"_shards\" : {\\n\"total\" : 0,\\n\"successful\" : 0,\\n\"skipped\" : 0,\\n\"failed\" : 0\\n},\\n\"hits\" : {\\n\"total\" : {\\n\"value\" : 0,\\n\"relation\" : \"eq\"\\n},\\n\"max_score\" : 0.0,\\n\"hits\" : \\n}\\n}\\nHere is my Filebeat configuration:\\n#================================ Outputs =====================================\\n\\nConfigure what output to use when sending the data collected by the beat.\\n#-------------------------- Elasticsearch output ------------------------------\\n##output.elasticsearch:\\nArray of hosts to connect to.\\n#hosts: [\"172.26.73.113:9200\"]\\nOptional protocol and basic auth credentials.\\n#protocol: \"https\"\\n#username: \"elastic\"\\n#password: \"changeme\"\\n#----------------------------- Logstash output --------------------------------\\noutput.logstash:\\nThe Logstash hosts\\nhosts: [\"172.26.73.113:5044\"]\\nOptional SSL. By default is off.\\nList of root certificates for HTTPS server verifications\\n#ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\\nhere my logstash configuration file:\\ninput {\\nbeats {\\n    port =&gt; 5044\\n    host =&gt; \"172.26.73.113\"\\n}\\n\\n}\\noutput {\\nelasticsearch {\\nhosts =&gt; [\"172.26.73.113:9200\"]\\nindex =&gt; \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\"\\ndocument_type =&gt; \"%{[@metadata][type]}\"\\n}\\n}'},\n",
       " {'text_field': 'Why are you looking for a logstash-* index. You are setting the index to \"%{[@metadata][beat]}-%{+YYYY.MM.dd}\", which will be filebeat...'},\n",
       " {'text_field': 'Using filebeat 7.3.0 on ubuntu and can\\'t seem to change the index name for any modules that get enabled.\\nAll I want to do is make the indexes that get created be done so monthly names instead of daily.\\nI would expect this to work, but has no change:\\noutput.elasticsearch:\\nindex: \"filebeat-%{[agent.version]}-%{+yyyy.MM}\"\\nTried a lot a different things and it just seems to ignore them.  Any ideas?  Thanks.'},\n",
       " {'text_field': \"The default ILM mode is 'auto'. If beats detect the Elasticsearch cluster you send to has ILM support, then it will be enabled. The Index name filebeat-7.3.0-2019.08.13-000001 is created by ILM, not by beats. In fact beats uses a write alias named filebeat-%{[agent.version]} here.\\nWith ILM enabled, the output.elasticsearch.index setting will be overwritten with the write alias.\\nIn order to disable ILM stop beats, remove the filebeat-* templates, add setup.ilm.enabled: false to your config file and restart.\"},\n",
       " {'text_field': \"It seems that if logstash encounters an exception in the elasticsearch output pipeline it will completely halt processing of any further entries.\\n[2019-08-08T22:14:44,075][WARN ][io.netty.channel.DefaultChannelPipeline] An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\\njava.lang.ClassCastException: java.util.HashMap cannot be cast to java.lang.String\\n        at org.logstash.Event.existingTag(Event.java:398) ~[logstash-core.jar:?]\\n        at org.logstash.Event.tag(Event.java:374) ~[logstash-core.jar:?]\\n        at org.logstash.ext.JrubyEventExtLibrary$RubyEvent.ruby_tag(JrubyEventExtLibrary.java:257) ~[logstash-core.jar:?]\\n        at usr.share.logstash.vendor.bundle.jruby.$2_dot_5_dot_0.gems.logstash_minus_input_minus_beats_minus_5_dot_1_dot_8_minus_java.lib.logstash.inputs.beats.raw_event_transform.RUBY$method$transform$0(/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstas\\nh-input-beats-5.1.8-java/lib/logstash/inputs/beats/raw_event_transform.rb:14) ~[?:?]\\n        at usr.share.logstash.vendor.bundle.jruby.$2_dot_5_dot_0.gems.logstash_minus_input_minus_beats_minus_5_dot_1_dot_8_minus_java.lib.logstash.inputs.beats.message_listener.RUBY$method$onNewMessage$0(/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstas\\nh-input-beats-5.1.8-java/lib/logstash/inputs/beats/message_listener.rb:41) ~[?:?]\\n        at usr.share.logstash.vendor.bundle.jruby.$2_dot_5_dot_0.gems.logstash_minus_input_minus_beats_minus_5_dot_1_dot_8_minus_java.lib.logstash.inputs.beats.message_listener.RUBY$method$onNewMessage$0$__VARARGS__(/usr/share/logstash/vendor/bundle/jruby/2.5.0/\\ngems/logstash-input-beats-5.1.8-java/lib/logstash/inputs/beats/message_listener.rb) ~[?:?]\\n        at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:91) ~[jruby-complete-9.2.6.0.jar:?]\\n        at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:90) ~[jruby-complete-9.2.6.0.jar:?]\\n        at LogStash$$Inputs$$Beats$$MessageListener_1219897284.onNewMessage(LogStash$$Inputs$$Beats$$MessageListener_1219897284.gen:13) ~[?:?]\\n        at org.logstash.beats.BeatsHandler.channelRead0(BeatsHandler.java:52) ~[logstash-input-beats-5.1.8.jar:?]\\n        at org.logstash.beats.BeatsHandler.channelRead0(BeatsHandler.java:12) ~[logstash-input-beats-5.1.8.jar:?]\\n        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:323) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:297) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:38) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:353) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.util.concurrent.DefaultEventExecutor.run(DefaultEventExecutor.java:66) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-all-4.1.30.Final.jar:4.1.30.Final]\\n        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]\\n\\nI've tried looking up documentation on logstash configuration files to see if there is any way to handle this scenario, but I have found nothing.\\nSo I ask here: how are you supposed to configure logstash to handle exceptions, or are logstash configuration files not the place to do it?\"},\n",
       " {'text_field': \"\\n\\n\\n cd-agutierrez:\\n\\nhow are you supposed to configure logstash to handle exceptions, or are logstash configuration files not the place to do it?\\n\\n\\nIt's up to the author of the code to handle exceptions. As an end-user you cannot do much. The exception is in the beats input.\\nThat said, does setting the option 'include_codec_tag =&gt; false' on the beats input help?\"},\n",
       " {'text_field': 'Hi\\nI have just started using the X-pack.\\nI testing the \"anomaly detection\" tool. I would like to know what algorithm has been used to detect anomalies. For example,\\nIf it is unsupervised; is it k-means, DBSCAN, etc ?\\nIf it is statistics; is it the mean, the median, etc ?\\nThank you'},\n",
       " {'text_field': 'Some relevant videos that will help explain:\\n30 Min Meetup w/ Steve Dodson:\\n\\n2017 ElasticON:\\nhttps://www.elastic.co/elasticon/conf/2017/sf/machine-learning-and-statistical-methods-for-time-series-analysis\\n2018 ElasticON:\\nhttps://www.elastic.co/elasticon/conf/2018/sf/the-math-behind-elastic-machine-learning'},\n",
       " {'text_field': 'Hello,\\nI search some help in order to optimize grok filter.\\nActually, i have 100% CPU used.\\nHave you some tricks ?\\nSincerely'},\n",
       " {'text_field': 'You have too much alternation. Take some of that alternation out of the patterns and put it into the filter configuration.\\nThe access logs should be fine. The mailbox, audit, and other similar logs will be where the problem is.\\nLooking at the mailbox pattern, you have a fixed header, followed by\\n(various patterns, some optional %{GREEDYDATA:mailbox_message}|%{GREEDYDATA:mailbox_trace})$\\n\\nThat looks expensive to me if it ends up delivering mailbox_trace. You could try changing that to\\n\"^%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_source} %{SYSLOGPROG} %{GREEDYDATA:mailbox_trace}\"\\n\\nThen use a second grok to see if mailbox_trace can be parsed. In fact I might split it into 3, using the first to grab mailbox_timestamp, mailbox_loglevel, mailbox_protocole, and consuming everything inside the square brackets as a lump. That minimizes the amount of text you are feeding to a pattern that contains alternation and optional fields.\\nSimilarly, looking at the zimbra logs. AMAVIS is an alternation of more than a dozen patterns, most of which start with an AMAVIS_THREAD. So for an AMAVIS_RESULT it will parse that AMAVIS_THREAD a dozen times as it fails to match all the other options in the alternation. That is all wasted work. You could use an alternation of STARTUP, REEXEC and a third pattern that is an AMAVIS_THREAD followed by GREEDYDATA, then grok that GREEDYDATA against an array of anchored patterns.'},\n",
       " {'text_field': 'Hi guys,\\nI currently struggeling in understanding how this can be done within the elastic stack properly. I take netflow data as an input source in netflow. Specifically, I use elastiflow https://github.com/robcowart/elastiflow, thus I need to use Logstash. However, as I understand it, it might be possible to add an ingest pipeline on elasticsearch side to handle the kind of task I want to perform.\\nSo, my router and everything sends data via netflow to logstash, which then sends it to elasticsearch and I see fancy stuff in Kibana.\\nOn the other hand I have a fully managed network and I should know every device. I have a cmdb, which I can export as a csv, or can talk to as an API.\\nWhat I would like to do is the following:\\n\\nNetflow is received\\nMac, or Hostname, or IP is compared to the cmdb content.\\n\\nif one of those match ==&gt; add field \"known: yes\"\\nif none of those match ==&gt; add field \"known:false\"\\n\\n\\n\\nI do know how to manipulate event data, enrich it, extract messages and so on. I cannot figure out, how to load information from a file or an API inside the filter in Logstash. I know, that this might lead to increased CPU% or RAM and the bucket may take longer to be filled properly, depending on the lookups that have to be done. However, how am I even supposed to do this crossreference?\\nThanks in advance for any idea.'},\n",
       " {'text_field': 'You could use a translate filter. If you do not like that you could consider an elasticsearch, jdbc_streaming, http, or memcached filter.'},\n",
       " {'text_field': 'Hello I am wondering if it would be possible to set the service.environment field from the java agent side using -D options. The main idea is that we would like to use one APM server for multiple environments and we would like to have the possibility to filter the data from the environment.\\nKibana version: 7.3.0\\nElasticsearch version: 7.3.0\\nAPM Server version: 7.3.0\\nAPM Agent language and version: java 1.8.0\\nBrowser version: Safari 13.0\\nOriginal install method (e.g. download page, yum, deb, from source, etc.) and version: apt repo\\nFresh install or upgraded from other version? Fresh install\\nIs there anything special in your setup? Nothing special\\n** I am wondering if it would be possible to set the service.environment field from the java agent side using -D options. The main idea is that we would like to use one APM server for multiple environments and we would like to have the possibility to filter the data from the environment.**:\\nSteps to reproduce:\\n1.\\n2.\\n3.\\nErrors in browser console (if relevant):\\nProvide logs and/or server output (if relevant):'},\n",
       " {'text_field': 'Hi @zozo6015\\nIs this what you are looking for?\\nIf so you should be able to set it with -Delastic.apm.environment=\"production\"\\nThen you can filter on it and there is even a built in filter in the APM UI for versions &gt;= 7.2\\n\\nScreen Shot 2019-08-12 at 2.42.50 PM.png908×758 37 KB\\n'},\n",
       " {'text_field': 'Hi,\\nI have a string like - 123RGT78.XY.ABD.COM\\nI need only 123RGT78 from this string and want to remove everything coming after the first dot (.)\\nHow to achieve this.\\nfilter {\\nmutate {\\ngsub =&gt; [\"message\", \"]\\n}\\n}'},\n",
       " {'text_field': 'In a mutate like\\nmutate { gsub =&gt; [ \"someField\", \"\\\\..*\", \"\" ] }\\n\\nthe second string \"..*\" is a regexp. That particular one says a literal dot, followed by zero or more characters (dot matches any character, which is why the literal dot has to be escaped).\\nIf you want to remove the space, dash, space, and everything after it then use\\nmutate { gsub =&gt; [ \"message\", \" - .*\", \"\" ] }\\n\\nIf you want to remove just the dash and everything after it (but not the space before it) then remove the leading space in the regexp. The pattern you tried, \"-*\", means zero or more occurrences of dash.\\nYou appear to think that * matches a string of characters, like it does in a UNIX filename wildcard. That is not true in most regexps. Instead it modifies whatever comes before it, to say \"zero or more of\".\\nRuby regexps are similar-ish to perl regexps, which are similar, but more functional than ed regexps, which are not similar to /bin/sh regexps, which are very different to /bin/csh regexps. There are many other regexp dialects.\\nIn logstash itself you are always dealing with Ruby regexps (although you could be using plugins that interact with external systems that use other types of regexps). It is possible, if you are using a jdbc input, and an http filter for enrichment, you might use three different types of regular expression in a single configuration, but that really would be unusual. For help with any particular dialect just Google \"ruby regular expression\" or \"oracle sql regular expression\" or \"perl regular expression\" and you will find sites that can guide you.'},\n",
       " {'text_field': 'Hi\\nI am testing the X-Pack tool.\\nSo far I have done:\\n\\nAnomaly detection\\nForecasting\\n\\nI was wondering if it is possible to do:\\n\\nClustering\\nClassification\\nRegression\\netc\\n\\nThank you'},\n",
       " {'text_field': 'Starting in v7.3, Elastic ML\\'s Data Frame analytics allows \"outlier detection\" (via cluster/distance/density analysis). See the API here: https://www.elastic.co/guide/en/elastic-stack-overview/current/ml-dfanalytics-apis.html\\nData Frame analytics functionality will expand to include other types of analysis in future releases.'},\n",
       " {'text_field': 'Hi, i have a logstash configuration like this:\\nfile {\\n         path =&gt; \"/opt/audit_logs_modsec/xx/*/*/*\"\\n    #     path =&gt; \"/opt/audit_logs_modsec/xx/20171231/20171231-2358/*\"\\n         sincedb_path =&gt; \"/dev/null\"\\n        start_position =&gt; \"beginning\"\\n        type =&gt; mod_security\\n\\n        codec =&gt; multiline {\\n          charset =&gt; \"US-ASCII\"\\n          pattern =&gt; \"^--[a-fA-F0-9]{8}-Z--$\"\\n          negate =&gt; true\\n          what =&gt; previous\\n        }\\n      }\\n    }\\n\\nlogstash starts but never reads any file\\nif i uncomment the line:\\n# path =&gt; \"/opt/audit_logs_modsec/xx/20171231/20171231-2358/*\"\\nand comment the:\\npath =&gt; \"/opt/audit_logs_modsec/xx/*/*/*\"\\nit loads data.\\nWhat am i missing?\\nregards'},\n",
       " {'text_field': 'Well, in tail mode, the default limit is that it will only tail 4095 files. Does read mode fit your use case?'},\n",
       " {'text_field': 'I have a field called price in my elasticsearch index whose values I need to convert before sorting -\\n{\\n  \"took\" : 0,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 4,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 1.0,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"products\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"1\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"prod_id\" : 1,\\n          \"currency\" : \"USD\",\\n          \"price\" : 1\\n        }\\n      },\\n      {\\n        \"_index\" : \"products\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"2\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"prod_id\" : 2,\\n          \"currency\" : \"INR\",\\n          \"price\" : 60\\n        }\\n      },\\n      {\\n        \"_index\" : \"products\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"3\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"prod_id\" : 3,\\n          \"currency\" : \"EUR\",\\n          \"price\" : 2\\n        }\\n      },\\n      {\\n        \"_index\" : \"products\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"5\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"prod_id\" : 5,\\n          \"currency\" : \"MYR\",\\n          \"price\" : 1\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nAs the currency field for each product is different,\\nI plan to convert each product\\'s price into USD,\\nUsing the script_score function, as recommended here -\\n\\n  \\n      stackoverflow.com\\n  \\n  \\n      \\n    \\n  \\n\\n  Elastic Search sort preprocessing\\n\\n\\n\\n  elasticsearch, lucene\\n\\n\\n\\n  \\n  answered by\\n  \\n    Val\\n  \\n  on 03:58AM - 18 Aug 15 UTC\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nI tried the following query -\\nGET products/_search\\n{\\n\\t\"query\": {\\n\\t\\t\"function_score\": {\\n\\t\\t\\t\"query\": {\\n\\t\\t\\t\\t\"match_all\": {}\\n\\t\\t\\t},\\n\\t\\t\\t\"functions\": [{\\n\\t\\t\\t\\t\"script_score\": {\\n\\t\\t\\t\\t\\t\"script\": {\\n\\t\\t\\t\\t\\t\\t\"params\": {\\n\\t\\t\\t\\t\\t\\t\\t\"USD\": 1,\\n\\t\\t\\t\\t\\t\\t\\t\"SGD\": 0.72,\\n\\t\\t\\t\\t\\t\\t\\t\"MYR\": 0.24,\\n\\t\\t\\t\\t\\t\\t\\t\"INR\": 0.014,\\n\\t\\t\\t\\t\\t\\t\\t\"EUR\": 1.12\\n\\t\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t\\t\"source\": \"doc[\\'price\\'].value * params.doc[\\'currency\\']\"\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}]\\n\\t\\t}\\n\\t},\\n\\t\"sort\": [\\n    {\\n      \"_score\": {\\n        \"order\": \"desc\"\\n      }\\n    }\\n  ]\\n}\\n\\nI\\'m getting an error -\\n{\\n  \"error\": {\\n    \"root_cause\": [\\n      {\\n        \"type\": \"script_exception\",\\n        \"reason\": \"runtime error\",\\n        \"script_stack\": [\\n          \"doc[\\'price\\'].value * params.doc[\\'currency\\']\",\\n          \"                               ^---- HERE\"\\n        ],\\n        \"script\": \"doc[\\'price\\'].value * params.doc[\\'currency\\']\",\\n        \"lang\": \"painless\"\\n      }\\n    ],\\n    \"type\": \"search_phase_execution_exception\",\\n    \"reason\": \"all shards failed\",\\n    \"phase\": \"query\",\\n    \"grouped\": true,\\n    \"failed_shards\": [\\n      {\\n        \"shard\": 0,\\n        \"index\": \"products\",\\n        \"node\": \"5-fQ27BhSUKycVJ2SwyH4A\",\\n        \"reason\": {\\n          \"type\": \"script_exception\",\\n          \"reason\": \"runtime error\",\\n          \"script_stack\": [\\n            \"doc[\\'price\\'].value * params.doc[\\'currency\\']\",\\n            \"                               ^---- HERE\"\\n          ],\\n          \"script\": \"doc[\\'price\\'].value * params.doc[\\'currency\\']\",\\n          \"lang\": \"painless\",\\n          \"caused_by\": {\\n            \"type\": \"class_cast_exception\",\\n            \"reason\": \"Cannot apply [*] operation to types [java.lang.Long] and [org.elasticsearch.index.fielddata.ScriptDocValues.Strings].\"\\n          }\\n        }\\n      }\\n    ]\\n  },\\n  \"status\": 400\\n}\\n\\nExpected product sequence -\\n\\n\\nprod_id 3, _score 2.24 = 2(price) * 1.12 (USD multiplier)\\n\\n\\nprod_id 1, _score 1 = 1(price) * 1 (USD multiplier) = 1\\n\\n\\nprod_id 2, _score 0.84 = 60(price) * 0.014 (USD multiplier)\\n\\n\\nprod_id 5, _score 0.24 = 1(price) * 0.24 (USD multiplier)\\n\\n\\nIndex Mapping -\\n{\\n  \"mapping\": {\\n    \"properties\": {\\n      \"currency\": {\\n        \"type\": \"text\",\\n        \"fields\": {\\n          \"keyword\": {\\n            \"type\": \"keyword\",\\n            \"ignore_above\": 256\\n          }\\n        },\\n        \"fielddata\": true\\n      },\\n      \"price\": {\\n        \"type\": \"long\"\\n      },\\n      \"prod_id\": {\\n        \"type\": \"long\"\\n      }\\n    }\\n  }\\n}\\n\\nCan someone please help for this use case query?'},\n",
       " {'text_field': \"Your Painless script is not quite correct. Try this:\\ndoc['price'].value * params[doc['currency.keyword'].value]\\n\"},\n",
       " {'text_field': \"I was able to configure logstash 6.7.1 to enable monitoring and it's successfully showing up in Kibana.\\nHowever the host name is incorrect, it defaults to 127.0.0.1:9600 and 127.0.0.1:9601 for all 3 nodes in my cluster. Am I missing something?\\n\\nimage.png1086×448 29.2 KB\\n\"},\n",
       " {'text_field': 'This is the defaul behavior,  127.0.0.1  is the default value for the setting  http.host  in the  logstash.yml  file.\\nThe  http.host  variable is the metrics REST endpoint, used for get the monitoring metrics, if you do not specify an ip address, it will bind to the localhost ip, which is  127.0.0.1 .\\nIf you want it to show another ip address present in the server, you will need to set this variable to that ip address.\\nGot the above solution from [leandrojmp]'},\n",
       " {'text_field': 'I am trying to import one CSV file in Data Visualizer in Kibana.\\nMappings :\\n{\\n  \"@timestamp\": {\\n    \"type\": \"date\"\\n  },  \\n  \"Date\": {\\n    \"type\": \"date\"\\n    format : \"dd-MM-yyyy\"\\n  },\\n  \"Order_Demand\": {\\n    \"type\": \"long\"\\n  },\\n  \"Product_Category\": {\\n    \"type\": \"keyword\"\\n  },\\n  \"Product_Code\": {\\n    \"type\": \"keyword\"\\n  },\\n  \"Warehouse\": {\\n    \"type\": \"keyword\"\\n  }\\n}\\n\\nIngest Pipeline :\\n{\\n  \"description\": \"Ingest pipeline created by file structure finder\",\\n  \"processors\": [\\n    {\\n      \"date\": {\\n        \"field\": \"Date\",\\n        \"timezone\": \"{{ beat.timezone }}\",\\n        \"formats\": [\\n          \"dd-MM-yyyy\"\\n        ]\\n      }\\n    }\\n  ]\\n}\\n\\nWhile importing it, it displays this error :\\nError parsing JSON\\nError parsing mappings: Unexpected token f in JSON at position 81'},\n",
       " {'text_field': \"Yep, looks like your json is not formatted correctly, specifically that format is not in quotes, and that there isn't a comma at the end of the previous line.\"},\n",
       " {'text_field': \"Hi,\\nI'm testing the refresh.interval [2]of my ES cluster (6 nodes, ES 6.5.0).\\nI'm doing sync upserts into an index with refresh.interval=30s.\\nIn a differnt app, I'm executing queries.\\nHowever I set index.interval, my second app directly see any change done my the upsert (no explicit refresh in the upsert. Using java high level rest client [1]).\\nMy understanding of refresh.interval was that new data will not be visible until the interval is reached. How can it be that I always directly see any new/changed data?\\ntx for help\\nJean-Marc\\n[1] https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high.html\\n[2] https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html\"},\n",
       " {'text_field': 'This is expected then. GET index/_doc/id is a realtime operation.'},\n",
       " {'text_field': 'How to add body to a request while checking for status of the url in heartbeat.\\nHere is my sample from heartbeat.yml\\n\\ntype: http\\nenabled: true\\nname: \"vtrack\"\\nurls: [\"\"]\\ncheck.request:\\nmethod: GET\\nheaders: [\\n\\'Content-Type\\': \\'application/json\\',\\n\\'sessionId\\': \\'\\',\\n\\'userType\\': \\'\\']\\nbody: {\"data\":[\"XXXXX\"],\"key\":\"YYYYY\"}\\ncheck.response.status: 200\\nschedule: \\'@every 60s\\'\\n\\nAbove line throws error that body should be an string and not an object. And moreover how do we add username and password to the url ?'},\n",
       " {'text_field': \"Yes, the error is correct. You'll need to quote the JSON body as a string, perhaps using YAML multiline strings.\"},\n",
       " {'text_field': 'Hi,\\nnot sure if this issue is better be posted here or in metricbeat section.\\nI have deployed metricbeat in kubernetes using this manual:\\nhttps://www.elastic.co/guide/en/beats/metricbeat/current/running-on-kubernetes.html\\nNow I am trying to get kubernetes metrics to elasticsearch\\nI changed following things:\\n-&gt; Output is changed redis, not elasticsearch.\\nLogstash is pulling from redis and directly pushing to elasticsearch, without any filters. I am just using redis as message broker for all other logs, etc. So I want to keep same route for all data which is coming to elasticsearch.\\nIn Logstash I can see following errors:\\n[2019-08-13T08:46:19,364][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"metricbeat-7.3.0-2019.w33\", :_type=&gt;\"_doc\", :routing=&gt;nil}, #&lt;LogStash::Event:0x4b443107&gt;], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"metricbeat-7.3.0-2019.w33\", \"_type\"=&gt;\"_doc\", \"_id\"=&gt;\"PLIqimwBthAGD_OhXAJU\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"failed to parse field [kubernetes.labels.app] of type [keyword] in document with id \\'PLIqimwBthAGD_OhXAJU\\'. Preview of field\\'s value: \\'{kubernetes={io/part-of=ingress-nginx, io/name=ingress-nginx}}\\'\", \"caused_by\"=&gt;{\"type\"=&gt;\"illegal_state_exception\", \"reason\"=&gt;\"Can\\'t get text on a START_OBJECT at 1:450\"}}}}}\\nSo why can logstash not parse the field. Output of metricbeat to redis looks like plain json, so what problem does logstash have?\\nIs there any other best practice to ship metricbeat data via redis to elasticsearch?\\nThanks, Andreas'},\n",
       " {'text_field': 'So here the results of my research:\\nyes, if labels have dots in their name, they will be interpreted as structure. This may fail on indexing when you have different labels like app=myapp and app.kubernetes.io/name=ingress-nginx. They are conflicting in elasticsearch because the first one is a string, the second one is a structure.\\nI found this here: https://github.com/elastic/beats/pull/9939\\nThis is quite the solution. setting dedot options for labels and anotations are fixing the issue. Then the . will be replaced with _ in field names.'},\n",
       " {'text_field': 'following is my config\\ninput {\\n  file {\\n    path =&gt; \"/*.log\"\\n    start_position =&gt; \"beginning\"\\n    sincedb_path =&gt; \"/sincedb.log\"\\n    exclude =&gt; [\"*current.log\", \"*sincedb.log\"]\\n  }\\n}\\n\\nfilter {\\n  json {\\n    source =&gt; \"message\"\\n    # add_field =&gt; { \"new_field2\" =&gt; \"My name is %{controller}\" }\\n    remove_field =&gt; [ \"message\" ]\\n  }\\n}\\n\\n\\noutput {\\n  elasticsearch {\\n    hosts =&gt; \"http://127.0.0.1:9200\"\\n    index =&gt; \"log_${???}\"   # what i want to know\\n    codec =&gt; \"json\"\\n  }\\n\\n}\\n\\n\\n\\n\\ninput\\ni\\'m reading multiple json lines in multiple files. (read mode)\\nin these json strings including \\'date\\' property (yyyy-MM-dd HH:mm:ss)\\n\\n\\nfilter\\ni added logstash\\'s message  to source and delete message\\n\\n\\noutput\\ni want to insert this log into \\'log_2019.08.13\\' index in elasticsearch.\\n\\n\\ni has created indices name with server date.\\nbut, sometime \\'date\\' key\\'s value in log and actual server time is not same.\\nfor example, value about log\\'s \\'date\\' key is \\'2019-08-12 23:59:59\\'.\\nbut when this logs going in index, and server time has changed to next day,\\nthis log will be come to \\'log_2019.08.13\\'.  so, my aggregation result has lower correctness.\\nso, i decided to make index name\\'s date \\'yyyy.MM.dd\\' format via log\\'s \\'date\\' field.\\nhow can i substring(for substring \\' HH:mm:ss\\', \\'yyyy-MM-dd\\')\\nand join(for join \"yyyy + \\'.\\' + MM + \\'.\\' + dd\")\\nand use in index name in logstash config?'},\n",
       " {'text_field': 'If you use a time reference in the index name, like \"logstash-%{+YYYY.MM.dd}\", then the YYYY, MM, and dd values are extracted from the [@timestamp] field of the event. So you just need a date filter to parse your date field into @timestamp.'},\n",
       " {'text_field': 'I\\'m writing a java application that injects data into Elastic and displays Kibana in an embedded browser window.  During initialization, it attempts to start the services for both.  The init code for that looks like this:\\nLogging.logInfo(\"Starting Elastic service\");\\ntry\\n{\\n    gElastic = new Elastic();\\n}\\ncatch (IOException ioEx)\\n{\\n    Logging.showError(this, ioEx, \"Failed to start Elastic service\", \"Critical Error\");\\n    exit(EXIT_CRITICAL_FAILURE);\\n}\\nwhile (!gElastic.gReady)\\n{\\n    DateTime.wait(DateTime.HALF_SECOND);\\n}\\nDateTime.wait(DateTime.ONE_SECOND);\\nLogging.logInfo(\"Starting Kibana service\");\\ntry\\n{\\n    gKibana = new Kibana();\\n}\\ncatch (IOException ioEx)\\n{\\n    Logging.showError(this, ioEx, \"Failed to start Kibana service\", \"Critical Error\");\\n    exit(EXIT_CRITICAL_FAILURE);\\n}\\nwhile (!gKibana.gReady)\\n{\\n    DateTime.wait(DateTime.HALF_SECOND);\\n}\\nDateTime.wait(DateTime.ONE_SECOND);\\n\\nThe code that handles the services looks like this:\\npublic static final String KIBANA_BASH = String.format(\"bash %skibana%sbin%skibana\", System.getProperty(\"user.dir\"), File.separator, File.separator);\\n\\npublic Process gProc;\\npublic boolean gReady = false;\\n\\npublic Kibana() throws IOException\\n{\\n    gProc = Runtime.getRuntime().exec(KIBANA_BASH);\\n    new Threading(() -&gt;\\n    {\\n        try\\n        {\\n            FileOutputStream fos = new FileOutputStream(new File(System.getProperty(\"user.dir\") + \"kibana.txt\"));\\n            PrintWriter pw = new PrintWriter(fos, true);\\n            Scanner scan = new Scanner(gProc.getInputStream());\\n            while (gProc.isAlive() &amp;&amp; !gReady)\\n            {\\n                if (scan.hasNext())\\n                {\\n                    String line = scan.nextLine();\\n                    pw.println(line);\\n                    if (line.trim().endsWith(\"\\\\\"http server running\\\\\"}\"))\\n                    {\\n                        gReady = true;\\n                        Logging.logInfo(\"Kibana service ready\");\\n                    }\\n                }\\n                else\\n                {\\n                    DateTime.wait(DateTime.HALF_SECOND);\\n                }\\n            }\\n            scan.close();\\n            pw.flush();\\n            pw.close();\\n            fos.close();\\n        }\\n        catch (Exception ex)\\n        {\\n            Logging.logError(ex, \"\");\\n        }\\n    });\\n}\\n\\npublic void close()\\n{\\n    if (gProc.isAlive())\\n    {\\n        gProc.destroy();\\n    }\\n}\\n\\nThe Elastic class is identical to the Kibana one except that it runs /elasticsearch/bin/elasticsearch instead of kibana/bin/kibana.\\nIf I use the terminal to start the two services, they both work exactly as expected.  Using the above method to start them results in Elastic working as expected - I\\'m able to navigate to the page and see the JSON output - but going to localhost:5601 results in failure to connect even using a standard browser.\\nThe PrintWriter that you can see in the code is there to dump the console output into a file, and by performing a diff on the output from the regular console and the java console I know that with the exception of time stamps and process numbers they are exactly identical.  This leads me to believe that either something on the java side is blocking connections to Kibana, although I can\\'t imagine why it would allow connections to Elastic in that case, or something about my (default) Kibana config doesn\\'t like being started by java and doesn\\'t show in the logs.'},\n",
       " {'text_field': 'I wrote the following program as a testbed:\\npackage elastic;\\n\\nimport java.awt.*;\\nimport java.io.File;\\nimport java.util.Scanner;\\n\\nimport javax.swing.*;\\n\\n/**\\n * &lt;h1&gt;Elastic&lt;/h1&gt;\\n * @since  2019-08-16\\n */\\npublic class Elastic extends JFrame\\n{\\n    /* #Region Constants */\\n\\n    private static final long serialVersionUID = 1L;\\n\\n    /* #Region Globals */\\n\\n    Process elastic;\\n    JPanel ePanel;\\n    JScrollPane eScroll;\\n    int eGrid = 0;\\n\\n    Process kibana;\\n    JPanel kPanel;\\n    JScrollPane kScroll;\\n    int kGrid = 0;\\n\\n    /* #Region Constructors */\\n\\n    public Elastic()\\n    {\\n        final String path = System.getProperty(\"user.dir\").endsWith(File.separator) ? System.getProperty(\"user.dir\") : System.getProperty(\"user.dir\") + File.separator;\\n\\n        setLayout(new GridBagLayout());\\n\\n        GridBagConstraints gbc = new GridBagConstraints();\\n\\n        gbc.fill = GridBagConstraints.BOTH;\\n        gbc.insets = new Insets(5, 5, 5, 5);\\n        gbc.weightx = 0.5;\\n        gbc.weighty = 0.0;\\n\\n        ePanel = new JPanel();\\n        ePanel.setLayout(new BoxLayout(ePanel, BoxLayout.Y_AXIS));\\n\\n        eScroll = new JScrollPane(ePanel, JScrollPane.VERTICAL_SCROLLBAR_AS_NEEDED, JScrollPane.HORIZONTAL_SCROLLBAR_AS_NEEDED);\\n\\n        JButton bElastic = new JButton(\"Start Elastic\");\\n        bElastic.addActionListener(event -&gt;\\n        {\\n            new Thread(() -&gt;\\n            {\\n\\n                if (elastic != null &amp;&amp; elastic.isAlive())\\n                {\\n                    elastic.destroy();\\n                    return;\\n                }\\n\\n                try\\n                {\\n                    elastic = Runtime.getRuntime().exec(String.format(\"%selasticsearch%sbin%selasticsearch\", path, File.separator, File.separator));\\n\\n                    bElastic.setText(\"Stop Elastic\");\\n\\n                    Scanner scan = new Scanner(elastic.getInputStream());\\n                    while (elastic.isAlive())\\n                    {\\n                        if (scan.hasNext())\\n                        {\\n                            String line = scan.nextLine();\\n                            System.out.println(line);\\n                            log(ePanel, eScroll, eGrid++, line);\\n                        }\\n                    }\\n                    scan.close();\\n                }\\n                catch (Exception ex)\\n                {\\n                    log(ePanel, eScroll, eGrid++, ex.getMessage());\\n                    for (StackTraceElement ste : ex.getStackTrace())\\n                    {\\n                        log(ePanel, eScroll, eGrid++, ste.toString());\\n                    }\\n                }\\n            }).start();\\n        });\\n\\n        kPanel = new JPanel();\\n        kPanel.setLayout(new BoxLayout(kPanel, BoxLayout.Y_AXIS));\\n\\n        kScroll = new JScrollPane(kPanel, JScrollPane.VERTICAL_SCROLLBAR_AS_NEEDED, JScrollPane.HORIZONTAL_SCROLLBAR_AS_NEEDED);\\n\\n        JButton bKibana = new JButton(\"Start Kibana\");\\n        bKibana.addActionListener(event -&gt;\\n        {\\n            new Thread(() -&gt;\\n            {\\n\\n                if (kibana != null &amp;&amp; kibana.isAlive())\\n                {\\n                    kibana.destroy();\\n                    return;\\n                }\\n\\n                try\\n                {\\n                    kibana = Runtime.getRuntime().exec(String.format(\"%skibana%sbin%skibana\", path, File.separator, File.separator));\\n\\n                    bKibana.setText(\"Stop Kibana\");\\n\\n                    Scanner scan = new Scanner(kibana.getInputStream());\\n                    while (kibana.isAlive())\\n                    {\\n                        if (scan.hasNext())\\n                        {\\n                            String line = scan.nextLine();\\n                            System.out.println(line);\\n                            log(kPanel, kScroll, kGrid++, line);\\n                        }\\n                    }\\n                    scan.close();\\n                }\\n                catch (Exception ex)\\n                {\\n                    log(kPanel, kScroll, kGrid++, ex.getMessage());\\n                    for (StackTraceElement ste : ex.getStackTrace())\\n                    {\\n                        log(kPanel, kScroll, kGrid++, ste.toString());\\n                    }\\n                }\\n            }).start();\\n        });\\n\\n        getContentPane().add(bElastic, gbc);\\n\\n        gbc.gridy = 1;\\n        gbc.weighty = 1.0;\\n        getContentPane().add(eScroll, gbc);\\n\\n        gbc.gridx = 1;\\n        getContentPane().add(kScroll, gbc);\\n\\n        gbc.gridy = 0;\\n        gbc.weighty = 0.0;\\n        getContentPane().add(bKibana, gbc);\\n\\n        setExtendedState(JFrame.MAXIMIZED_BOTH);\\n\\n        this.addWindowListener(new WindowAdapter()\\n        {\\n            @Override\\n            public void windowClosing(WindowEvent event)\\n            {\\n                if (kibana != null)\\n                {\\n                    kibana.destroy();\\n                }\\n                if (elastic != null)\\n                {\\n                    elastic.destroy();\\n                }\\n                System.exit(1);\\n            }\\n        });\\n\\n        setVisible(true);\\n        toFront();\\n        requestFocus();\\n    }\\n\\n    /* #Region Methods */\\n\\n    private void log(JPanel panel, JScrollPane scroll, int gridy, String text)\\n    {\\n        GridBagConstraints gbc = new GridBagConstraints();\\n        gbc.gridy = gridy;\\n        panel.add(new JLabel(text), gbc);\\n\\n        try\\n        {\\n            scroll.validate();\\n\\n            JScrollBar scrollBar = scroll.getVerticalScrollBar();\\n            if ((double) (scrollBar.getValue() + scrollBar.getHeight()) / (double) scrollBar.getMaximum() &gt; 0.95)\\n            {\\n                scrollBar.setValue(scrollBar.getMaximum());\\n            }\\n        }\\n        catch (Exception ex)\\n        {\\n        }\\n\\n        scroll.validate();\\n        scroll.repaint();\\n    }\\n\\n    public static void main(String[] args)\\n    {\\n        try\\n        {\\n            new Elastic();\\n        }\\n        catch (Exception ex)\\n        {\\n            ex.printStackTrace();\\n        }\\n    }\\n}\\n\\nThis works exactly as expected, with both Elastic and Kibana accepting connections.  I\\'ll spend some more time playing with it in the main application.'},\n",
       " {'text_field': 'Please help How to clean metricbeat index every 30 days without x-pack?'},\n",
       " {'text_field': 'Here is a complete list of what features are available in the basic subscription the comparison to other subscription levels and open source.\\nAnd yes if you downloaded the stack from the elastic website and do not install the trial license you will be running the basic distribution with all the basic features available to you.\\nIndex lifecycle management is a basic feature.'},\n",
       " {'text_field': 'Hi,\\nThis is a newly installed Elasticsearch and Kibana on CentOS7 Core. This system was hardened following \"CIS_CentOS_Linux_7_Benchmark_v2.2.0.pdf\". I\\'m experiencing a very weird situation wherein Kibana restarts every 14 seconds, please refer to the following stackoverflow post for the session excerpts:\\n\\n  \\n      stackoverflow.com\\n  \\n  \\n      \\n    \\n  \\n\\n  Kibana restarts every 14seconds in CentOS7 Core\\n\\n\\n\\n  kibana, centos7\\n\\n\\n\\n  asked by\\n  \\n  \\n    bad rabbit\\n  \\n  on 05:06PM - 19 Oct 19 UTC\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nI posted in stackoverflow because they allow up to 30000 characters in a post while here is only 7000.\\nI actually checked the status of kibana service every second and it does restart at the 14th second as below:\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 2s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]#\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 3s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]#\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 5s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]#\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 6s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 7s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 8s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 9s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 10s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 12s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 13s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           └─25870 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n[root@syslog ~]# systemctl status kibana\\n● kibana.service - Kibana\\n   Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled)\\n   Active: active (running) since Sun 2019-10-20 08:18:55 +08; 14s ago\\n Main PID: 25870 (node)\\n   CGroup: /system.slice/kibana.service\\n           ‣ 25870 [node]\\n\\nOct 20 08:18:55 syslog systemd[1]: Started Kibana.\\n\\nI have the kibana.log but the Upload function doesn\\'t allow me to upload .log file\\nLet me know how can I share the kibana.log and appreciate your help.\\nThank you'},\n",
       " {'text_field': 'Hi,\\nI re-installed Elasticsearch and Kibana but still getting that restart every 14 seconds. I was able to resolve this issue by removing \"noexec\" from /var; got the solution from Kibana server is not ready yet - throw er; // Unhandled \\'error\\' event\\nI actually debugged the startup of Kibana and this was the log I got during the stop to start:\\nOct 30 15:58:16 syslog kibana: {\"type\":\"log\",\"@timestamp\":\"2019-10-30T07:58:16Z\",\"tags\":[\"status\",\"plugin:reporting@7.4.1\",\"info\"],\"pid\":14119,\"state\":\"green\",\"message\":\"Status changed from uninitialized to green - Ready\",\"prevState\":\"uninitialized\",\"prevMsg\":\"uninitialized\"}\\nOct 30 15:58:16 syslog kibana: {\"type\":\"log\",\"@timestamp\":\"2019-10-30T07:58:16Z\",\"tags\":[\"reporting\",\"warning\"],\"pid\":14119,\"message\":\"The Reporting plugin encountered issues launching Chromium in a self-test. You may have trouble generating reports: [TypeError: input.on is not a function]\"}\\nOct 30 15:58:16 syslog kibana: {\"type\":\"log\",\"@timestamp\":\"2019-10-30T07:58:16Z\",\"tags\":[\"reporting\",\"warning\"],\"pid\":14119,\"message\":\"See Chromium\\'s log output at \\\\\"/var/lib/kibana/headless_shell-linux/chrome_debug.log\\\\\"\"}\\nOct 30 15:58:16 syslog kibana: {\"type\":\"log\",\"@timestamp\":\"2019-10-30T07:58:16Z\",\"tags\":[\"reporting\",\"warning\"],\"pid\":14119,\"message\":\"Reporting plugin self-check failed. Please check the Kibana Reporting settings. Error: Could not close browser client handle!\"}\\nOct 30 15:58:17 syslog kibana: events.js:174\\nOct 30 15:58:17 syslog kibana: throw er; // Unhandled \\'error\\' event\\nOct 30 15:58:17 syslog kibana: ^\\nOct 30 15:58:17 syslog kibana: Error: spawn /var/lib/kibana/headless_shell-linux/headless_shell EACCES\\nOct 30 15:58:17 syslog kibana: at Process.ChildProcess._handle.onexit (internal/child_process.js:240:19)\\nOct 30 15:58:17 syslog kibana: at onErrorNT (internal/child_process.js:415:16)\\nOct 30 15:58:17 syslog kibana: at process._tickCallback (internal/process/next_tick.js:63:19)\\nOct 30 15:58:17 syslog kibana: Emitted \\'error\\' event at:\\nOct 30 15:58:17 syslog kibana: at Process.ChildProcess._handle.onexit (internal/child_process.js:246:12)\\nOct 30 15:58:17 syslog kibana: at onErrorNT (internal/child_process.js:415:16)\\nOct 30 15:58:17 syslog kibana: at process._tickCallback (internal/process/next_tick.js:63:19)\\nOct 30 15:58:17 syslog systemd: kibana.service: main process exited, code=exited, status=1/FAILURE\\nOct 30 15:58:17 syslog systemd: Unit kibana.service entered failed state.\\nOct 30 15:58:17 syslog systemd: kibana.service failed.\\nOct 30 15:58:17 syslog systemd: kibana.service holdoff time over, scheduling restart.\\nOct 30 15:58:17 syslog systemd: Stopped Kibana.\\nOct 30 15:58:17 syslog systemd: Started Kibana.\\nOct 30 15:58:20 syslog kibana: {\"type\":\"log\",\"@timestamp\":\"2019-10-30T07:58:20Z\",\"tags\":[\"info\",\"plugins-system\"],\"pid\":14144,\"message\":\"Setting up [4] plugins: [security,translations,inspector,data]\"}\\nOct 30 15:58:20 syslog kibana: {\"type\":\"log\",\"@timestamp\":\"2019-10-30T07:58:20Z\",\"tags\":[\"info\",\"plugins\",\"security\"],\"pid\":14144,\"message\":\"Setting up plugin\"}\\n\\nwhich led me to search for \"kibana: Error: spawn /var/lib/kibana/headless_shell-linux/headless_shell EACCES\" that directed me to the solution URL.\\nThanks to Kaldayr for the solution.'},\n",
       " {'text_field': 'Where app search data stored ? and is safe run self managed app search on stateless deployment on kubernetes ?'},\n",
       " {'text_field': 'Data is stored in Elasticsearch. So in that sense the app server and worker are stateless but you would need a stateful Elasticsearch installation.'},\n",
       " {'text_field': 'Hi,\\nThe query at the bottom of this thread returns data in this shape, ordered by the key value asc:\\n\\nimage.png464×844 15.8 KB\\n\\nIs there a way for me to sort the aggregations by the sum of the user_count values per key?  I want the data to stay in the same shape, but just change the sorting.\\nQuery:\\n\\nimage.png344×683 8 KB\\n'},\n",
       " {'text_field': 'You can try Bucket Sort Aggregation\\nhere is an example to get the top 3 sales buckets with highest sum ...\\nPOST /sales/_search\\n{\\n    \"size\": 0,\\n    \"aggs\" : {\\n        \"sales_per_month\" : {\\n            \"date_histogram\" : {\\n                \"field\" : \"date\",\\n                \"calendar_interval\" : \"month\"\\n            },\\n            \"aggs\": {\\n                \"total_sales\": {\\n                    \"sum\": {\\n                        \"field\": \"price\"\\n                    }\\n                },\\n                \"sales_bucket_sort\": {\\n                    \"bucket_sort\": {\\n                        \"sort\": [\\n                          {\"total_sales\": {\"order\": \"desc\"}}\\n                        ],\\n                        \"size\": 3\\n                    }\\n                }\\n            }\\n        }\\n    }\\n}'},\n",
       " {'text_field': 'Halo guys\\nI\\'m new with ELK Stack\\nI try to send IIS log from FileBeat to Logstash and further but it doesn\\'t work. I get an error Failed to publish events caused by: lumberjack protocol error when start FileBeat (Logstash is running)\\nHere all my config\\nfilebeat.yml\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n    - e:\\\\\\\\elk\\\\\\\\iislog\\\\\\\\*\\n  exclude_lines: [\\'#\\']\\nfilebeat.config.modules:\\n  path: ${path.config}/modules.d/*.yml\\n  reload.enabled: false\\noutput.logstash:\\n  hosts: [\"localhost:5044\"]\\n\\nlogstash.yml\\n node.name: main\\n pipeline.id: main\\n pipeline.workers: 2\\n http.host: \"localhost\"\\n http.port: 5044\\n\\nlogstash.iis.conf\\ninput {  \\n   beats {\\n        port =&gt; \"5044\"\\n    }\\n}\\n\\noutput {\\n\\n}\\n\\niis.yml\\n- module: iis\\n  # Access logs\\n  access:\\n    enabled: true\\n    var.paths: \\n        - e:\\\\elk\\\\iislog\\\\*.log\\n  error:\\n    enabled: true\\n\\nLogstash screen stand at line Successfully started Logstash API endpoint {:port=&gt;5044} All stack are version 7.4.0\\nCan you guys show me what am i doing wrong\\nThanks'},\n",
       " {'text_field': '\\n\\n\\n Sam:\\n\\nhttp.host: \"localhost\" http.port: 5044\\n\\n\\nChange this monitoring port to the default 9600 as it clashes with the beats input. You can not have two different things bind to the same port.'},\n",
       " {'text_field': 'Is there a way to apply a life cycle management policy only to metricbeat-* pattern index in 7.4 logstash/Elasticsearch/kibana cluster to delete any index older than 30 days?'},\n",
       " {'text_field': 'Yes possible, check theses links\\nhttps://www.elastic.co/guide/en/beats/metricbeat/current/ilm.html\\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html'},\n",
       " {'text_field': 'Hi, I use elk container with latest tag.\\nI\\'ve added this line in kibana.yml to use OpenStreetMap:\\ntilemap.url: \"http://a.tile.openstreetmap.org/{z}/{x}/{y}.png\"\\nbut I got the following error:\\nError: \"tilemap.url\" setting was not applied. Check for spelling errors and ensure that expected plugins are installed.\\nwhat is the problem ? what plugin needs to be installed?\\nThanx'},\n",
       " {'text_field': 'Use map.tilemap.url instead of tilemap.url\\nCheck on https://www.elastic.co/guide/en/kibana/current/settings.html#tilemap-settings'},\n",
       " {'text_field': 'Смысл... два разных индекса для двух разных проектов. Надо чтобы они не мешали друг другу.\\nПробовал как написано в доке:\\nhttps://www.elastic.co/guide/en/elasticsearch/reference/master/shard-allocation-filtering.html\\nРаботает, но если только если указанные IP или имя доступны.\\nЕсли указываешь не существующее имя индекс остается там где есть, а я хочу чтобы в этом случае он выключился. А эластик ругнулся мол \"Этот индекс некуда аллоцировать поэтому я его выключил\".'},\n",
       " {'text_field': 'Решилось. Указанное выше поведение выше поведение характерно только для уже существующих индексов.\\nЕсли в темплейте для вновь создаваемых индексов задать не существующий хост\\n\"routing.allocation.include._name\":\"non_existent_host\"\\n\\nИндекс создастся, но так и останется красным. Вставка документа в такой индекс подвисает и через минуту падает с руганью:\\n{\"error\":{\"root_cause\":[{\"type\":\"unavailable_shards_exception\",\"reason\":\"[doc-fordel2][0] primary shard is not active Timeout: [1m]....\\n\\nP.S. Игорь, если есть добавить, добавь пожалуйста!  )'},\n",
       " {'text_field': 'Добрый день!\\nЕсть  запрос SQL  как правильно сделать в Elastic\\nИщем слова где and ( city_id IN ( \"12,34,56,76,67,\") or city IN ( \\'Киев\\',   \\'Харьков\\') )'},\n",
       " {'text_field': 'Вы ранее написали:\\n\\n\\n\\n Dimasp:\\n\\nНет запрос не правильно делает выборку.\\n\\n\\nтеперь вы пишите\\n\\n\\n\\n Dimasp:\\n\\nвсе работает.\\n\\n\\nЯ что-то не очень понимаю.\\nЕсли SQL работает и вам надо получить именно тот запрос, который SQL использует, то можно запустить\\nPOST /_sql/translate\\n{\\n \"query\": \"SELECT * FROM p WHERE MATCH(name,\\'купить iphone 6s\\') AND category_id IN ( 2,3,4,5,6,7,8,9,10,1) and (region = 2121 or city_id IN ( 3331,3332,3343) or city IN ( \\'Бар\\',\\'Бершадь\\',\\'Винница\\',\\'Винницкая область\\' ) ) \"\\n}\\n\\nИ тогда вы получите, большой запрос, который можно упростить до:\\n{\\n  \"query\" : {\\n    \"bool\" : {\\n      \"must\" : [\\n        {\\n          \"match\" : {\\n            \"name\" : {\\n              \"query\" : \"купить iphone 6s\"\\n            }\\n          }\\n        },\\n        {\\n          \"terms\" : {\\n            \"category_id\" : [ 2, 3, 4, 5, 6, 7, 8, 9, 10, 1]\\n          }\\n        },\\n        {\\n          \"bool\" : {\\n            \"should\" : [\\n              {\\n                \"term\" : {\\n                  \"region\" : {\\n                    \"value\" : 2121\\n                  }\\n                }\\n              },\\n              {\\n                \"terms\" : {\\n                  \"city_id\" : [ 3331, 3332, 3343 ]\\n                }\\n              },\\n              {\\n                \"terms\" : {\\n                  \"city.keyword\" : [ \"Бар\", \"Бершадь\", \"Винница\", \"Винницкая область\"]\\n                }\\n              }\\n            ]\\n          }\\n        }\\n      ]\\n    }\\n  },\\n  \"sort\" : [\\n    {\\n      \"_doc\" : {\\n        \"order\" : \"asc\"\\n      }\\n    }\\n  ]\\n}\\n\\n'},\n",
       " {'text_field': 'One index was created as Unassigned:\\n/localhost:9200/_cat/shards?v\\nindex                        shard prirep state       docs   store ip            node\\n.kibana_task_manager_1       0     p      STARTED        2  46.1kb 192.168.96.11 es03\\n.kibana_task_manager_1       0     r      STARTED        2  46.1kb 192.168.96.5  es05\\n.security-7                  0     p      STARTED       36  74.7kb 192.168.96.5  es05\\n.security-7                  0     r      STARTED       36  77.8kb 192.168.96.3  es01\\nkibana_sample_data_ecommerce 0     p      STARTED     4675   4.9mb 192.168.96.11 es03\\nkibana_sample_data_ecommerce 0     r      STARTED     4675   4.8mb 192.168.96.9  es02\\nkibana_sample_data_flights   0     p      STARTED    13059   6.2mb 192.168.96.9  es02\\nkibana_sample_data_flights   0     r      UNASSIGNED\\n.kibana_1                    0     p      STARTED      118 985.2kb 192.168.96.9  es02\\n.kibana_1                    0     r      STARTED      118   978kb 192.168.96.8  es06\\ntest123                      0     p      UNASSIGNED\\ntest123                      0     r      UNASSIGNED\\n\\n\\n\\n/localhost:9200/_cat/allocation?v\\nshards disk.indices disk.used disk.avail disk.total disk.percent host          ip            node\\n     0           0b    32.2gb    163.6gb    195.8gb           16 192.168.96.10 192.168.96.10 es04\\n     2        4.9mb    32.2gb    163.6gb    195.8gb           16 192.168.96.11 192.168.96.11 es03\\n     1        978kb    32.2gb    163.6gb    195.8gb           16 192.168.96.8  192.168.96.8  es06\\n     3       12.1mb    32.2gb    163.6gb    195.8gb           16 192.168.96.9  192.168.96.9  es02\\n     2      120.9kb    32.2gb    163.6gb    195.8gb           16 192.168.96.5  192.168.96.5  es05\\n     1       77.8kb    32.2gb    163.6gb    195.8gb           16 192.168.96.3  192.168.96.3  es01\\n     3                                                                                       UNASSIGNED\\n\\n\\n\\nip            heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\\n192.168.96.5            33          99   3    0.29    0.45     0.68 dilm      -      es05\\n192.168.96.10           32          99   3    0.29    0.45     0.68 dilm      -      es04\\n192.168.96.3            39          99   3    0.29    0.45     0.68 dilm      -      es01\\n192.168.96.8            27          99   3    0.29    0.45     0.68 dilm      -      es06\\n192.168.96.11           36          99   3    0.29    0.45     0.68 dilm      -      es03\\n192.168.96.9            41          99   3    0.29    0.45     0.68 dilm      *      es02\\n\\nCluster setting:\\n/localhost:9200/_cluster/settings?pretty\\n{\\n  \"persistent\" : { },\\n  \"transient\" : {\\n    \"cluster\" : {\\n      \"routing\" : {\\n        \"allocation\" : {\\n          \"exclude\" : {\\n            \"rack_id\" : \"rack_1,rack_3,rack_2\"\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nNode (es04) setting:\\n  \"nodes\" : {\\n    \"PtXX3FXcR66MoOvkZKm3gg\" : {\\n      \"name\" : \"es04\",\\n      \"transport_address\" : \"192.168.96.10:9300\",\\n      \"host\" : \"192.168.96.10\",\\n      \"ip\" : \"192.168.96.10\",\\n      \"version\" : \"7.4.0\",\\n      \"build_flavor\" : \"default\",\\n      \"build_type\" : \"docker\",\\n      \"build_hash\" : \"22e1767283e61a198cb4db791ea66e3f11ab9910\",\\n      \"total_indexing_buffer\" : 51897958,\\n      \"roles\" : [\\n        \"ingest\",\\n        \"master\",\\n        \"data\",\\n        \"ml\"\\n      ],\\n      \"attributes\" : {\\n        \"rack_id\" : \"rack_1\",\\n        \"ml.machine_memory\" : \"15692754944\",\\n        \"ml.max_open_jobs\" : \"20\",\\n        \"xpack.installed\" : \"true\"\\n      },\\n      \"settings\" : {\\n        \"cluster\" : {\\n          \"initial_master_nodes\" : \"es01,es02,es03\",\\n          \"name\" : \"docker-cluster\",\\n          \"election\" : {\\n            \"strategy\" : \"supports_voting_only\"\\n          }\\n        },\\n        \"node\" : {\\n          \"attr\" : {\\n            \"rack_id\" : \"rack_1\",\\n            \"xpack\" : {\\n              \"installed\" : \"true\"\\n            },\\n            \"ml\" : {\\n              \"machine_memory\" : \"15692754944\",\\n              \"max_open_jobs\" : \"20\"\\n            }\\n          },\\n          \"name\" : \"es04\"\\n        },\\n        \"path\" : {\\n          \"logs\" : \"/usr/share/elasticsearch/logs\",\\n          \"home\" : \"/usr/share/elasticsearch\",\\n          \"repo\" : [\\n            \"/snapshot\"\\n          ]\\n        },\\n        \"discovery\" : {\\n          \"seed_hosts\" : \"es01,es02,es03\"\\n        },\\n        \"client\" : {\\n          \"type\" : \"node\"\\n        },\\n        \"http\" : {\\n          \"cors\" : {\\n            \"allow-origin\" : \"\\\\\"*\\\\\"\",\\n            \"allow-headers\" : \"\\'X-Requested-With, X-Auth-Token, Content-Type, Content-Length, Authorization, Access-Control-Allow-Headers, Accept\\'\",\\n            \"allow-credentials\" : \"true\",\\n            \"enabled\" : \"true\"\\n          },\\n          \"compression\" : \"false\",\\n          \"type\" : \"security4\",\\n          \"type.default\" : \"netty4\"\\n        },\\n        \"bootstrap\" : {\\n          \"memory_lock\" : \"true\"\\n        },\\n        \"transport\" : {\\n          \"type\" : \"security4\",\\n          \"features\" : {\\n            \"x-pack\" : \"true\"\\n          },\\n          \"type.default\" : \"netty4\"\\n        },\\n        \"xpack\" : {\\n          \"license\" : {\\n            \"self_generated\" : {\\n              \"type\" : \"trial\"\\n            }\\n          },\\n          \"security\" : {\\n            \"http\" : {\\n              \"ssl\" : {\\n                \"enabled\" : \"true\"\\n              }\\n            },\\n            \"enabled\" : \"true\",\\n            \"transport\" : {\\n              \"ssl\" : {\\n                \"enabled\" : \"true\"\\n              }\\n            }\\n          }\\n        },\\n        \"network\" : {\\n          \"host\" : \"0.0.0.0\"\\n        }\\n      },\\n      \"os\" : {\\n        \"refresh_interval_in_millis\" : 1000,\\n        \"name\" : \"Linux\",\\n        \"pretty_name\" : \"CentOS Linux 7 (Core)\",\\n        \"arch\" : \"amd64\",\\n        \"version\" : \"4.15.0-65-generic\",\\n        \"available_processors\" : 8,\\n        \"allocated_processors\" : 8\\n      },\\n      \"process\" : {\\n        \"refresh_interval_in_millis\" : 1000,\\n        \"id\" : 1,\\n        \"mlockall\" : true\\n      },\\n\\nWhy? Thanks'},\n",
       " {'text_field': 'The allocation explain API is the tool of choice for questions like this.\\n\\n  \\n      \\n      Elastic Blog – 25 Jan 17\\n  \\n  \\n    \\n\\nRED Elasticsearch Cluster? Panic no longer\\n\\nYour cluster is RED - you have unassigned shards and your cluster is not fully operational.  Introducing a new Elasticsearch API which will save the day.\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n'},\n",
       " {'text_field': 'Can I add symbols like for temperatures,  such as °C etc...?'},\n",
       " {'text_field': 'Solution. I never explored Kibana that much but I used the Visual Builder instead. There I could use \"Metric\", choose an index and value, and add the symbol in the template field: {{value}}°C'},\n",
       " {'text_field': \"Hi\\nIm just start in ES about some days\\nI have 3 machine in network\\nips: machine 1 : 10.255.255.6 ,machine 2 : 10.255.255.7, machine 3: 10.255.255.8\\ni want to setup a es cluster with machine 1 is a master node and two other are slave node.\\nI've installed ES 7.4 in 3 machine and started\\nbut when i check by _cluster/state?pretty\\ni cant see all node in cluster\\ni find some topic with same problem but i can's resolve it\\nanybody can give me a solution to fix ?\\nScreen Shot 2019-10-20 at 21.27.49.png1124×648 119 KB\\n\\nthank you\"},\n",
       " {'text_field': '@JustinLe\\nsorry I missed one thing here:\\nSee this documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-bootstrap-cluster.html#modules-discovery-bootstrap-cluster-joining\\nSince you already started all the nodes once without the proper configuration they formed individual \"clusters\". You\\'ll have to follow the steps in that link to merge them into a single cluster now.'},\n",
       " {'text_field': 'Подскажите в какую сторону искать и в чем может быть дело?\\nЕсли перезагрузить сервер - то именно elasticsearch лежит\\nПосле команды service elasticsearch restart - начинает нормально работать, но хочется привезти к норм виду - если сервер перегрузился по какой то причине - чтоб эластик работал.'},\n",
       " {'text_field': 'Какая ОС?\\nЕсли там systemd, то systemctl enable elasticsearch'},\n",
       " {'text_field': 'Hi. I wanna have sum of network usage per client in nginx. This is the logstash before changes:\\ngrok { match =&gt; { \"message\" =&gt; \"%{IP:clientip} ...  %{NUMBER:bytes}\" } }\\nso at first I changed the NUMBER to INT:\\ngrok { match =&gt; { \"message\" =&gt; \"%{IP:clientip} ...  %{INT:bytes}\" } }\\nbut it didn\\'t work and in elasticsearch it\\'s still text format and unable to do sum on it.\\nI added this line:\\nmutate { convert =&gt; [\"bytes\", \"integer\"] }\\nbut it\\'s not working yet. Do I need to delete index ? I prefer not to delete it.\\nthis is logstash output:\\noutput {\\nelasticsearch {\\nhosts =&gt; [\"localhost\"]\\nmanage_template =&gt; true\\nilm_enabled =&gt; false\\nindex =&gt; \"logstash-%{+YYYY.MM.dd}\"\\n}\\n}\\nThis is elasticsearch mapping:\\n\"bytes\": {\\n\"type\": \"text\",\\n\"norms\": false,\\n\"fields\": {\\n\"keyword\": {\\n\"type\": \"keyword\",\\n\"ignore_above\": 256\\n}\\n}\\n}\\nor can I use PUT command to change the \"bytes\" format ?\\nThanx'},\n",
       " {'text_field': '\\n\\n\\n Mohammad_Mousavi:\\n\\n%{NUMBER:bytes}\\n\\n\\nYou can convert that in grok using\\n%{NUMBER:bytes:int}\\n\\nHowever, since you have already indexed that field as text elasticsearch will convert int to text as it is indexed. You will need to re-index.'},\n",
       " {'text_field': 'Hi guys,\\nPeriodically in debug logs we can see:\\n[2019-10-20T18:07:48,342][DEBUG][o.e.a.a.c.n.t.c.TransportCancelTasksAction] [node-00] Received ban for the parent [s8jcicF9TDy_iIzFgH07zw:10741071] on the node [A9AdXLb5QA-ZMcicCn26OQ], reason: [by user request]\\n[2019-10-20T18:07:59,983][DEBUG][o.e.a.a.c.n.t.c.TransportCancelTasksAction] [node-00] Received ban for the parent [s8jcicF9TDy_iIzFgH07zw:10741223] on the node [A9AdXLb5QA-ZMcicCn26OQ], reason: [by user request]\\n[2019-10-20T18:08:06,033][DEBUG][o.e.a.a.c.n.t.c.TransportCancelTasksAction] [node-00] Removing ban for the parent [s8jcicF9TDy_iIzFgH07zw:10741071] on the node [A9AdXLb5QA-ZMcicCn26OQ]\\n[2019-10-20T18:08:06,126][DEBUG][o.e.a.a.c.n.t.c.TransportCancelTasksAction] [node-00] Removing ban for the parent [s8jcicF9TDy_iIzFgH07zw:10741223] on the node [A9AdXLb5QA-ZMcicCn26OQ]\\n\\nIn this time network traffic temporary is decreased\\n\\nimage.png812×277 34.2 KB\\n\\nWhat does it mean?\\nThanks\\nP.S. In logs UTC, on chart UTC+3'},\n",
       " {'text_field': \"Hi @Denis_Lamanov\\nThose logs mean that the node received cancellations for the given tasks.\\nSince the task had child tasks, a ban for the task was set on the node, which is basically just a flag that prevents any of the tasks children from executing.  (see code in the TaskManager class for details if you're interested)\"},\n",
       " {'text_field': \"We are indexing our Web Proxy logs and have a large number of usable fields...\\ntimestamp, URL visited, bytes transferred etc...\\nWhat I'd like to do is create a visualisation that aggregates the URL's by the sum of bytes transferred...\\nI can get a count of of URLS i.e. 50,000 hits to facebook.com; 5,000 hits to youtube.com etc etc, but that doesn't show that those 5,000 hits to YouTube transferred 3x the amount of bandwidth\\nSo what I'd like is to add-up all of the bytes transferred against {URLs}  and do a top_N results against that..\\ni.e.\\nURL1 --- 6,005,000 bytes\\nURL2 --- 4,001,000 bytes\\nURL3 --- 1,001,000 bytes\"},\n",
       " {'text_field': \"Solved \\nSo after playing around with the Visualization interface I was able to achieve exactly what I needed, I'll document this below in-case anyone else is trying to achieve the same results...\\n\\nCreate a new Visualization (type: TSVB)\\nTop N\\nAggregation = Sum (on field bytes)\\nSub Aggregation = Cumulative Sum on same field (bytes) shows up as Sum of bytes\\n\\nGroup by = Terms URL\\n\\n\\nThis will then group the results by URL and provide the cumulative sum of those results \\nPerfect\"},\n",
       " {'text_field': 'I\\'m getting this error in our some of our Grafana panels now after updating to 7.4 from 6.8:\\ntype:\"too_many_buckets_exception\"\\n\\nreason:\"Trying to create too many buckets. Must be less than or equal to: [10000] but was [10001]. This limit can be set by changing the [search.max_buckets] cluster level setting.\"\\n\\nElastic ref says:\\nThe maximum number of buckets allowed in a single response is limited by a dynamic cluster setting namedsearch.max_buckets. It defaults to 10,000, requests that try to return more than the limit will fail with an exception\\nSeeing this issue also after updating to 7.x, mentioning: \\'So be careful about making the limit too high\\'\\nWhat might be considered too high, when nodes trips over with OOM or...?'},\n",
       " {'text_field': 'You should decrease time period or increase group interval for charts in order to decrease  buckets count. Also Grafana should use composite aggregator or scroll API\\n\\n  \\n      github.com/grafana/grafana\\n  \\n  \\n    \\n\\n\\n\\nIssue: ElasticSearch 7.x too_many_buckets_exception\\n\\n\\n\\t\\n\\topened by bhozar\\n\\ton 2019-05-28\\n\\t\\n\\t\\n\\tclosed by marefr\\n\\ton \\n\\t\\n\\n\\nWhat happened:\\nUpgraded to ES 7.x and Grafana 6.2.x. Some panels relying on ES datasource was showing \"Unknown elastic error response\" in...\\n\\n\\n \\tdatasource/Elasticsearch\\n \\ttype/feature-request\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n'},\n",
       " {'text_field': 'Hello, I wish to do a vizualization in this style : radial viz but I want the elements of the viz (the marks) to be a clickable link to a webpage. Is it possible to do so in a kibana vega vizualization ?\\nI  heard about markdown viz with a link but I want the link to be an integral part of the vizualization.\\nThanks in advance.\\nHubert'},\n",
       " {'text_field': \"\\n  \\n      \\n      Vega-Lite\\n  \\n  \\n    \\n\\nScatterplot with External Links and Tooltips\\n\\nA scatterplot showing horsepower and miles per gallons that opens a Google search for the car that you click on.\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nTheres an example of what you're looking for by the sounds of it. External links have to be enabled in the Kibana.yml\"},\n",
       " {'text_field': 'I think I have found an issue with arrays of nested fields and bool matching.\\nI have a nested field which has multiple values where I want to search for records which have multiple values within it.  If I search for a single value, it works fine.  If I search for multiple values it does not.\\nI think I\\'ve created a somewhat minimal test case below.\\ncurl -X PUT \"localhost:9200/nested_bool?pretty=true&amp;refresh=true\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n    \"mappings\": {\\n        \"properties\": {\\n            \"export_commodities\": {\\n                \"type\": \"nested\",\\n                \"properties\": {\\n                    \"name\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\" } } }\\n                }\\n            }\\n        }\\n    }\\n}\\n\\'\\n\\ncurl -X POST \"localhost:9200/nested_bool/_doc/?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n    \"export_commodities\" : [{\"name\": \"Hydrogen Fuel\"},{\"name\": \"Animal Monitors\"}]\\n}\\n\\'\\n\\ncurl -X POST \"localhost:9200/nested_bool/_doc/?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n    \"export_commodities\" : [{\"name\": \"Hydrogen Fuel\"},{\"name\": \"Biowaste\"}]\\n}\\n\\'\\n\\ncurl -X POST \"localhost:9200/nested_bool/_doc/?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n    \"export_commodities\" : {\"name\":\" Biowaste\"}\\n}\\n\\'\\n\\ncurl -X POST \"localhost:9200/nested_bool/_doc/?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n    \"export_commodities\" : [{\"name\": \"Platinum\"},{\"name\": \"Animal Monitors\"}]\\n}\\n\\'\\n\\nThis search returns nothing\\ncurl -H \\'Content-Type: application/json\\' -X GET \"http://localhost:9200/nested_bool/_search?pretty=true&amp;size=10\" -d \\'\\n{\\n    \"query\": {\\n        \"nested\": {\\n            \"path\": \"export_commodities\",\\n            \"query\": {\\n                \"bool\": {\\n                    \"filter\": [\\n                        {\\n                            \"term\": {\\n                                \"export_commodities.name.keyword\": \"Animal Monitors\"\\n                            }\\n                        },\\n                        {\\n                            \"term\": {\\n                                \"export_commodities.name.keyword\": \"Hydrogen Fuel\"\\n                            }\\n                        }\\n                    ]\\n                }\\n            }\\n        }\\n    }\\n}\\n\\'\\n\\nThis search returns as expected\\ncurl -H \\'Content-Type: application/json\\' -X GET \"http://localhost:9200/nested_bool/_search?pretty=true&amp;size=10\" -d \\'\\n{\\n    \"query\": {\\n        \"nested\": {\\n            \"path\": \"export_commodities\",\\n            \"query\": {\\n                \"bool\": {\\n                    \"filter\": [\\n                        {\\n                            \"term\": {\\n                                \"export_commodities.name.keyword\": \"Hydrogen Fuel\"\\n                            }\\n                        }\\n                    ]\\n                }\\n            }\\n        }\\n    }\\n}\\n\\'\\n\\nI\\'m happy to register an issue on github, but I thought it would be best to ask here first in case I\\'ve missed something really obvious.'},\n",
       " {'text_field': 'Try this instead:\\nGET /nested_bool/_search\\n{\\n  \"query\": {\\n    \"bool\": {\\n      \"filter\": [\\n        {\\n          \"nested\": {\\n            \"path\": \"export_commodities\",\\n            \"query\": {\\n              \"term\": {\\n                \"export_commodities.name.keyword\": \"Animal Monitors\"\\n              }\\n            }\\n          }\\n        },\\n        {\\n          \"nested\": {\\n            \"path\": \"export_commodities\",\\n            \"query\": {\\n              \"term\": {\\n                \"export_commodities.name.keyword\": \"Hydrogen Fuel\"\\n              }\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}\\n'},\n",
       " {'text_field': 'I am using Elasticsearch 6.5. I am writing a script for an update API in painless. I need to add structures to an array of structures. If this field does not exist in the document (I can detect that) I am creating a new array with the first element:\\nctx._source.myStructArr = new def[] {struct_1};\\n\\nLater I want to add additional structures:\\nctx._source.myStructArr.add(struct_n);\\n\\nbut I receive the following error:\\n\"type\": \"illegal_argument_exception\",\\n\"reason\": \"dynamic method [java.lang.Object[], add/1] not found\"\\n\\nHow can I add an element to this array? Or maybe I should create/initialize an array in a different way?'},\n",
       " {'text_field': \"An array is fixed size, so what you're seeing here is that you cannot add an element to a fixed size array. If you want to add elements, create an ArrayList instead, using this syntax:\\nctx._source.myStructArr = [ struct_1 ];\\n\"},\n",
       " {'text_field': 'If I want to use the life cycle, then the index is named after the end of 00001?'},\n",
       " {'text_field': 'Yes so if your old index is named eg. logs-000001, your new index would be named: logs-000002.\\nYou can refer to this docs page.'},\n",
       " {'text_field': 'I have setup winlogbeat according to instructions, however logs do not show up in Kibana, despite log stating \"successfully published\".\\n\\n\\n\\n\\n2019-10-21T11:37:59.647+0200\\nWARN\\nelasticsearch/client.go:535\\nCannot index event publisher.Event{Content:beat.Event{Timestamp:time.Time{wall:0x946c75c, ext:63707247477, loc:(*time.Location)(nil)}, Meta:common.MapStr(nil), Fields:common.MapStr{\"agent\":common.MapStr{\"ephemeral_id\":\"10e9afc2-1576-4643-86d2-0ef0070fcca7\", \"hostname\":\"LAPTOP-FEM9SVF4\", \"id\":\"0349223d-36a1-485d-bd95-8aa5e07a3a6c\", \"type\":\"winlogbeat\", \"version\":\"7.4.0\"}, \"ecs\":common.MapStr{\"version\":\"1.1.0\"}, \"event\":common.MapStr{\"action\":\"User Account Management\", \"code\":0x12be, \"created\":common.Time{wall:0x24355bdc, ext:63707247478, loc:(*time.Location)(nil)}, \"kind\":\"event\"}, \"host\":common.MapStr{\"architecture\":\"x86_64\", \"hostname\":\"LAPTOP-FEM9SVF4\", \"id\":\"7f1b6202-cde5-4247-b9ab-da3c0724d18b\", \"name\":\"LAPTOP-FEM9SVF4\", \"os\":common.MapStr{\"build\":\"18362.418\", \"family\":\"windows\", \"kernel\":\"10.0.18362.418 (WinBuild.160101.0800)\", \"name\":\"Windows 10 Pro\", \"platform\":\"windows\", \"version\":\"10.0\"}}, \"log\":common.MapStr{\"level\":\"information\"}, \"message\":\"A user\\'s local group membership was enumerated.\\\\n\\\\nSubject:\\\\n\\\\tSecurity ID:\\\\t\\\\tS-1-5-18\\\\n\\\\tAccount Name:\\\\t\\\\tLAPTOP-FEM9SVF4$\\\\n\\\\tAccount Domain:\\\\t\\\\tWORKGROUP\\\\n\\\\tLogon ID:\\\\t\\\\t0x3E7\\\\n\\\\nUser:\\\\n\\\\tSecurity ID:\\\\t\\\\tS-1-5-21-3162102966-2696753098-2875345929-1001\\\\n\\\\tAccount Name:\\\\t\\\\tArtjoms Jakovenko\\\\n\\\\tAccount Domain:\\\\t\\\\tLAPTOP-FEM9SVF4\\\\n\\\\nProcess Information:\\\\n\\\\tProcess ID:\\\\t\\\\t0x2ee0\\\\n\\\\tProcess Name:\\\\t\\\\tC:\\\\Windows\\\\System32\\\\LogonUI.exe\", \"winlog\":common.MapStr{\"activity_id\":\"{83670da8-8403-0002-4f0e-67830384d501}\", \"api\":\"wineventlog\", \"channel\":\"Security\", \"computer_name\":\"LAPTOP-FEM9SVF4\", \"event_data\":common.MapStr{\"CallerProcessId\":\"0x2ee0\", \"CallerProcessName\":\"C:\\\\Windows\\\\System32\\\\LogonUI.exe\", \"SubjectDomainName\":\"WORKGROUP\", \"SubjectLogonId\":\"0x3e7\", \"SubjectUserName\":\"LAPTOP-FEM9SVF4$\", \"SubjectUserSid\":\"S-1-5-18\", \"TargetDomainName\":\"LAPTOP-FEM9SVF4\", \"TargetSid\":\"S-1-5-21-3162102966-2696753098-2875345929-1001\", \"TargetUserName\":\"Artjoms Jakovenko\"}, \"event_id\":0x12be, \"keywords\":string{\"Audit Success\"}, \"opcode\":\"Info\", \"process\":common.MapStr{\"pid\":0x2fc, \"thread\":common.MapStr{\"id\":0x359c}}, \"provider_guid\":\"{54849625-5478-4994-a5ba-3e3b0328c30d}\", \"provider_name\":\"Microsoft-Windows-Security-Auditing\", \"record_id\":0xbee7, \"task\":\"User Account Management\"}}, Private:checkpoint.EventLogState{Name:\"Security\", RecordNumber:0xbee7, Timestamp:time.Time{wall:0x946c75c, ext:63707247477, loc:(*time.Location)(nil)}, Bookmark:\"\\\\r\\\\n  \\\\r\\\\n\"}, TimeSeries:false}, Flags:0x1} (status=400): {\"type\":\"invalid_index_name_exception\",\"reason\":\"Invalid index name [winLogs], must be lowercase\",\"index_uuid\":\"na\",\"index\":\"winLogs\"}\\n\\n\\n\\n\\n2019-10-21T11:37:59.647+0200\\nINFO\\nbeater/eventlogger.go:76\\nEventLog[Security] successfully published 1 events\\n\\n\\n\\n#======================= Winlogbeat specific options ===========================\\nwinlogbeat.event_logs:\\n\\n\\nname: Application\\nignore_older: 72h\\n\\n\\nname: System\\n\\n\\nname: Security\\nprocessors:\\n\\nscript:\\nlang: javascript\\nid: security\\nfile: ${path.home}/module/security/config/winlogbeat-security.js\\n\\n\\n\\nname: Microsoft-Windows-Sysmon/Operational\\nprocessors:\\n\\nscript:\\nlang: javascript\\nid: sysmon\\nfile: ${path.home}/module/sysmon/config/winlogbeat-sysmon.js\\n\\n\\n\\n#==================== Elasticsearch template settings ==========================\\nsetup.template.name: \"winlogbeat\"\\nsetup.template.pattern: \"winlogbeat-*\"\\nsetup.template.settings:\\nindex.number_of_shards: 1\\n#============================== Kibana =====================================\\nsetup.kibana:\\ncloud.id: \"CONFIDENTIALFORQUESTION\"\\ncloud.auth: \"CONFIDENTIALFORQUESTION\"\\n#-------------------------- Elasticsearch output ------------------------------\\noutput.elasticsearch:\\nindex: \"winLogs\"\\n#================================ Processors =====================================\\nprocessors:\\n\\nadd_host_metadata: ~\\nadd_cloud_metadata: ~\\n'},\n",
       " {'text_field': '\\n\\n\\n ajawm:\\n\\nstatus=400): {\"type\":\"invalid_index_name_exception\",\"reason\":\"Invalid index name [winLogs], must be lowercase\",\"index_uuid\":\" na \",\"index\":\"winLogs\"}\\n\\n\\nElasticsearch requires index names to be lowercase.\\nI would not recommend customizing the index name if this is your first time using Winlogbeat. Give it a try first with the defaults, and then read a bit more about Elasticsearch indices, mappings, and templates. When you change the index name you will need to change the index template pattern (setup.template.pattern) to match the index naming scheme that you have chosen.'},\n",
       " {'text_field': 'I am using logstash-7.4.0 , jdk-11.0.5  and Microsoft JDBC Driver 7.4 for SQL Server.\\nI am getting an jdbc connection issue . Any idea ?\\nbatch.delay\"=&gt;50, \"pipeline.max_inflight\"=&gt;500, :thread=&gt;\"#&lt;Thread:0x2dcedb2a run&gt;\"}\\n[2019-10-21T15:52:48,968][INFO ][logstash.outputs.elasticsearch][main] Attempting to install template {:manage_template=&gt;{\"index_patterns\"=&gt;\"logstash-\", \"version\"=&gt;60001, \"settings\"=&gt;{\"index.refresh_interval\"=&gt;\"5s\", \"number_of_shards\"=&gt;1}, \"mappings\"=&gt;{\"dynamic_templates\"=&gt;[{\"message_field\"=&gt;{\"path_match\"=&gt;\"message\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false}}}, {\"string_fields\"=&gt;{\"match\"=&gt;\"\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false, \"fields\"=&gt;{\"keyword\"=&gt;{\"type\"=&gt;\"keyword\", \"ignore_above\"=&gt;256}}}}}], \"properties\"=&gt;{\"@timestamp\"=&gt;{\"type\"=&gt;\"date\"}, \"@version\"=&gt;{\"type\"=&gt;\"keyword\"}, \"geoip\"=&gt;{\"dynamic\"=&gt;true, \"properties\"=&gt;{\"ip\"=&gt;{\"type\"=&gt;\"ip\"}, \"location\"=&gt;{\"type\"=&gt;\"geo_point\"}, \"latitude\"=&gt;{\"type\"=&gt;\"half_float\"}, \"longitude\"=&gt;{\"type\"=&gt;\"half_float\"}}}}}}}\\n[2019-10-21T15:52:49,321][INFO ][logstash.javapipeline    ][main] Pipeline started {\"pipeline.id\"=&gt;\"main\"}\\n[2019-10-21T15:52:49,422][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;}\\n[2019-10-21T15:52:49,988][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n[2019-10-21T15:52:50,109][ERROR][logstash.inputs.jdbc     ][main] Failed to load C:\\\\Program Files\\\\sqljdbc_7.4\\\\enu\\\\mssql-jdbc-7.4.1.jre8.jar {:exception=&gt;#&lt;TypeError: failed to coerce jdk.internal.loader.ClassLoaders$AppClassLoader to java.net.URLClassLoader&gt;}\\n[2019-10-21T15:52:50,180][ERROR][logstash.javapipeline    ][main] A plugin had an unrecoverable error. Will restart this plugin.\\nPipeline_id:main\\nPlugin: &lt;LogStash::Inputs::Jdbc jdbc_user=&gt;\"ess\", jdbc_password=&gt;, statement=&gt;\"SELECT * FROM [UsersDB].dbo.[Employee]\", jdbc_driver_library=&gt;\"C:\\\\Program Files\\\\sqljdbc_7.4\\\\enu\\\\mssql-jdbc-7.4.1.jre8.jar\", jdbc_connection_string=&gt;\"jdbc:sqlserver://TLTTVMLT123\\\\SQLEXPRESS;user=ess;password=admin123*;\", id=&gt;\"b9d05cd015a29052bcfc052e5878dcf2e65fe1cb6aa0dbdf98ae9d6ece8bb8bf\", jdbc_driver_class=&gt;\"com.microsoft.sqlserver.jdbc.SQLServerDriver\", enable_metric=&gt;true, codec=&gt;&lt;LogStash::Codecs::Plain id=&gt;\"plain_9cf811ba-2763-49c1-be3e-af4fa039eece\", enable_metric=&gt;true, charset=&gt;\"UTF-8\"&gt;, jdbc_paging_enabled=&gt;false, jdbc_page_size=&gt;100000, jdbc_validate_connection=&gt;false, jdbc_validation_timeout=&gt;3600, jdbc_pool_timeout=&gt;5, sql_log_level=&gt;\"info\", connection_retry_attempts=&gt;1, connection_retry_attempts_wait_time=&gt;0.5, parameters=&gt;{\"sql_last_value\"=&gt;1970-01-01 00:00:00 UTC}, last_run_metadata_path=&gt;\"C:\\\\Users\\\\User/.logstash_jdbc_last_run\", use_column_value=&gt;false, tracking_column_type=&gt;\"numeric\", clean_run=&gt;false, record_last_run=&gt;true, lowercase_column_names=&gt;true&gt;\\nError: com.microsoft.sqlserver.jdbc.SQLServerDriver not loaded. Are you sure you\\'ve included the correct jdbc driver in :jdbc_driver_library?\\nException: LogStash::ConfigurationError\\nStack: C:/Users/User/Downloads/logstash-7.4.0/logstash-7.4.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/jdbc.rb:163:in open_jdbc_connection\\' C:/Users/User/Downloads/logstash-7.4.0/logstash-7.4.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/jdbc.rb:221:inexecute_statement\\'\\nC:/Users/User/Downloads/logstash-7.4.0/logstash-7.4.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/inputs/jdbc.rb:277:in execute_query\\' C:/Users/User/Downloads/logstash-7.4.0/logstash-7.4.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/inputs/jdbc.rb:263:inrun\\'\\nC:/Users/User/Downloads/logstash-7.4.0/logstash-7.4.0/logstash-core/lib/logstash/java_pipeline.rb:314:in inputworker\\' C:/Users/User/Downloads/logstash-7.4.0/logstash-7.4.0/logstash-core/lib/logstash/java_pipeline.rb:306:inblock in start_input\\'\\n[2019-10-21T15:52:51,206][ERROR][logstash.inputs.jdbc     ][main] Failed to load C:\\\\Program Files\\\\sqljdbc_7.4\\\\enu\\\\mssql-jdbc-7.4.1.jre8.jar {:exception=&gt;#&lt;TypeError: failed to coerce jdk.internal.loader.ClassLoaders$AppClassLoader to java.net.URLClassLoader&gt;}'},\n",
       " {'text_field': '\\n\\n\\n adarsh-c:\\n\\nexception=&gt;#&lt;TypeError: failed to coerce jdk.internal.loader.ClassLoaders$AppClassLoader to java.net.URLClassLoader&gt;\\n\\n\\nRead this.'},\n",
       " {'text_field': \"I have a little problem.\\nShould I do something to make Kibana see Logstash?\\nI don't see any information about Logstash in the Kibany web panel.\\nMaybe I need to configure it additionally?\\nLogstash is integrated with Elastic Search and works well.\\nELK - ver. 6.8.3.\\nAny idea?\"},\n",
       " {'text_field': 'Are you talking about monitoring logstash from Kibana ?\\nMay be this link may help you : https://www.elastic.co/guide/en/logstash/current/monitoring-internal-collection.html'},\n",
       " {'text_field': 'Hi,\\nI have data in which user_id  field appears and what i  want is to show in a metric on dashboard the number of users, Unique(user_id appeared only once) and returning users(user_id appeared more than once).\\nThanks in advance'},\n",
       " {'text_field': 'Have a look at https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html'},\n",
       " {'text_field': 'Hello,\\nI\\'m struggling to set up filebeats with postrgesql module in order to ship postgresql logs to Elastic.\\nThe logs can be seen in Kibana, however they are not parsed and present following error message\\nProvided Grok expressions do not match field value\\n\\nI\\' m struggling to understand how I can tell filebeats, what is the proper log pattern\\nIn PG, I have set following\\nlog_line_prefix = \\'%t [%p]: db=%d,user=%u,app=%a,client=%h \\'\\t\\n\\t\\t\\t\\t\\t# special values:\\n\\t\\t\\t\\t\\t#   %a = application name\\n\\t\\t\\t\\t\\t#   %u = user name\\n\\t\\t\\t\\t\\t#   %d = database name\\n\\t\\t\\t\\t\\t#   %r = remote host and port\\n\\t\\t\\t\\t\\t#   %h = remote host\\n\\t\\t\\t\\t\\t#   %p = process ID\\n\\t\\t\\t\\t\\t#   %t = timestamp without milliseconds\\n\\t\\t\\t\\t\\t#   %m = timestamp with milliseconds\\n\\t\\t\\t\\t\\t#   %n = timestamp with milliseconds (as a Unix epoch)\\n\\t\\t\\t\\t\\t#   %i = command tag\\n\\t\\t\\t\\t\\t#   %e = SQL state\\n\\t\\t\\t\\t\\t#   %c = session ID\\n\\t\\t\\t\\t\\t#   %l = session line number\\n\\t\\t\\t\\t\\t#   %s = session start timestamp\\n\\t\\t\\t\\t\\t#   %v = virtual transaction ID\\n\\t\\t\\t\\t\\t#   %x = transaction ID (0 if none)\\n\\t\\t\\t\\t\\t#   %q = stop here in non-session processes\\n\\t\\t\\t\\t\\t#   %% = \\'%\\'\\n\\t\\t\\t\\t\\t# e.g. \\'&lt;%u%%%d&gt; \\'\\n\\n{\\n  \"_index\": \"filebeat-7.4.0-2019.10.21-000001\",\\n  \"_type\": \"_doc\",\\n  \"_id\": \"HrIw7m0Bye1teIScpU9u\",\\n  \"_version\": 1,\\n  \"_score\": null,\\n  \"_source\": {\\n    \"agent\": {\\n      \"hostname\": \"ms-vg-stats-db-01-prod.mnw.no\",\\n      \"id\": \"4569fe1c-798f-4eee-9a3f-ffe29f6de791\",\\n      \"type\": \"filebeat\",\\n      \"ephemeral_id\": \"1795d61c-2a2d-4e73-a565-7aee5574c09c\",\\n      \"version\": \"7.4.0\"\\n    },\\n    \"log\": {\\n      \"file\": {\\n        \"path\": \"/var/log/postgresql/postgresql-Mon.log\"\\n      },\\n      \"offset\": 53929347\\n    },\\n    \"message\": \"2019-10-21 14:00:26 CEST [3176]: db=postgres,user=patroni,app=Patroni,client=127.0.0.1 LOG:  statement: WITH replication_info AS (SELECT usename, application_name, client_addr, state, sync_state, sync_priority FROM pg_catalog.pg_stat_replication) SELECT pg_catalog.to_char(pg_catalog.pg_postmaster_start_time(), \\'YYYY-MM-DD HH24:MI:SS.MS TZ\\'), CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 ELSE (\\'x\\' || pg_catalog.substr(pg_catalog.pg_walfile_name(pg_catalog.pg_current_wal_lsn()), 1, 8))::bit(32)::int END, CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 ELSE pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), \\'0/0\\')::bigint END, pg_catalog.pg_wal_lsn_diff(COALESCE(pg_catalog.pg_last_wal_receive_lsn(), pg_catalog.pg_last_wal_replay_lsn()), \\'0/0\\')::bigint, pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_last_wal_replay_lsn(), \\'0/0\\')::bigint, pg_catalog.to_char(pg_catalog.pg_last_xact_replay_timestamp(), \\'YYYY-MM-DD HH24:MI:SS.MS TZ\\'), pg_catalog.pg_is_in_recovery() AND pg_catalog.pg_is_wal_replay_paused(), (SELECT pg_catalog.array_to_json(pg_catalog.array_agg(pg_catalog.row_to_json(ri))) FROM replication_info ri)\",\\n    \"fileset\": {\\n      \"name\": \"log\"\\n    },\\n    \"error\": {\\n      \"message\": \"Provided Grok expressions do not match field value: [2019-10-21 14:00:26 CEST [3176]: db=postgres,user=patroni,app=Patroni,client=127.0.0.1 LOG:  statement: WITH replication_info AS (SELECT usename, application_name, client_addr, state, sync_state, sync_priority FROM pg_catalog.pg_stat_replication) SELECT pg_catalog.to_char(pg_catalog.pg_postmaster_start_time(), \\'YYYY-MM-DD HH24:MI:SS.MS TZ\\'), CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 ELSE (\\'x\\' || pg_catalog.substr(pg_catalog.pg_walfile_name(pg_catalog.pg_current_wal_lsn()), 1, 8))::bit(32)::int END, CASE WHEN pg_catalog.pg_is_in_recovery() THEN 0 ELSE pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_current_wal_lsn(), \\'0/0\\')::bigint END, pg_catalog.pg_wal_lsn_diff(COALESCE(pg_catalog.pg_last_wal_receive_lsn(), pg_catalog.pg_last_wal_replay_lsn()), \\'0/0\\')::bigint, pg_catalog.pg_wal_lsn_diff(pg_catalog.pg_last_wal_replay_lsn(), \\'0/0\\')::bigint, pg_catalog.to_char(pg_catalog.pg_last_xact_replay_timestamp(), \\'YYYY-MM-DD HH24:MI:SS.MS TZ\\'), pg_catalog.pg_is_in_recovery() AND pg_catalog.pg_is_wal_replay_paused(), (SELECT pg_catalog.array_to_json(pg_catalog.array_agg(pg_catalog.row_to_json(ri))) FROM replication_info ri)]\"\\n    },\\n    \"input\": {\\n      \"type\": \"log\"\\n    },\\n    \"@timestamp\": \"2019-10-21T12:00:27.522Z\",\\n    \"ecs\": {\\n      \"version\": \"1.1.0\"\\n    },\\n    \"service\": {\\n      \"type\": \"postgresql\"\\n    },\\n    \"host\": {\\n      \"hostname\": \"ms-vg-stats-db-01-prod.mnw.no\",\\n      \"os\": {\\n        \"kernel\": \"4.18.0-80.11.2.el8_0.x86_64\",\\n        \"codename\": \"Core\",\\n        \"name\": \"CentOS Linux\",\\n        \"family\": \"redhat\",\\n        \"version\": \"8 (Core)\",\\n        \"platform\": \"centos\"\\n      },\\n      \"containerized\": false,\\n      \"name\": \"ms-vg-stats-db-01-prod.mnw.no\",\\n      \"id\": \"875bf5a456aa4f9dab3b7f184af3e63b\",\\n      \"architecture\": \"x86_64\"\\n    },\\n    \"event\": {\\n      \"module\": \"postgresql\",\\n      \"dataset\": \"postgresql.log\"\\n    }\\n  },\\n  \"fields\": {\\n    \"suricata.eve.timestamp\": [\\n      \"2019-10-21T12:00:27.522Z\"\\n    ],\\n    \"@timestamp\": [\\n      \"2019-10-21T12:00:27.522Z\"\\n    ]\\n  },\\n  \"sort\": [\\n    1571659227522\\n  ]\\n}\\n\\nAnd the log pipeline filebeat has added is as follows\\nGET /_ingest/pipeline/filebeat-7.4.0-postgresql-log-pipeline\\n{\\n  \"filebeat-7.4.0-postgresql-log-pipeline\": {\\n    \"description\": \"Pipeline for parsing PostgreSQL logs.\",\\n    \"processors\": [\\n      {\\n        \"grok\": {\\n          \"field\": \"message\",\\n          \"ignore_missing\": true,\\n          \"patterns\": [\\n            \"^%{DATETIME:postgresql.log.timestamp} \\\\\\\\[%{NUMBER:process.pid:long}(-%{BASE16FLOAT:postgresql.log.core_id:long})?\\\\\\\\] ((\\\\\\\\[%{USERNAME:user.name}\\\\\\\\]@\\\\\\\\[%{POSTGRESQL_DB_NAME:postgresql.log.database}\\\\\\\\]|%{USERNAME:user.name}@%{POSTGRESQL_DB_NAME:postgresql.log.database}) )?%{WORD:log.level}:  (?:%{NUMBER:postgresql.log.error.code:long}|%{SPACE})(duration: %{NUMBER:temp.duration:float} ms  statement: %{GREEDYDATA:postgresql.log.query}|: %{GREEDYDATA:message}|%{GREEDYDATA:message})\"\\n          ],\\n          \"pattern_definitions\": {\\n            \"DATETIME\": \"[-0-9]+ %{TIME} %{WORD:event.timezone}\",\\n            \"GREEDYDATA\": \"(.|\\\\n|\\\\t)*\",\\n            \"POSTGRESQL_DB_NAME\": \"[a-zA-Z0-9_]+[a-zA-Z0-9_\\\\\\\\$]*\"\\n          }\\n        }\\n      },\\n      {\\n        \"date\": {\\n          \"field\": \"postgresql.log.timestamp\",\\n          \"target_field\": \"@timestamp\",\\n          \"formats\": [\\n            \"yyyy-MM-dd HH:mm:ss.SSS zz\",\\n            \"yyyy-MM-dd HH:mm:ss zz\"\\n          ]\\n        }\\n      },\\n      {\\n        \"script\": {\\n          \"source\": \"ctx.event.duration = Math.round(ctx.temp.duration * params.scale)\",\\n          \"params\": {\\n            \"scale\": 1000000.0\\n          },\\n          \"if\": \"ctx.temp?.duration != null\",\\n          \"lang\": \"painless\"\\n        }\\n      },\\n      {\\n        \"remove\": {\\n          \"field\": \"temp.duration\",\\n          \"ignore_missing\": true\\n        }\\n      }\\n    ],\\n    \"on_failure\": [\\n      {\\n        \"set\": {\\n          \"field\": \"error.message\",\\n          \"value\": \"{{ _ingest.on_failure_message }}\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nNot an expert in grok, but it indeed does not look compatible with what PG is outputting.'},\n",
       " {'text_field': 'Solved by changing the pipeline pattern\\nPUT /_ingest/pipeline/filebeat-7.4.0-postgresql-log-pipeline\\n{\\n    \"description\": \"Pipeline for parsing PostgreSQL logs.\",\\n    \"processors\": [\\n      {\\n        \"grok\": {\\n          \"ignore_missing\": true,\\n          \"patterns\": [\\n            \"^%{DATETIME:postgresql.log.timestamp} \\\\\\\\[%{NUMBER:process.pid:long}\\\\\\\\]: db=%{POSTGRESQL_DB_NAME:postgresql.log.database},user=%{USERNAME:user.name},app=%{DATA:postgresql.log.app},client=%{IP:postgresql.log.client} %{WORD:log.level}:  (?:%{NUMBER:postgresql.log.error.code:long}|%{SPACE})(duration: %{NUMBER:temp.duration:float} ms  statement: %{GREEDYDATA:postgresql.log.query}|: %{GREEDYDATA:message}|%{GREEDYDATA:message})\"\\n          ],\\n          \"pattern_definitions\": {\\n            \"DATETIME\": \"[-0-9]+ %{TIME} %{WORD:event.timezone}\",\\n            \"GREEDYDATA\": \"(.|\\\\n|\\\\t)*\",\\n            \"POSTGRESQL_DB_NAME\": \"[a-zA-Z0-9_]+[a-zA-Z0-9_\\\\\\\\$]*\"\\n          },\\n          \"field\": \"message\"\\n        }\\n      },\\n      {\\n        \"date\": {\\n          \"target_field\": \"@timestamp\",\\n          \"formats\": [\\n            \"yyyy-MM-dd HH:mm:ss.SSS zz\",\\n            \"yyyy-MM-dd HH:mm:ss zz\"\\n          ],\\n          \"field\": \"postgresql.log.timestamp\"\\n        }\\n      },\\n      {\\n        \"script\": {\\n          \"lang\": \"painless\",\\n          \"source\": \"ctx.event.duration = Math.round(ctx.temp.duration * params.scale)\",\\n          \"params\": {\\n            \"scale\": 1000000.0\\n          },\\n          \"if\": \"ctx.temp?.duration != null\"\\n        }\\n      },\\n      {\\n        \"remove\": {\\n          \"field\": \"temp.duration\",\\n          \"ignore_missing\": true\\n        }\\n      }\\n    ],\\n    \"on_failure\": [\\n      {\\n        \"set\": {\\n          \"field\": \"error.message\",\\n          \"value\": \"{{ _ingest.on_failure_message }}\"\\n        }\\n      }\\n    ]\\n  }\\n'},\n",
       " {'text_field': 'Hello all,\\nI am trying to create an index and trying to define mappings for index.\\nMy Elasticsearch version is 7.3.2.\\nBelow is the code snippet I am trying.\\nPUT dish\\n{\\n  \"settings\": {\\n    \"index\": {\\n      \"number_of_shards\": 1,\\n      \"number_of_replicas\": 1\\n    },\\n    \"analysis\": {\\n      \"analyzer\": {\\n        \"lowercase_keyword_analyzer\": {\\n          \"tokenizer\": \"keyword\",\\n          \"filter\": [\\n            \"lowercase\"\\n          ]\\n        },\\n        \"suggestion_analyzer\": {\\n          \"tokenizer\": \"standard\",\\n          \"filter\": [\\n            \"lowercase\"\\n          ]\\n        }\\n      },\\n      \"normalizer\": {\\n        \"lowercase_normalizer\": {\\n          \"type\": \"custom\",\\n          \"char_filter\": [],\\n          \"filter\": [\\n            \"lowercase\"\\n          ]\\n        }\\n      }\\n    },\\n    \"mappings\": {\\n      \"dynamic_templates\": [\\n        {\\n          \"search_result_data\": {\\n            \"mapping\": {\\n              \"type\": \"keyword\",\\n              \"index\": \"false\"\\n            },\\n            \"path_match\": \"search_result_data.*\"\\n          }\\n        },\\n        {\\n          \"scores\": {\\n            \"mapping\": {\\n              \"type\": \"double\"\\n            },\\n            \"path_match\": \"scores.*\"\\n          }\\n        },\\n        {\\n          \"category_scores\": {\\n            \"mapping\": {\\n              \"type\": \"integer\"\\n            },\\n            \"path_match\": \"category_scores.*\"\\n          }\\n        },\\n        {\\n          \"category\": {\\n            \"mapping\": {\\n              \"type\": \"keyword\",\\n              \"index\": \"not_analyzed\"\\n            },\\n            \"path_match\": \"category.*\"\\n          }\\n        },\\n        {\\n          \"string_sort\": {\\n            \"mapping\": {\\n              \"analyzer\": \"lowercase_keyword_analyzer\",\\n              \"type\": \"keyword\"\\n            },\\n            \"path_match\": \"string_sort.*\"\\n          }\\n        },\\n        {\\n          \"number_sort\": {\\n            \"mapping\": {\\n              \"index\": \"not_analyzed\",\\n              \"type\": \"double\"\\n            },\\n            \"path_match\": \"number_sort.*\"\\n          }\\n        }\\n      ],\\n      \"properties\": {\\n        \"search_data\": {\\n          \"type\": \"nested\",\\n          \"properties\": {\\n            \"full_text\": {\\n              \"type\": \"keyword\",\\n              \"index_analyzer\": \"standard\",\\n              \"search_analyzer\": \"standard\",\\n              \"fields\": {\\n                \"no-decompound\": {\\n                  \"type\": \"keyword\",\\n                  \"index_analyzer\": \"standard\",\\n                  \"search_analyzer\": \"standard\"\\n                },\\n                \"no-stem\": {\\n                  \"type\": \"keyword\",\\n                  \"index_analyzer\": \"standard\",\\n                  \"search_analyzer\": \"standard\"\\n                }\\n              }\\n            },\\n            \"full_text_boosted\": {\\n              \"type\": \"keyword\",\\n              \"index_analyzer\": \"standard\",\\n              \"search_analyzer\": \"standard\",\\n              \"fields\": {\\n                \"edge\": {\\n                  \"type\": \"keyword\",\\n                  \"index_analyzer\": \"standard\",\\n                  \"search_analyzer\": \"standard\"\\n                },\\n                \"no-decompound\": {\\n                  \"type\": \"keyword\",\\n                  \"index_analyzer\": \"standard\",\\n                  \"search_analyzer\": \"standard\"\\n                },\\n                \"no-stem\": {\\n                  \"type\": \"keyword\",\\n                  \"index_analyzer\": \"standard\",\\n                  \"search_analyzer\": \"standard\"\\n                }\\n              }\\n            },\\n            \"string_facet\": {\\n              \"type\": \"nested\",\\n              \"properties\": {\\n                \"facet-name\": {\\n                  \"type\": \"keyword\",\\n                  \"index\": \"not_analyzed\"\\n                },\\n                \"facet-value\": {\\n                  \"type\": \"keyword\",\\n                  \"index\": \"not_analyzed\"\\n                }\\n              }\\n            },\\n            \"number_facet\": {\\n              \"type\": \"nested\",\\n              \"properties\": {\\n                \"facet-name\": {\\n                  \"type\": \"keyword\",\\n                  \"index\": \"not_analyzed\"\\n                },\\n                \"facet-value\": {\\n                  \"type\": \"double\"\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nThe output I am getting :\\n{\\n  \"error\": {\\n    \"root_cause\": [\\n      {\\n        \"type\": \"settings_exception\",\\n        \"reason\": \"Failed to load settings from ...\"\\n      }\\n    ],\\n    \"type\": \"settings_exception\",\\n    \"reason\": \"Failed to load settings from ...\",\\n    \"caused_by\": {\\n      \"type\": \"illegal_state_exception\",\\n      \"reason\": \"only value lists are allowed in serialized settings\"\\n    }\\n  },\\n  \"status\": 500\\n} \\n\\nThe error only pops up when i try to have properties in my mappings.\\nWhat could be the reason for this?'},\n",
       " {'text_field': 'mappings should be a top-level key in the body instead of a child of settings.'},\n",
       " {'text_field': 'Hello all. Long time lurker, first time poster.\\nUsing Elastic Stack 7.4.\\nI have spent the better part of the past 2 weeks setting up Heartbeats to monitor all of my internal systems and services. It was a ton of work but I finally got it working - almost.\\nI have 3 systems that insist on using https to connect, but I have not installed CA-signed certificates for those systems. When Heartbeat tries to connect to those systems I get the following error messages in the index document:\\nerror.message Get https://xxx.mydomain.com: x509: certificate is valid for localhost.mydomain.com, not xxx.mydomain.com\\nor\\nerror.message Get https://yyy.mydomain.com: x509: certificate is not valid for any names, but wanted to match yyy.mydomain.com\\nDifferent systems give different error messages, but they all seem to point to the fact that I don\\'t have a formally signed cert on these systems. Not surprising (I know, that\\'s how it works), but I don\\'t want to go through the effort of installing (&amp; maintaining) certs for a handful of internal systems. So I tried to get around this by disabling ssl verification in my YAML file:\\n    - type: http\\n      name: http_xxx\\n      enabled: true\\n      schedule: \\'@every 5s\\'\\n      urls: [\"https://xxx.mydomain.com\"]\\n      ipv4: true\\n      ipv6: false\\n      mode: any\\n      timeout: 16s\\n      ssl: \\n        ssl.enabled: true\\n        ssl.verification_mode: none\\n\\nI\\'ve tried multiple variations of SSL options but none of them seem to work. I still get the connection error messages in Heartbeat and those systems continually show as \"down.\"\\nI\\'m hoping someone here can show me what I am doing wrong, or a workaround to enable Heartbeat to connect to these systems.\\nThanks in advance.'},\n",
       " {'text_field': \"Apologies for the delay here. The issue here is that you should use\\nssl:\\n  verification_mode: none\\n\\nI think the confusion comes from the fact that you can skip the object nesting and also write:\\nssl.verification_mode:none\\n\\nIn your config snippet you have ssl repeated in the nested object.\\nLet us know if there's anything else we can do to help.\"},\n",
       " {'text_field': 'Hello there\\nI have apache logs in which are documents that I want to remove completely.\\nWhole documents not fields so I do not want to use remove_field in mutate.\\nI have tried drop:\\nfilter {\\n      if \"field_name\" == \"exact_value\" {\\n        drop {}\\n      }\\n      } \\n\\nso it looked like this in final:\\nfilter {\\n      if \"url.original\" == \"/im/nagios.plc\" {\\n        drop {}\\n      }\\n      } \\n\\nBut it will not work, the documents are still there.\\nI have also tried drop { } with space inside.\\nI have tried [url.original] and [url.original] ==&gt;\\nLogs of logstash will not show any problems.\\nI prefer not to create tags and then delete the documents with tags but if there will be no other way I will do it\\nThank you for help!'},\n",
       " {'text_field': 'I found the answer. The if statement was good, but I fixed my format of apache logs.'},\n",
       " {'text_field': \"Hello, all!\\nI have an index with web transaction logs. It consists of requests and responses.\\nAn index looks like this:\\n{ request_id: '1234', message: 'request', path: '/fare' }\\n{ request_id: '1234', message: 'response', status: 202 }\\n{ request_id: '5789', message: 'request', path: '/orders' }\\n{ request_id: '5789', message: 'response', status: 202 }\\n{ request_id: '4512', message: 'request', path: '/orders' }\\n{ request_id: '4512', message: 'response', status: 500 }\\n\\nI need to count the number of failed (status != 202) '/orders' requests. But I can't figure out how to write a query.\\nI grouped requests-responses by request_id but then couldn't find out how to filter  aggregation results by request's path and response's status. Also, I tried to find something in Pipeline Aggregation. There is a Bucket Selector Aggregation in docs, but the problem is that it works only with numeric metrics. So, I'm stuck(\\nIs it possible to achieve my goal at all? If so, can you give me a direction, please?\\nOr maybe it would be better to reconsider index design?\\nThanks in advance!\"},\n",
       " {'text_field': 'Sorry, yeah, I should have noticed the requests and responses are in distinct docs.\\n(The detail and clarity - \"verbosity\" - is always much appreciated.)\\nYes, it would be a very good idea to merge these into a single document. The scenario you\\'re trying to accomplish is resounding testimony to the benefit. Elasticsearch\\'s join-like capabilities are limited to nested documents and parent/child; with this data set, just merging the response into the request document is sufficient.\\nIf you stand up a Kibana instance, you can load sample data. One of the sample data sets is Web Logs, which you\\'ll find is supported by a single index, in which the documents contain all the request info plus the response.'},\n",
       " {'text_field': 'Hi,\\nI am working with Logstash and Elasticsearch but I have a problem. In my case I want Elasticsearch to be updated with the new data added into a SQL Database, all done via Logstash.\\nI set the configuration file as below:\\ninput {\\n    jdbc {\\n        jdbc_connection_string =&gt; \"JDBC-Connection-String\"\\n        jdbc_driver_class =&gt; \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\\n        jdbc_user =&gt; \"JDBC-Connection-User\"\\n        jdbc_driver_library =&gt; \"JDBC-Driver-Path\"\\n\\n        statement =&gt; \"SELECT MyCol1 MyCol2 FROM MyTable\"\\n        use_column_value =&gt; true\\n        tracking_column =&gt; \"MyCol1\"\\n        tracking_column_type =&gt; \"numeric\"\\n        clean_run =&gt; true\\n        schedule =&gt; \"*/1 * * * *\"\\n    }\\n}\\n\\noutput {\\n    elasticsearch {\\n    hosts =&gt; \"http://localhost:9200\"\\n    index =&gt; \"MyIndex\"\\n    document_id =&gt; \"%{MyCol1}\"\\n}\\n    stdout { }\\n}\\n\\nAs you can see, I run a SELECT into all the DB once a minute, than I add new lines found in the DB into Elasticsearch.\\nIt works fine, but my problem is that every time logstash executes the query, it scans all the rows into the DB (I know that it is set to do that, but I haven\\'t found another way) and then it adds in Elasticsearch the ones that are not there yet, obviously causing bad performances.\\nI am struggling to find a way that Logstash doesn\\'t scans all the DB every time but simply adds the new rows into ElasticSearch.\\nP.S. MyCol1 is the Primary Key of the SQL table.'},\n",
       " {'text_field': 'If you have either a timestamp or a sequence that allows you to add a WHERE clause that identifies new entries then you can persist state about which rows have been read. But if not, you have to read the whole table every time.'},\n",
       " {'text_field': 'Hi,\\nWe run a backup of our data every 3 hours. We have recently started getting the below message because of which the backup fails sometimes. When we retry sometimes it succeeds, sometimes it fails with the same error.\\n\\\\reason\\\\\\\\:\\\\\\\\[1363871--elasticsearch--2019-06-24--14-20-00--full:snapshot_1571664901787/S2KnlgBKQWWmkOOO5ZB0nw] failed to create snapshot\\\\\\\\,\\\\\\\\caused_by\\\\\\\\:\\\\\\\\type\\\\\\\\:\\\\\\\\i_o_exception\\\\\\\\,\\\\\\\\reason\\\\\\\\:\\\\\\\\Unable to upload object [indices/G2g4EI-WS4SGcQuaEMVwCA/meta-S2KnlgBKQWWmkOOO5ZB0nw.dat] using a single upload\\\\\\\\,\\\\\\\\caused_by\\\\\\\\:\\\\\\\\type\\\\\\\\:\\\\\\\\sdk_client_exception\\\\\\\\,\\\\\\\\reason\\\\\\\\:\\\\\\\\sdk_client_exception: Unable to execute HTTP request: Read timed out\\\\\\\\,\\\\\\\\caused_by\\\\\\\\:\\\\\\\\type\\\\\\\\:\\\\\\\\i_o_exception\\\\\\\\,\\\\\\\\reason\\\\\\\\:\\\\\\\\Read timed out\\\\\\\\,\\\\\\\\status\\\\\\\\:500[\\\\18485.63 MB\\\\, \\\\15.29 pct\\\\, \\\\/usr/share/elasticsearch/jdk/bin/java -Xms16g -Xmx16g -XX:+U\\\\, \\\\8.90 MB\\\\, \\\\0.01 pct\\\\'},\n",
       " {'text_field': 'Hi @akshaymaniyar\\nThis is a known issue with S3 snapshots that we recently fixed in https://github.com/elastic/elasticsearch/pull/46589. Upcoming ES v7.5.0 should fix your issue.'},\n",
       " {'text_field': 'Fala pessoal, blz? Então.. estou com dificuldade em configurar um cluster de 3 nós, com o modo security ativo na versão 7.4.0 segue a configuração realizada no cluster:\\nelk_master:\\n\\ncluster.name: homolog\\nnode.name: elk_master\\nnode.master: true\\nnode.data: false\\nnode.ingest: false\\nnode.ml: false\\n\\n\\ndiscovery.seed_hosts: [\"10.200.19.121\", \"10.200.19.122\", \"10.200.19.123\"]\\ncluster.initial_master_nodes: [\"10.200.19.121\"]\\ncluster.remote.connect: false\\n\\n\\nxpack.security.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.keystore.path: /etc/elasticsearch/certs/elk_master.p12\\nxpack.security.transport.ssl.truststore.path: /etc/elasticsearch/certs/elk_master.p12\\nxpack.security.http.ssl.enabled: true\\nxpack.security.http.ssl.keystore.path: /etc/elasticsearch/certs/elk_master.p12\\nxpack.security.http.ssl.truststore.path: /etc/elasticsearch/certs/elk_master.p12\\n\\nelk_node1:\\n\\ncluster.name: homolog\\nnode.name: elk_node1\\nnode.data: true\\nnode.master: false\\nnode.ingest: false\\nnode.ml: false\\n\\n\\ndiscovery.seed_hosts: [\"10.200.19.121\", \"10.200.19.122\", \"10.200.19.123\"]\\ncluster.initial_master_nodes: [\"10.200.19.121\"]\\ncluster.remote.connect: false\\n\\n\\nxpack.security.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.keystore.path: /etc/elasticsearch/certs/elk_node1.p12\\nxpack.security.transport.ssl.truststore.path: /etc/elasticsearch/certs/elk_node1.p12\\nxpack.security.http.ssl.enabled: true\\nxpack.security.http.ssl.keystore.path: /etc/elasticsearch/certs/elk_node1.p12\\nxpack.security.http.ssl.truststore.path: /etc/elasticsearch/certs/elk_node1.p12\\n\\nelk_node2:\\n\\ncluster.name: homolog\\nnode.name: elk_node2\\nnode.data: true\\nnode.master: false\\nnode.ingest: false\\nnode.ml: false\\n\\n\\ndiscovery.seed_hosts: [\"10.200.19.121\", \"10.200.19.122\", \"10.200.19.123\"]\\ncluster.initial_master_nodes: [\"10.200.19.121\"]\\ncluster.remote.connect: false\\n\\n\\nxpack.security.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.keystore.path: /etc/elasticsearch/certs/elk_node2.p12\\nxpack.security.transport.ssl.truststore.path: /etc/elasticsearch/certs/elk_node2.p12\\nxpack.security.http.ssl.enabled: true\\nxpack.security.http.ssl.keystore.path: /etc/elasticsearch/certs/elk_node2.p12\\nxpack.security.http.ssl.truststore.path: /etc/elasticsearch/certs/elk_node2.p12\\n\\nSegui os passos conforme documentação porém ao iniciar o serviço do elasticsearch recebo as seguintes informações nos logs do elk_master:\\norg.elasticsearch.xpack.monitoring.exporter.ExportException: failed to flush export bulks\\n        at org.elasticsearch.xpack.monitoring.exporter.ExportBulk$Compound.lambda$doFlush$0(ExportBulk.java:109) [x-pack-monitoring-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.xpack.monitoring.exporter.local.LocalBulk.lambda$doFlush$1(LocalBulk.java:112) [x-pack-monitoring-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.ActionListener$1.onFailure(ActionListener.java:70) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.support.TransportAction$1.onFailure(TransportAction.java:79) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.support.ContextPreservingActionListener.onFailure(ContextPreservingActionListener.java:50) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:227) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:93) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:153) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:86) [x-pack-security-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$4(SecurityActionFilter.java:172) [x-pack-security-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$15(AuthorizationService.java:342) [x-pack-security-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:99) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:225) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:93) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:85) [elasticsearch-7.4.0.jar:7.4.0]\\n        at java.util.ArrayList.forEach(ArrayList.java:1507) [?:?]\\n        at org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:85) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:114) [elasticsearch-7.4.0.jar:7.4.0]\\n\\nOBS.:  Também não consigo gerar senha no modo interativo \\n\\nexibe a seguinte mensagem:\\nUnexpected response code [500] from calling GET https://10.200.19.123:9200/_security/_authenticate?pretty\\nIt doesn\\'t look like the X-Pack security feature is enabled on this Elasticsearch node.\\nPlease check if you have enabled X-Pack security in your elasticsearch.yml configuration file.\\n\\nERROR: X-Pack Security is disabled by configuration.\\n\\nalguém já passou por isso? poderiam ajudar ?'},\n",
       " {'text_field': 'Para ficar registrado! Após várias análises foi identificado que o problema era de configuração. Para solucionar, apaguei as configurações do cluster com o comando:\\nrm -rf /var/lib/elasticsearch/nodes/\\napós reiniciar os serviços, o cluster funcionou corretamente!\\nVale ressaltar que não foi necessário a inclusão dessas propriedades:\\n\\nnode.data: false\\nnode.ingest: false\\nnode.ml: false\\n\\nDeixei a configuração default e funcionou corretamente após os ajustes mencionados.'},\n",
       " {'text_field': 'Hello!\\nI am trying to copy the day and hour from the timestamp field and add each to their own fields.\\nHere is my code:\\nif [source] =~ \"tableBASEtest\" {\\n        mutate{ add_field =&gt; {\\n            \"[@metadata][timestamp]\" =&gt; \"%{SMF30DTE} %{SMF30TME}\"\\n        }}\\n        date{ match =&gt; [\\n            \"[@metadata][timestamp]\", \"yyyy-MM-dd HH:mm:ss:SS\"\\n        ]}\\t\\n\\t\\tmutate {\\n\\t\\t\\tadd_field =&gt; {\"hour\" =&gt; \"%{+HH}\"}\\n\\t\\t\\tadd_field =&gt; {\"day\" =&gt; \"%{+EEE}\"}\\n\\t\\t}\\n        }\\n\\nFor some reason, when this runs, no new fields are created. And sometimes kibana defaults to an old index from a previous configuration (no clue as to what would cause that).\\nAlso,\\nI\\'ve tried this as well:\\nif [source] =~ \"tableBASEtest\" {\\n        mutate{ add_field =&gt; {\\n            \"[@metadata][timestamp]\" =&gt; \"%{SMF30DTE} %{SMF30TME}\"\\n        }}\\n        date{ match =&gt; [\\n            \"[@metadata][timestamp]\", \"yyyy-MM-dd HH:mm:ss:SS\"\\n        ]}\\t\\n\\t\\truby{\\n\\t\\t\\tcode =&gt; \"event.set(\\'[day_of_week]\\',event.get(\\'@timestamp\\').time.strftime \\'%a\\')\"\\n\\t\\t}\\n\\t}\\n\\nThis just errors out with:\\n[2019-10-21T18:58:28,368][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SyntaxError) (ruby filter code):2: syntax error, unexpected tSTRING_BEG\\n event.set(\\'day_of_week\\',event.get(\\'@timestamp\\').time.strftime \\'%a\\')\\n\\nAny help would be awesome.'},\n",
       " {'text_field': 'ruby { code =&gt; \"event.set(\\'[day_of_week]\\',event.get(\\'@timestamp\\').time.strftime(\\'%a\\'))\" }\\n\\ngets me\\n\"day_of_week\" =&gt; \"Mon\"\\n\\nWhat synxtax error do you get?\\nYou can format posts using markdown. In particular code and syntax highlighting. The &lt;/&gt; button in the toolbar above the edit panel will indent your code by four spaces for you.'},\n",
       " {'text_field': 'I\\'m looking for a way to move a 2.3 GB index from Server A to Server B, that is completely lives on a different domain/host.\\nI tried logstash code, but it fails at some point and keeps pushing the data infinetly!\\ninput {\\n  elasticsearch {\\n        hosts =&gt; \"https://elasticServer-hosts\"\\n        index =&gt; \"index-1\"\\n        query =&gt; \\'{ \"query\": { \"match_all\": {} }}\\'\\n        size =&gt; 1\\n        scroll =&gt; \"5m\"\\n      }\\n}\\nfilter {\\n}\\noutput {\\nelasticsearch {\\n    hosts =&gt; [\"http://elastic-server2:9200\"]\\n    index =&gt; \"index-1\"\\n  }\\n    }\\n'},\n",
       " {'text_field': 'Have you tried the other way, of taking the snapshot of that 2.3gb index and restore that on the new server B ?'},\n",
       " {'text_field': 'On our site, users can perform searches. We log the start time and end time of each search, and are now curious to find all the searches that took longer than X time.\\nI have a Metadata object containing a FromDateTime and a ToDateTime. Both are formatted as DateTime objects.\\nI was looking for a way to do something like this:\\n(metadata.searchTime.fromDateTime - metadata.searchTime.toDateTime) &gt; 1000ms\\nObviously over simplified, but i hope it gets the point across better.'},\n",
       " {'text_field': 'Hi @AlanMark, there are no \"ad hoc\" scripted fields like this, they have to be part of the index pattern.\\nWhether to do calculations like this while querying or while ingesting is mostly a trade-off between flexibility and performance. It depends on your use case which one is more important. If your data set is small and doing the calculations on query time is still fast enough for your use case, it is probably the right choice.\\nIn some cases it can get problematic, e.g. if you have peta-bytes of data and want to filter out a few documents based on a scripted field - then the scripted field would have to be calculated for each document in your index.'},\n",
       " {'text_field': 'just the pipeline starts up,  sql_last_value  will be  reading from the file(last_run_metadata_path).\\nso logstash is running, cant get change value from the file. ( There is a shell timing script that initializes values every day. I want logstash to read. So I have to use the logstash restart every time to read the new value.  )   help'},\n",
       " {'text_field': 'You can enable --config.reload.automatic so that logstash will restart a pipeline every time its configuration changes. Set path.config to (for example) /home/user/conf/foo*.conf and put your actual configuration into /home/user/conf/foo.conf, then have your shell script append a blank line to /home/user/fooBlank.conf every time it runs. This will cause logstash to restart the pipeline and re-read last_run_metadata_path.'},\n",
       " {'text_field': 'Hello everyone,\\nI have a problem once i have configured SIEM and auditbeat on the \"client\" machine.\\nI configured one machine only with auditbeat and the coniguration standars. I will post below the configuration.\\nThe problem is that when I enter Kibana SIEM and fix the fielddata (is there any way to make it more comfortable?), it appears as if I had 4 hosts configured. Attached image.\\n\\nhosts.PNG839×727 37.5 KB\\n\\nHere i post the configuration of auditbeat:\\n###################### Auditbeat Configuration Example #######################\\nauditbeat.modules:\\n\\n- module: auditd\\n  # Load audit rules from separate files. Same format as audit.rules(7).\\n  audit_rule_files: [ \\'${path.config}/audit.rules.d/*.conf\\' ]\\n  #audit_rules: |\\n    ## Define audit rules here.\\n    ## Create file watches (-w) or syscall audits (-a or -A). Uncomment these\\n    ## examples or add your own rules.\\n\\n    ## If you are on a 64 bit platform, everything should be running\\n    ## in 64 bit mode. This rule will detect any use of the 32 bit syscalls\\n    ## because this might be a sign of someone exploiting a hole in the 32\\n    ## bit API.\\n    #-a always,exit -F arch=b32 -S all -F key=32bit-abi\\n\\n    ## Executions.\\n    #-a always,exit -F arch=b64 -S execve,execveat -k exec\\n\\n    ## External access (warning: these can be expensive to audit).\\n    #-a always,exit -F arch=b64 -S accept,bind,connect -F key=external-access\\n\\n    ## Identity changes.\\n    #-w /etc/group -p wa -k identity\\n    #-w /etc/passwd -p wa -k identity\\n    #-w /etc/gshadow -p wa -k identity\\n\\n    ## Unauthorized access attempts.\\n    #-a always,exit -F arch=b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EACCES -k access\\n    #-a always,exit -F arch=b64 -S open,creat,truncate,ftruncate,openat,open_by_handle_at -F exit=-EPERM -k access\\n\\n- module: file_integrity\\n  paths:\\n  - /bin\\n  - /usr/bin\\n  - /sbin\\n  - /usr/sbin\\n  - /etc\\n\\n- module: system\\n  datasets:\\n    - host    # General host information, e.g. uptime, IPs\\n    - login   # User logins, logouts, and system boots.\\n    - package # Installed, updated, and removed packages\\n    - user    # User information\\n  period: 1m\\n\\n- module: system\\n  datasets:\\n          - process #Started and stopped processes\\n          - socket #Opened and closed sockets\\n  period: 1s\\n  socket.enable_ipv6: false\\n\\n  user.detect_password_changes: true\\n\\n  login.wtmp_file_pattern: /var/log/wtmp*\\n  login.btmp_file_pattern: /var/log/btmp*\\n\\n#================================ General =====================================\\n\\n# The name of the shipper that publishes the network data. It can be used to group\\n# all the transactions sent by a single shipper in the web interface.\\n#name:\\n\\n# The tags of the shipper are included in their own field with each\\n# transaction published.\\n#tags: [\"service-X\", \"web-tier\"]\\n\\n# Optional fields that you can specify to add additional information to the\\n# output.\\n#fields:\\n#  env: staging\\n\\n#================================ Outputs =====================================\\n\\n#----------------------------- Logstash output --------------------------------\\noutput.logstash:\\n  # The Logstash hosts\\n  hosts: [\"10.10.12.114:5044\"]\\n\\n  # Optional SSL. By default is off.\\n  # List of root certificates for HTTPS server verifications\\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\\n\\n  # Certificate for SSL client authentication\\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\\n\\n  # Client Certificate Key\\n  #ssl.key: \"/etc/pki/client/cert.key\"\\n\\n#================================ Processors =====================================\\n\\n# Configure processors to enhance or manipulate events generated by the beat.\\n\\n  #processors:\\n  #- add_host_metadata: ~\\n  #- add_cloud_metadata: ~\\n\\n#================================ Logging =====================================\\n\\n# Sets log level. The default log level is info.\\n# Available log levels are: error, warning, info, debug\\n#logging.level: debug\\n\\n# At debug level, you can selectively enable logging only for some components.\\n# To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\",\\n# \"publish\", \"service\".\\n#logging.selectors: [\"*\"]\\n\\n#============================== X-Pack Monitoring ===============================\\n# auditbeat can export internal metrics to a central Elasticsearch monitoring\\n# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The\\n# reporting is disabled by default.\\n\\n# Set to true to enable the monitoring reporter.\\n#monitoring.enabled: false\\n\\n# Sets the UUID of the Elasticsearch cluster under which monitoring data for this\\n# Auditbeat instance will appear in the Stack Monitoring UI. If output.elasticsearch\\n# is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch.\\n#monitoring.cluster_uuid:\\n\\n# Uncomment to send the metrics to Elasticsearch. Most settings from the\\n# Elasticsearch output are accepted here as well.\\n# Note that the settings should point to your Elasticsearch *monitoring* cluster.\\n# Any setting that is not set is automatically inherited from the Elasticsearch\\n# output configuration, so if you have the Elasticsearch output configured such\\n# that it is pointing to your Elasticsearch monitoring cluster, you can simply\\n# uncomment the following line.\\n#monitoring.elasticsearch:\\n\\n#================================= Migration ==================================\\n\\n# This allows to enable 6.7 migration aliases\\n#migration.6_to_7.enabled: false\\n\\nAnyone can help me?\\nThanks!'},\n",
       " {'text_field': 'Hi @Aleix_Abrie_Prat - if you had to fix the fielddata you probably did not run auditbeat setup before running auditbeat? Can you try that and see if the problem persists?'},\n",
       " {'text_field': 'I went through my rollup job JSON file and I think current_position in below config is the way how a rollup job memorizes the last document it processed.\\n\"status\": {\\n    \"job_state\": \"started\",\\n    \"current_position\": {\\n      \"@timestamp.date_histogram\": 1571728849000,\\n      \"status.terms\": \"complete\"\\n    }\\n\\nMy question is what if I insert data which have @timestamp value less than @timestamp.date_histogram  (last processed timestamp value)\\nWill they also be processed?\\nOr  @timestamp values which are greater than last processed timestamp value will only be processed?'},\n",
       " {'text_field': \"Hi @isharamadhawa! The timestamp is important. It's used as the after_key. If you add, change or delete data before that timestamp it won't be rolled up again.\\nTo compensate for ingest delay, you can use the delay parameter. See the Create rollup job example and the Rollup job configuration documentation for more details.\"},\n",
       " {'text_field': \"We have some data which displays a low hourly average when viewed across say 30 days but when you zoom in to a few days the average jumps up which is wrong: I can't show a 20s average and then, when I zoom in show a 75s average...\\nThe data points exist for every 5min.\\nZoomed Out (Oct 7-22) - timestamp buckets are per hour:\\n\\nimage.png1518×832 25.7 KB\\n\\n\\nZoomed In (Oct 13-22) - timestamp buckets are per 3 hours:\\n\\nimage.png1479×818 28.8 KB\\n\\n\\nAm I just bad at stats or is this a bug?\\nAlso, I've set it to show hourly but it wants to show 3h - how can I change this? The UI suggests we can have custom values:\\n\\nSelect an option or create a custom value. Examples: 30s, 20m, 24h, 2d, 1w, 1M\\n\\nimage.png1282×600 35.2 KB\\n\\n\"},\n",
       " {'text_field': 'You are running into this issue: https://github.com/elastic/kibana/issues/42611\\nAs linked in that issue, there is a behavior change that you will see in the future 7.5 release: https://github.com/elastic/kibana/pull/47599'},\n",
       " {'text_field': 'Hi there,\\nI\\'ve an array, where elements look like this:\\n\\n\\n t    _id  \\t          htqe7m0BSOdvtuXlL_IK\\n\\n\\n\\n  t   _index\\t      currency_data\\n\\n\\n\\n  #   _score\\t       - \\n\\n\\n\\n  t   _type\\t          data\\n\\n\\n\\n  #   player_id\\t       457\\n\\n\\n\\n  #   soft_currency_amount\\t    95,792\\n\\n\\n\\n  soft_currency_last_update \\tOct 21, 2019 @ 16:57:01.000\\n\\n\\n\\nThis is information about a game player, his currency balance and the date when his balance was updated. There can be several recods for the same player.\\nI need to visualize (firstly, as a Data Table) the balance of each player at the moment of the LAST update\\n{\\n  \"aggs\": {\\n    \"4\": {\\n      \"terms\": {\\n        \"field\": \"player_id\",\\n        \"order\": {\\n          \"_key\": \"asc\"\\n        },\\n        \"size\": 400\\n      },\\n      \"aggs\": {\\n        \"2\": {\\n          \"terms\": {\\n            \"field\": \"soft_currency_amount\",\\n            \"order\": {\\n              \"2-orderAgg\": \"desc\"\\n            },\\n            \"size\": 1\\n          },\\n          \"aggs\": {\\n            \"3\": {\\n              \"terms\": {\\n                \"field\": \"soft_currency_last_update\",\\n                \"order\": {\\n                  \"1\": \"desc\"\\n                },\\n                \"size\": 1\\n              },\\n              \"aggs\": {\\n                \"1\": {\\n                  \"cardinality\": {\\n                    \"field\": \"player_id\"\\n                  }\\n                }\\n              }\\n            },\\n            \"2-orderAgg\": {\\n              \"max\": {\\n                \"field\": \"soft_currency_last_update\"\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"size\": 0,\\n  \"_source\": {\\n    \"excludes\": []\\n  },\\n  \"stored_fields\": [\\n    \"*\"\\n  ],\\n  \"script_fields\": {},\\n  \"docvalue_fields\": [\\n    {\\n      \"field\": \"soft_currency_last_update\",\\n      \"format\": \"date_time\"\\n    }\\n  ],\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"range\": {\\n            \"soft_currency_last_update\": {\\n              \"format\": \"strict_date_optional_time\",\\n              \"gte\": \"2019-10-11T21:00:00.000Z\",\\n              \"lte\": \"2019-10-12T20:30:00.000Z\"\\n            }\\n          }\\n        }\\n      ],\\n      \"filter\": [\\n        {\\n          \"match_all\": {}\\n        }\\n      ],\\n      \"should\": [],\\n      \"must_not\": []\\n    }\\n  }\\n\\nAs a result, I\\'ve a table with following columns: player_id, soft_currency_amount, \\tsoft_currency_last_update and Unique count of player_id. The last metric is used just for fit.\\nAll data are correct, but the \"soft_currency_last_update\" column doesnt contain the real last update. It shows the date when the appropriate balance amount appeared for the first time.\\nFor example, a player has 1000 coins on the 21 Oct at 2p.m., then his balance counted 1300 coins at 5p.m.  The next (and the last) balance update was on the 22 Oct at 8a.m. when the balance remained the same (1300 coins).\\nIn this situation a table record will contain \\tsoft_currency_amount - 1300, \\tsoft_currency_last_update - 21 Oct 5p.m (instead of 22 Oct at 8a.m.)\\nWhat\\'s wrong with it?'},\n",
       " {'text_field': 'You should be using the Top Hits aggregation, which can return a single record sorted by something like timestamp. This is possible using Visualize'},\n",
       " {'text_field': 'Good morning,\\nI\\'m using Logstash with elapsed plugin to calculate the time between two timestamps. It works perfectly and the field is a Number in seconds.\\nIs there a way to create a new field with that value in a human readable way? I need the seconds field to make some statistics (number of elapsed seconds in a month, for example), but I need another field to read the value (74685 seconds to something like \"20 hours, 44 minutes and 45 seconds\".\\nI tried to convert the field to date and to use date filter, but it doesn\\'t work.\\nI think that the biggest elapsed time will have months.\\nThank you so much,'},\n",
       " {'text_field': 'You can do it in a ruby filter. Take a look at this.'},\n",
       " {'text_field': 'Hi  - Im trying to define a TimeLion visualisation usint the following expression\\n.es(corr,timefield=rt,split=\"DeviceVendor.keyword:10\")\\nThis works but the labels show ion the chart are as follows\\nq:corr &gt; DeviceVendor.keyword:F-Secure &gt; count\\nq:corr &gt; DeviceVendor.keyword:Microsoft &gt; count\\nq:corr &gt; DeviceVendor.keyword:ArcSight &gt; count\\nq:corr &gt; DeviceVendor.keyword:Check Point &gt; count\\nI want to make these look a little tidier so tried to specify my own label by changing the expression to\\n.es(corr,timefield=rt,split=\"DeviceVendor.keyword:10\").label(\"[$1]\", \"^.* &gt; DeviceVendor.keyword:(\\\\S+) &gt; .*\")\\nThe labels now show\\n[F-Secure]\\n[Microsoft]\\n[ArcSight]\\nq:corr &gt; DeviceVendor.keyword:Check Point &gt; count\\nNote the issue with the last label - is this caused by this vendor string having a space in it? if so how can I change it so that the space is ignored and I get\\n[Check Point] as the last label\\nThanks for your help'},\n",
       " {'text_field': '\\n\\n\\n j_a_griffiths:\\n\\n.label(\"[$1]\", \"^.* &gt; DeviceVendor.keyword:(\\\\S+) &gt; .*\")\\n\\n\\nYes, that\\'s the reason. If I\\'m not wrong you can fix the regex as the following:\\n.label(\"[$1]\", \"^.* &gt; DeviceVendor.keyword(.*?) &gt; .*\")\\n'},\n",
       " {'text_field': 'Есть массив, каждый элемент которого выглядит следующим образом:\\n\\n\\n t    _id  \\t                  htqe7m0BSOdvtuXlL_IK\\n\\n\\n\\n  t   _index\\t              currency_data\\n\\n\\n\\n  #   _score\\t                - \\n\\n\\n\\n  t   _type\\t                   data\\n\\n\\n\\n  #   player_id\\t                457\\n\\n\\n\\n  #   soft_currency_amount\\t    95,792\\n\\n\\n\\n  soft_currency_last_update \\tOct 21, 2019 @ 16:57:01.000\\n\\n\\n\\nЭто информация об игроке, его балансе и дате последнего обновления баланса. Для отдельно взятого игрока может быть несколько таких накопившихся записей.\\nЗадача - визуализировать данные, для начала в виде таблицы, отобразив баланс игрока на момент самого последнего обновления.\\n{\\n  \"aggs\": {\\n    \"4\": {\\n      \"terms\": {\\n        \"field\": \"player_id\",\\n        \"order\": {\\n          \"_key\": \"asc\"\\n        },\\n        \"size\": 400\\n      },\\n      \"aggs\": {\\n        \"2\": {\\n          \"terms\": {\\n            \"field\": \"soft_currency_amount\",\\n            \"order\": {\\n              \"2-orderAgg\": \"desc\"\\n            },\\n            \"size\": 1\\n          },\\n          \"aggs\": {\\n            \"3\": {\\n              \"terms\": {\\n                \"field\": \"soft_currency_last_update\",\\n                \"order\": {\\n                  \"1\": \"desc\"\\n                },\\n                \"size\": 1\\n              },\\n              \"aggs\": {\\n                \"1\": {\\n                  \"cardinality\": {\\n                    \"field\": \"player_id\"\\n                  }\\n                }\\n              }\\n            },\\n            \"2-orderAgg\": {\\n              \"max\": {\\n                \"field\": \"soft_currency_last_update\"\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"size\": 0,\\n  \"_source\": {\\n    \"excludes\": []\\n  },\\n  \"stored_fields\": [\\n    \"*\"\\n  ],\\n  \"script_fields\": {},\\n  \"docvalue_fields\": [\\n    {\\n      \"field\": \"soft_currency_last_update\",\\n      \"format\": \"date_time\"\\n    }\\n  ],\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"range\": {\\n            \"soft_currency_last_update\": {\\n              \"format\": \"strict_date_optional_time\",\\n              \"gte\": \"2019-10-11T21:00:00.000Z\",\\n              \"lte\": \"2019-10-12T20:30:00.000Z\"\\n            }\\n          }\\n        }\\n      ],\\n      \"filter\": [\\n        {\\n          \"match_all\": {}\\n        }\\n      ],\\n      \"should\": [],\\n      \"must_not\": []\\n    }\\n  }\\n\\nВ результате получается таблица со следующими колонками: player_id, soft_currency_amount, \\tsoft_currency_last_update и Unique count of player_id.  Последняя метрика по сути бесполезна, используется для того, чтобы посчитать \"хоть что-нибудь\" для отображения остальных данных.\\nИнформация в итоговой таблице получается верной за одним исключением: колонка \"soft_currency_last_update\" показывает не фактическую дату последнего обновления, а дату, когда появился соответствующий баланс.\\nНапример, у игрока было 1200 монет на момент времени 21 октября в 13:00. Затем, в 17:00 баланс изменился, насчитывая 1300 монет. Следующее (и последнее) обновление баланса произошло на другой день, 22 октября в 8:00. На этот момент баланс остался неизменным, 1300 монет.\\nВ данной ситуации строка таблицы будет содержать корректный  id игрока, последнее значение его баланса (1300 монет), но графа даты будет указывать на 21 октября 17:00, а не на 22 октября 8:00.\\nЧто не так?\\nИ можно ли это всё визуализировать не в виде таблицы, а с виде столбчатой диаграммы (по y - баланс игрока, по x - id игрока)'},\n",
       " {'text_field': '\\n\\n\\n Anne_Kim:\\n\\nЕсть массив, каждый элемент которого выглядит следующим образом:\\n\\n\\nТо есть каждый элемент массива - это запись? Если да - то вам под агрегаций по игрокам, надо выбрать поля из последней записи. Это делается с помощью агрегации top_hits отсортированной по @timestamp с размером в 1. То есть дата последнего обновления будет выглядеть как-то так:\\n\\nScreenshot from 2019-10-22 11-45-04.png314×709 20.4 KB\\n\\nВ кибане можно добавить несколько таких агрегаций по всем интересующим вас полям. Если вы работаете на прямую с elasticsearch, то все top_hits агрегации можно объединить в одну.'},\n",
       " {'text_field': 'I\\'m trying to parse a log date with an Elasticsearch ingest pipeline, using date processor, but the parsed date is not correct.\\nAll the other pipelines that I\\'ve created work correctly, except this one.\\nHere is what I\\'m using:\\nRaw log date: Tue Oct 22 10:58:00.645020 2019\\nProcessor:\\n\"date\": { \"field\": \"log.time\", \"target_field\": \"@timestamp\", \"formats\": [\"EEE MMM dd H:m:s.SSSSSS yyyy\"] }\\nResult: 2019-10-22T00:00:00.000Z\\nThe date shown in the log.time field is correct (Tue Oct 22 10:58:00.645020 2019)\\nI\\'ve also tried the following format, without success: EEE MMM dd HH:mm:ss.SSSSSS yyyy'},\n",
       " {'text_field': 'Apparently, I was looking at the wrong timestamp field (ingest one)...\\nUsing EEE MMM dd H:m:s.SSSSSS yyyy works.'},\n",
       " {'text_field': \"I would like to perform actions per each document. For example, for each sudo command that is being executed, send a Slack notification.\\nI can't find any documentation for doing such watch.\"},\n",
       " {'text_field': 'See https://www.elastic.co/guide/en/elasticsearch/reference/7.4/action-foreach.html'},\n",
       " {'text_field': \"Hi,\\nI am struggling to a seeming humble issue. I am not able to get the sorting by fields work on the 'Discover'.\\nSo, in Discover, I 'added' a few fields to view and tried clicking on sort up/down arrows. The sorting is not working for any field except for the 'time' field. I tried this on String and also numeric fields. I also noticed, that for some fields the sort 'up/down' arrows do not show up.\\nI am wondering if enabling sorting and getting it to work expectedly needs some specific changes somewhere - may be mappings file or elsewhere. I am not sure.\\nI read the docs and did not come across any particulars for any additional configurations. Seems like it should work easily and by default.\\nI am using 7.4.0.\\nAny help will be highly appreciated.\"},\n",
       " {'text_field': \"Yeah, handling of multi fields is still not ideal by a long shot. There is a ticket in github for this issue https://github.com/elastic/kibana/issues/7419\\nIt's possible this issue will get some traction soon, because we're now storing the field information we need in Kibana to understand the relationship between fields and their multi-fields, and the discover doc table will be getting some love soon.\"},\n",
       " {'text_field': 'Hello!\\nI would like to know if I can delete the logs below as I am trying to optimize disk space.\\nfiles location: cd /var/log/elasricsearch\\nelastic_audit.json\\nelastic-deprecation-1.json\\nelastic-deprecation.json\\nelastic_deprecation.log\\nelastic-index_indexing_slowlog.json\\nelastic-index_indexing_slowlog.log\\nelastic-index_search_slowlog.json\\nelastic-index_search_slowlog.log\\nelastic-prod.log\\nelastic-prod_server.json\\nIf I delete, will I have any side effects?\\nOr can I erase without problems?'},\n",
       " {'text_field': 'You can look at archiving (zipping) logs older than n days and then you can eventually delete those.\\nE.g. you can zip logs older than 7 days and delete the files older than 14 days.'},\n",
       " {'text_field': 'I am new to the whole Elasticsearch Topic and having some starter problems. I tried to search and find a solution on this topic in the Forum but couldnt find anything that could help me with it. I hope somebody can help me with this. Its seams to be a timeout problem with logstash but eather i am to stupid or inexperienced to implement the solution properly.\\nFilebeat 7.1\\nlogstash 7.1\\nOct 22 13:09:42 hdpkeyclo-prod1 filebeat[8963]: 2019-10-22T13:09:42.184Z        INFO        [monitoring]        log/log.go:144        Non-zero metrics in the last 30s        {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":440\\nOct 22 13:09:13 hdpkeyclo-prod1 filebeat[8963]: 2019-10-22T13:09:13.723Z        INFO        pipeline/output.go:105        Connection to backoff(async(tcp://hdpbss-elk:5044)) established\\nOct 22 13:09:13 hdpkeyclo-prod1 filebeat[8963]: 2019-10-22T13:09:13.721Z        INFO        pipeline/output.go:95        Connecting to backoff(async(tcp://hdpbss-elk:5044))\\nOct 22 13:09:13 hdpkeyclo-prod1 filebeat[8963]: 2019-10-22T13:09:13.721Z        ERROR        pipeline/output.go:121        Failed to publish events: write tcp 10.88.49.8:55516-&gt;10.88.49.40:5044: write: connection reset by peer\\nOct 22 13:09:12 hdpkeyclo-prod1 filebeat[8963]: 2019-10-22T13:09:12.311Z        ERROR        logstash/async.go:256        Failed to publish events caused by: write tcp 10.88.49.8:55516-&gt;10.88.49.40:5044: write: connection reset by peer\\n\\n\\n\\n#----------------------------- Logstash output --------------------------------\\noutput.logstash:\\n  # The Logstash hosts\\n  enabled: true\\n  hosts: [\"10.88.40.40:5044\"]\\n  loadbalance: true\\n  timeout: 300\\n  # Optional SSL. By default is off.\\n  # List of root certificates for HTTPS server verifications\\n  #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\\n\\n  # Certificate for SSL client authentication\\n  #ssl.certificate: \"/etc/pki/client/cert.pem\"\\n\\n  # Client Certificate Key\\n  #ssl.key: \"/etc/pki/client/cert.key\"\\n\\n\\ninput {\\n  beats {\\n    client_inactivity_timeout =&gt; 600\\n    port =&gt; 5044\\n    tcp_keep_alive =&gt; true\\n  }\\n}'},\n",
       " {'text_field': 'Closed because the Author of this Topic is an idiot.\\n\\ninput  {\\n  beats {\\n    port =&gt; 5044\\n    client_inactivity_timeout =&gt; 84600\\n  }\\n}'},\n",
       " {'text_field': 'Elasticsearch 6.5.4\\nWhile this looks a trivial task I did not find an easy way to include document position in search response\\nSample query\\n{\\n  \"query\":{\\n     \"bool\":{\\n        \"must\":[\\n           {\\n              \"term\":{\\n                 \"organizationId\":{\\n                    \"value\":21,\\n                    \"boost\":1.0\\n                 }\\n              }\\n           }\\n        ]\\n     }\\n  }\\n}\\n\\nDesired results. Note the position field:\\n    \"hits\": {\\n        \"total\": 25,\\n        \"max_score\": null,\\n        \"hits\": [\\n            {\\n                \"_index\": \"...\",\\n                \"_type\": \"...\",\\n                \"_id\": \"...\",\\n                \"position\": 0,\\n                \"_source\": {...}\\n            },\\n            {\\n                \"_index\": \"...\",\\n                \"_type\": \"...\",\\n                \"_id\": \"...\",\\n                \"position\": 1,\\n                \"_source\": {...}\\n            }\\n        ]\\n    }'},\n",
       " {'text_field': 'I don\\'t know why, but changing counter type from int to int array solved the issue:\\n\\t\"script_fields\" : {\\n    \"position\": {\\n        \"script\": {\\n            \"source\":\"params.counter[0]++\",\\n            \"lang\":\"painless\",\\n            \"params\":{\\n                \"counter\": [0]\\n            }\\n    \\t}\\n    }\\n}'},\n",
       " {'text_field': 'Hello,\\nwe are running in some issues and we dont know why.\\nKibana 7.3\\nKibana session timeout 60 minutes\\nWhen we running a dashboard from the default space and refresh this dashboard every 15 minutes, we dont run into a session timeout.\\nBut when we running a dashboard with the same user from another space and refresh this dashboard every 15 minutes, we run into a session timeout.\\nWhy is this happening ?\\nHow can we avoid a session timeout with dashboard which are not in the default space ?\\nBest regards'},\n",
       " {'text_field': 'Hey @treknado, there are currently a sub-set of XHR requests that are executed by Kibana which don\\'t extend the session timeouts because they\\'re using a \"non-standard\" approach. TSVB happens to be one of these.  https://github.com/elastic/kibana/pull/39477 will fix this issue, but the earliest this fix will be available is in 7.6.\\nIt\\'s an incredibly ugly hack, but if you add a non-TSVB Visualization to the dashboard, that should keep the dashboard from expiring your session prematurely.'},\n",
       " {'text_field': \"Hello,\\nThe following does not seem to work?\\n- module: windows\\n  metricsets: [perfmon]\\n  period: 10s\\n  perfmon.ignore_non_existent_counters: true\\n  perfmon.group_measurements_by_instance: true\\n  perfmon.counters:\\n    - instance_label: processor.name\\n      instance_name: total\\n      measurement_label: processor.time.total.pct\\n      query: '\\\\Processor Information(_Total)\\\\% Processor Time'\\n    \\n    - instance_label: logical_disk.name\\n      measurement_label: logical_disk.disk_time.pct\\n      query: '\\\\LogicalDisk(*)\\\\% Disk Time'\\n      processors:\\n      - add_fields:\\n          fields:\\n            metricset.counter: logical_disk\\n              \\n    - instance_label: logical_disk.name\\n      measurement_label: logical_disk.avg_disk_queue_length\\n      query: '\\\\LogicalDisk(*)\\\\Avg. Disk Queue Length'\\n      processors:\\n      - add_fields:\\n          fields:\\n            metricset.counter: logical_disk\\n\\nIs there a way to use the add_field processor per instance in the perfmon module? We would like to monitor some mssql performance counters and add the name of the database in a custom field. That way we can aggregate counters per host per database. If this is not possible, should I make a feature request?\\nTx!\\nGrtz\\nWillem\"},\n",
       " {'text_field': \"Ok, we found this, tested it and it works:\\n- module: windows\\n  metricsets: [perfmon]\\n  period: 10s\\n  perfmon.ignore_non_existent_counters: true\\n  perfmon.group_measurements_by_instance: true\\n  perfmon.counters:\\n    - instance_label: processor.name\\n      instance_name: total\\n      measurement_label: processor.time.total.pct\\n      query: '\\\\Processor Information(_Total)\\\\% Processor Time'\\n    \\n    - instance_label: logical_disk.name\\n      measurement_label: logical_disk.disk_time.pct\\n      query: '\\\\LogicalDisk(*)\\\\% Disk Time'\\n              \\n    - instance_label: logical_disk.name\\n      measurement_label: logical_disk.avg_disk_queue_length\\n      query: '\\\\LogicalDisk(*)\\\\Avg. Disk Queue Length'\\n  processors:\\n    - add_fields:\\n        when.has_fields:\\n          ['windows.perfmon.logical_disk.name']\\n        fields:\\n          metricset.counter: logical_disk\"},\n",
       " {'text_field': 'Hi there!\\nI\\'m having some problems developing a kibana plugin for this specific Kibana version.\\nIf I make EXACTLY the same steps which I\\'m about to describe to develop the plugin for version 7.0 or 7.1 or even 7.3, it works flawlessly. With version 7.2 I have the following error.\\nMy steps are:\\ngit clone https://github.com/elastic/kibana\\ncd kibana\\ngit checkout 7.2\\nHere I can see the .node-version is the same I am using (10.15.2). Perfect.\\nyarn kbn bootstrap\\nHere it gives me some warnings like:\\nwarning Resolution field \"core-js@2.5.3\" is incompatible with requested version \"core-js@^2.6.5\"\\nwarning Resolution field \"@types/node@10.12.27\" is incompatible with requested version \"@types/node@8.5.8\"\\nwarning Resolution field \"@types/node@10.12.27\" is incompatible with requested version \"@types/node@8.5.8\"\\nwarning Resolution field \"@types/node@10.12.27\" is incompatible with requested version \"@types/node@8.5.8\"\\nwarning Resolution field \"@types/node@10.12.27\" is incompatible with requested version \"@types/node@8.5.8\"\\n[3/5] Fetching packages...\\ninfo fsevents@1.2.7: The platform \"linux\" is incompatible with this module.\\ninfo \"fsevents@1.2.7\" is an optional dependency and failed compatibility check. Excluding it from installation.\\n[4/5] Linking dependencies...\\nwarning \" &gt; @elastic/charts@4.2.6\" has incorrect peer dependency \"@elastic/eui@10.4.1\".\\nwarning \"workspace-aggregator-bc398412-e147-4630-89fa-8201a4e98573 &gt; x-pack &gt; @mapbox/mapbox-gl-draw@1.1.1\" has inco\\nrrect peer dependency \"mapbox-gl@&gt;=0.27.0 &lt;=0.51.0\".\\nwarning \"workspace-aggregator-bc398412-e147-4630-89fa-8201a4e98573 &gt; x-pack &gt; react-shortcuts@2.0.0\" has incorrect p\\neer dependency \"react@^0.14.8 || ^15\".\\nwarning \"workspace-aggregator-bc398412-e147-4630-89fa-8201a4e98573 &gt; x-pack &gt; react-shortcuts@2.0.0\" has incorrect p\\neer dependency \"react-dom@^0.14.8 || ^15\".\\nwarning \"workspace-aggregator-bc398412-e147-4630-89fa-8201a4e98573 &gt; x-pack &gt; graphql-code-generator &gt; @graphql-modu\\nles/epoxy@0.1.9\" has unmet peer dependency \"@graphql-modules/logger@*\".\\n\\nBut it keeps bootstrapping and finally succeeds.\\nThen I go\\n`node scripts/generate_plugin.js my_sample_plugin` answering:\\n`? Provide a short description This is a sample plugin`\\n`? What Kibana version are you targeting? 7.2.2`\\n`? Should an app component be generated? Yes`\\n`? Should translation files be generated? No`\\n`? Should a hack component be generated? No`\\n`? Should a server API be generated? Yes`\\n`? Should SCSS be used? Yes`\\n\\nIt keeps loading (returning the same above-mentioned warnings) and it finally says:\\nBootstrapping completed!\\n\\nDone in 168.27s.\\nsuccess 🎉\\n\\n  Your plugin has been created in plugins/my_sample_plugin. Move into that directory to run it:\\n\\n    cd \"plugins/my_sample_plugin\"\\n    yarn start\\n\\nWhich is exactly what it says with version 7.3.\\nProblem is that now if I go to plugins/my_sample_plugin and run yarn start it immediately returns the error:\\nyarn run v1.19.1\\nwarning package.json: No license field\\n$ plugin-helpers start\\nTask \"start\" failed:\\n\\nError: spawnSync node ENOENT\\n    at Object.spawnSync (internal/child_process.js:990:20)\\n    at spawnSync (child_process.js:601:24)\\n    at execFileSync (child_process.js:629:13)\\n    at module.exports (/home/fabio/Scrivania/kibana_7.2_test_plugin/kibana/packages/kbn-plugin-helpers/tasks/start/start_action.js:43:3)\\n    at run (/home/fabio/Scrivania/kibana_7.2_test_plugin/kibana/packages/kbn-plugin-helpers/lib/run.js:30:10)\\n    at Command.&lt;anonymous&gt; (/home/fabio/Scrivania/kibana_7.2_test_plugin/kibana/packages/kbn-plugin-helpers/lib/commander_action.js:27:13)\\n    at Command.listener (/home/fabio/Scrivania/kibana_7.2_test_plugin/kibana/packages/kbn-plugin-helpers/node_modules/commander/index.js:315:8)\\n    at Command.emit (events.js:189:13)\\n    at Command.parseArgs (/home/fabio/Scrivania/kibana_7.2_test_plugin/kibana/packages/kbn-plugin-helpers/node_modules/commander/index.js:654:12)\\n    at Command.parse (/home/fabio/Scrivania/kibana_7.2_test_plugin/kibana/packages/kbn-plugin-helpers/node_modules/commander/index.js:474:21)\\nerror Command failed with exit code 1.\\ninfo Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.\\n\\nThis thing is really driving me nuts since I really need to develop the plugin for the 7.2 version and apparently that is the only version returning this error.\\nAt first I thought it was linked to the fact the from version 7.1 to 7.2 they changed the folder where the plugin is created (from kibana-extra/ sibling of kibana/ to the plugins/ which is a child of kibana/) but then I tried with version 7.3 which creates the plugin inside the plugins/ folder as well and it worked, so I really do not know what to do about it.\\nThank you for your support!'},\n",
       " {'text_field': 'Ok I guess I found the problem with the help of a colleague of mine.\\nReading the error there\\'s a call to kbn-plugin-helpers/tasks/start/start_action.js.\\nGoing back through all the linked files we noticed there\\'s a line in packages/kbn-plugin-helpers/lib/plugin_config.js which refers to the old folder structure, calling the path ../../kibana as if the plugin was placed inside the old kibana-extra folder.\\nThe error\\n`Error: The \\'KIBANA_ROOT\\' environment variable has been removed from \\'@kbn/plugin- helpers\\'. During development your plugin must be located in \\'../kibana-extra/{pluginName}\\' relative to the Kibana directory to work with this package.`\\n\\nshould have rung a bell about it.\\nThen we went with git blame packages/kbn-plugin-helpers/lib/plugin_config.js from which you can see that Tiago Costa applied a fix to that path in commit e4830625.\\nThus, running git show e4830625 | git apply from v7.2.0 branch, you can see that some files got modified\\n\\nimage.png804×338 37.5 KB\\n\\nIt\\'ll then be sufficient to add the edits (git add .) and commit ignoring the linter (git commit -m \"whatever\" --no-verify).\\nNow, running the yarn kbn bootstrap, node scripts/generate_plugin.js my_plugin and cd plugins/my_plugin/, yarn start everything works as expected.\\nIt\\'s kinda strange that such an important fix is not mentioned anywhere (apart from git history) and I really do not understand how it could work on your environment.\\nYou sure replicated my exact steps, cloning the repo, checking out to 7.2.0 and running everything else without changing anything? Since it really shouldn\\'t work because of that wrong path.\\nAnyway, hope this thread will be of help to anybody who needs to develop a plugin for Kibana 7.2.x.'},\n",
       " {'text_field': 'Hi Elasticsearch Gurus,\\nI’ve recently spun up an Elasticsearch v7.4.0 Docker container.  Within the container, I’ve been playing with the “elasticsearch-sql-cli” script and successfully submitting SQL-like queries against my ES index.  Which is pretty great.\\nHere’s a follow-up problem.  My index is really massive, with several million data records, and growing rapidly.  I need to find a way to submit an SQL-like query that basically says, “Give me X data only for the last five minutes.”  As luck would have it, my index includes a “timestamp” record:\\n{\\n  \"mapping\": {\\n    \"properties\": {\\n      \"@timestamp\": {\\n        \"type\": \"date\"\\n      },…\\n\\nHere’s a sample of what it looks like in a standard query:\\nTimestamp               | SrcIP      | DstIP      |Protocol|Port| sum\\n------------------------+------------+------------+--------+----+-------\\n2019-10-09T20:02:27.770Z|10.10.10.15 |10.10.10.16 |6       |5201|15000.0\\n2019-10-09T20:02:27.771Z|10.10.10.15 |10.10.10.16 |6       |5201|51000.0\\n2019-10-09T20:02:27.772Z|10.10.10.15 |10.10.10.16 |6       |5201|19500.0\\n2019-10-09T20:02:27.785Z|10.10.10.15 |10.10.10.16 |6       |5201|69000.0\\n2019-10-09T20:02:27.787Z|10.10.10.15 |10.10.10.16 |6       |5201|10500.0\\n2019-10-09T20:02:27.788Z|10.10.10.15 |10.10.10.16 |6       |5201|10500.0\\n2019-10-09T20:02:27.789Z|10.10.10.15 |10.10.10.16 |6       |5201|24000.0\\n2019-10-09T20:02:27.790Z|10.10.10.15 |10.10.10.16 |6       |5201|54000.0\\n2019-10-09T20:02:27.791Z|10.10.10.15 |10.10.10.16 |6       |5201|55500.0\\n\\nOkay, so how would you say “Give me records from the last five minutes?”  Standard SQL would have you do that like this.  But I’ve tried this in my elasticsearch-sql-cli script, and I’m missing something in the syntax.  Here’s a few attempts:\\nsql&gt; select  \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port, sum(Total)\\n&gt; from \\\\\"myindex2019.10.09\\\\\"\\n&gt; where Port = 5201 AND\\n&gt;       to_date(\\\\\"@timestamp\\\\\", \\'MM/DD/YYYY HH24:MI:SS\\') &gt;= sysdate - 5/(24*60)\\n&gt; group by \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port;\\n&gt; Bad request [Found 2 problem(s)\\nline 4:7: Unknown function [to_date], did you mean any of [CURDATE, TODAY, LOCATE, TRUNCATE]?\\nline 4:57: Unknown column [sysdate]]\\nsql&gt;\\nsql&gt;\\nsql&gt; select  \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port, sum(Total)\\n&gt; from \\\\\"myindex2019.10.09\\\\\"\\n&gt; where Port = 5201 AND\\n&gt;       \\\\\"@timestamp\\\\\" &gt;= CURDATE - 5/(24*60)\\n&gt; group by \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port;\\n&gt; Bad request [Found 1 problem(s)\\nline 4:23: Unknown column [CURDATE]]\\nsql&gt;\\nsql&gt;\\nsql&gt; select  \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port, sum(Total)\\n&gt; from \\\\\"myindex2019.10.09\\\\\"\\n&gt; where Port = 5201 AND\\n&gt;       \\\\\"@timestamp\\\\\" &gt;= CURDATE() - 5/(24*60)\\n&gt; group by \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port;\\n&gt; Bad request [Found 1 problem(s)\\nline 4:23: first argument of [CURDATE() - 5/(24*60)] must be [numeric], found value [CURDATE()] type [date]]\\nsql&gt;\\nsql&gt;\\n\\nAny advice on how to crack this syntax?\\nAnd… as a general question…  Is there an online guide to the ES/SQL syntax?  It will be pretty tedious if I have to post on the forums every time I have a general question like this.  Thanks!'},\n",
       " {'text_field': 'DATEADD and DATEDIFF were added recently to es-sql and will be available in 7.5.0.\\nBut, in your specific scenario you should be able to do\\nSELECT  \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port, SUM(Total) \\\\\"myindex2019.10.09\\\\\" WHERE Port = 5201 AND \\\\\"@timestamp\\\\\" &gt;= NOW() - INTERVAL 5 MINUTES GROUP BY \\\\\"@timestamp\\\\\", SrcIP, DstIP, Protocol, Port\\n\\nSo, basically NOW() - INTERVAL 5 MINUTES.\\nThough, I would suggest not grouping by @timestamp - it will be a rather intensive aggregation (creating buckets at millisecond level).\\nRegarding the documentation, I\\'d say it\\'s pretty good and comprehensive. The language and syntax in general is covered here. While the bit you are interested in is in the INTERVALS section. If you have a any suggestions for improvements, please don\\'t hesitate to ask for it in our github repo (Search/SQL label). Thanks.'},\n",
       " {'text_field': \"Hi,\\ncan anybody tell me where I can find the config, displayed in kibana to set ignore_above to a higher value? I'm using Ubuntu.\\nThe current 256 chars don't fit as we often get longer messages (even after parsing). Currently I've got to use the truncate filter which often cuts off relevant informations.\"},\n",
       " {'text_field': 'Hi @Christian_Lorenz,\\nJust as @LizaD recommended, I would suggest updating the ignore_above[1] configuration for strings in your index templates associated with your Logstash indices. This can either be done by editing the existing default logstash mapping, or by applying a new template that takes precedence [2] over the existing ones.\\nWithout knowing exactly how your Logstash mappings are determined, I will try and share a snippet of a new template that would define the behavior I believe you are hoping to modify.\\nPUT _template/ignore_above\\n{\\n  \"index_patterns\": [\\n    \"logstash*\"\\n  ],\\n  \"mappings\": {\\n    \"dynamic_templates\": [\\n      {\\n        \"strings_as_keyword\": {\\n          \"match_mapping_type\": \"string\",\\n          \"mapping\": {\\n            \"type\": \"text\",\\n            \"fields\": {\\n              \"keyword\": {\\n                \"ignore_above\": 512,\\n                \"type\": \"keyword\"\\n              }\\n            }\\n          }\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nYou can find the default template for Logstash and ES 7.x here [3] You will notice that there is no explicit mention of the ignore_above: 256 there. That is because this is a global Elasticsearch default on all dynamic fields that are recognized as strings. Here is a blog post that explains this. For more information about Logstash template management you can check out the docs\\nDoes that help?'},\n",
       " {'text_field': \"Hello all,\\nI'm trying  to get the hours of the day that the message was sent at so I can filter for during work hours in kibana but the doc['@timestamp'].date.hourOfDay that is suggested to use in the painless scripting documentation is not working.  any advice would be much appreciated\\nRegards,\"},\n",
       " {'text_field': \"This will depend on the context you're running in. It sounds like you are a filter context?\\n\\nFilter context example: https://www.elastic.co/guide/en/elasticsearch/reference/7.4/query-dsl-script-query.html\\n\\nAggregation context example: https://www.elastic.co/guide/en/elasticsearch/reference/7.4/search-aggregations-bucket-datehistogram-aggregation.html#_using_a_script_to_aggregate_by_day_of_the_week\\n\\n\"},\n",
       " {'text_field': 'i would like to copy logstash template to a newname template.  I did a GET _template/logstash , copied and edited (replace the word \"logstash\" with \"blah\")\\nDid a PUT  but got an error of:\\n{\\n  \"error\": {\\n    \"root_cause\": [\\n      {\\n        \"type\": \"parse_exception\",\\n        \"reason\": \"unknown key [blah] in the template \"\\n      }\\n    ],\\n    \"type\": \"parse_exception\",\\n    \"reason\": \"unknown key [blah] in the template \"\\n  },\\n  \"status\": 400\\n}\\n\\nPUT _template/blah\\n{\\n  \"blah\" : {\\n    \"order\" : 0,\\n    \"version\" : 60001,\\n    \"index_patterns\" : [\\n      \"blah-*\"\\n    ],\\n    \"settings\" : {\\n      \"index\" : {\\n        \"number_of_shards\" : \"1\",\\n        \"refresh_interval\" : \"5s\"\\n      }\\n    },\\n    \"mappings\" : {\\n      \"dynamic_templates\" : [\\n        {\\n          \"message_field\" : {\\n            \"path_match\" : \"message\",\\n            \"mapping\" : {\\n              \"norms\" : false,\\n              \"type\" : \"text\"\\n            },\\n            \"match_mapping_type\" : \"string\"\\n          }\\n        },\\n        {\\n          \"string_fields\" : {\\n            \"mapping\" : {\\n              \"norms\" : false,\\n              \"type\" : \"text\",\\n              \"fields\" : {\\n                \"keyword\" : {\\n                  \"ignore_above\" : 256,\\n                  \"type\" : \"keyword\"\\n                }\\n              }\\n            },\\n            \"match_mapping_type\" : \"string\",\\n            \"match\" : \"*\"\\n          }\\n        }\\n      ],\\n      \"properties\" : {\\n        \"@timestamp\" : {\\n          \"type\" : \"date\"\\n        },\\n        \"geoip\" : {\\n          \"dynamic\" : true,\\n          \"properties\" : {\\n            \"ip\" : {\\n              \"type\" : \"ip\"\\n            },\\n            \"latitude\" : {\\n              \"type\" : \"half_float\"\\n            },\\n            \"location\" : {\\n              \"type\" : \"geo_point\"\\n            },\\n            \"longitude\" : {\\n              \"type\" : \"half_float\"\\n            }\\n          }\\n        },\\n        \"@version\" : {\\n          \"type\" : \"keyword\"\\n        }\\n      }\\n    },\\n    \"aliases\" : { }\\n  }\\n}\\n\\nHow do I copy logstash template?  such that i can use \"blah\" in my index names?\\nthanks!\\nSirjune'},\n",
       " {'text_field': 'Thanks much @Badger!  I was able to create a new template.  My target is to (at least) have  the geoip mappings without \"logstash\" prefix in my index name.'},\n",
       " {'text_field': \"We currently have hot and warm nodes spread around 3 availability zones (AZ) in AWS.\\nI want to start using ILM, and I'd like to know how to configure the data transfer between warm and hot nodes so that hot nodes in an AZ always sends the data to warm nodes in the same AZ.\\nThis is because AWS charges for moving data between AZs, but not when we move data within an AZ.\\nIs it possible to do this using ILM?\\nThanks.\"},\n",
       " {'text_field': 'Hi dwilches,\\nILM can move data based on node attributes, so as long as the nodes have different attributes assigned to them within an AZ, then you would be able to configure ILM actions to move the data between different nodes.\\nI would recommend checking out https://www.elastic.co/blog/implementing-hot-warm-cold-in-elasticsearch-with-index-lifecycle-management for an example of how to do it (you don\\'t necessarily have to use \"hot\", \"warm\", or \"cold\" for attributes.'},\n",
       " {'text_field': 'Hi, I create a ingest pipeline on a ECE cluster, but everytime I try to ingest a document I\\'m receiving the next error:\\n\"root_cause\": [\\n      {\\n        \"type\": \"illegal_state_exception\",\\n        \"reason\": \"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\"\\n      }\\n\\nIf I validate there is a pipeline created:\\ncurl --request GET \\\\\\n  --url \\'https://c7fd43a0fc4846d7aa4845a36627d6c2.10.50.100.36.ip.es.io:9243/_ingest/pipeline/attachment?pretty=\\' \\\\\\n  --header \\'authorization: Basic ZWxhc3RpYzprcVdQZmszbnJTeDlFeFczaWJ4WjhjM1E=\\'\\n\\n{\\n  \"attachment\": {\\n    \"description\": \"Extract attachment information from non PDF documents\",\\n    \"processors\": [\\n      {\\n        \"attachment\": {\\n          \"field\": \"file\",\\n          \"indexed_chars\": -1,\\n          \"indexed_chars_field\": \"max_size\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nBut when I try to create a document\\ncurl --request POST \\\\\\n  --url \\'https://c7fd43a0fc4846d7aa4845a36627d6c2.10.50.100.36.ip.es.io:9243/digitaldocument/_doc/117?pipeline=attachment\\' \\\\\\n  --header \\'authorization: Basic ZWxhc3RpYzprcVdQZmszbnJTeDlFeFczaWJ4WjhjM1E=\\' \\\\\\n  --header \\'content-type: application/json\\' \\\\\\n  --data \\'{ \\n\\t\"documentId\": \"117\",\\n\\t\"documentName\": \"mypersonaltest.doc\",\\n\\t\"documentType\": \"WORD\",\\n\\t\"dateCreated\": \"04-10-2019 18:07:20\",\\n\\t\"userCreated\": \"francisco.noguera@ey.com\",\\n\\t\"language\":\"en\",\\n\\t\"file\":\"e1xydGYxXGFuc2kNCkxvcmVtIGlwc3VtIGRvbG9yIHNpdCBhbWV0DQpccGFyIH0=\"\\n}\\'\\n\\nI receive the error.\\nAny advices?'},\n",
       " {'text_field': 'Hello,\\nIt appears that the nodes you have in the cluster are not ingest capable - you can check/confirm that with a call to _cat/nodes and confirm you have at least a node marked as such.  If not, you should check the template (Platform -&gt; Templates -&gt; Deployment / Instance ) that the cluster is based on, that the node types you are using include the ingest capability.'},\n",
       " {'text_field': 'Hello, We just switched to Elasticsearch 7.0.0.\\nWhat is the correct setting in the elasticsearch.yml file?\\nWe are having issues shown below:\\nLog node 2\\n[2019-10-22T16:57:24,317][INFO ][o.e.p.PluginsService     ] [lumisportal-node-2] loaded module [x-pack-ml]\\n[2019-10-22T16:57:24,317][INFO ][o.e.p.PluginsService     ] [lumisportal-node-2] loaded module [x-pack-monitoring]\\n[2019-10-22T16:57:24,317][INFO ][o.e.p.PluginsService     ] [lumisportal-node-2] loaded module [x-pack-rollup]\\n[2019-10-22T16:57:24,317][INFO ][o.e.p.PluginsService     ] [lumisportal-node-2] loaded module [x-pack-security]\\n[2019-10-22T16:57:24,317][INFO ][o.e.p.PluginsService     ] [lumisportal-node-2] loaded module [x-pack-sql]\\n[2019-10-22T16:57:24,317][INFO ][o.e.p.PluginsService     ] [lumisportal-node-2] loaded module [x-pack-watcher]\\n[2019-10-22T16:57:24,317][INFO ][o.e.p.PluginsService     ] [lumisportal-node-2] no plugins loaded\\n[2019-10-22T16:57:28,740][INFO ][o.e.x.s.a.s.FileRolesStore] [lumisportal-node-2] parsed [0] roles from file [E:\\\\elasticsearch\\\\config\\\\roles.yml]\\n[2019-10-22T16:57:29,614][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [lumisportal-node-2] [controller/3020] [Main.cc@109] controller (64 bit): Version 7.1.0 (Build a8ee6de8087169) Copyright (c) 2019 Elasticsearch BV\\n[2019-10-22T16:57:30,067][DEBUG][o.e.a.ActionModule       ] [lumisportal-node-2] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security\\n[2019-10-22T16:57:30,192][INFO ][o.e.d.DiscoveryModule    ] [lumisportal-node-2] using discovery type [zen] and seed hosts providers [settings]\\n[2019-10-22T16:57:31,223][INFO ][o.e.n.Node               ] [lumisportal-node-2] initialized\\n[2019-10-22T16:57:31,223][INFO ][o.e.n.Node               ] [lumisportal-node-2] starting ...\\n[2019-10-22T16:57:31,410][INFO ][o.e.t.TransportService   ] [lumisportal-node-2] publish_address {IP_internal:9300}, bound_addresses {IP_internal:9300}\\n[2019-10-22T16:57:31,410][INFO ][o.e.b.BootstrapChecks    ] [lumisportal-node-2] bound or publishing to a non-loopback address, enforcing bootstrap checks\\n[2019-10-22T16:57:41,448][WARN ][o.e.c.c.ClusterFormationFailureHelper] [lumisportal-node-2] master not discovered yet: have discovered []; discovery will continue using [IP_internal:9300] from hosts providers and [{lumisportal-node-2}{TH8cWLD5TQa89m_uydyP2g}{_L8xfQ57Q7e0lU_y0ZjjPA}{IP_internal}{IP_internal:9300}{ml.machine_memory=34359267328, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\\n[2019-10-22T16:57:51,456][WARN ][o.e.c.c.ClusterFormationFailureHelper] [lumisportal-node-2] master not discovered yet: have discovered []; discovery will continue using [IP_internal:9300] from hosts providers and [{lumisportal-node-2}{TH8cWLD5TQa89m_uydyP2g}{_L8xfQ57Q7e0lU_y0ZjjPA}{IP_internal}{IP_internal:9300}{ml.machine_memory=34359267328, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\\n[2019-10-22T16:58:01,467][WARN ][o.e.n.Node               ] [lumisportal-node-2] timed out while waiting for initial discovery state - timeout: 30s\\n[2019-10-22T16:58:01,467][WARN ][o.e.c.c.ClusterFormationFailureHelper] [lumisportal-node-2] master not discovered yet: have discovered []; discovery will continue using [IP_internal:9300] from hosts providers and [{lumisportal-node-2}{TH8cWLD5TQa89m_uydyP2g}{_L8xfQ57Q7e0lU_y0ZjjPA}{IP_internal}{IP_internal:9300}{ml.machine_memory=34359267328, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\\n[2019-10-22T16:58:01,482][INFO ][o.e.h.AbstractHttpServerTransport] [lumisportal-node-2] publish_address {IP_internal:9200}, bound_addresses {IP_internal:9200}\\n[2019-10-22T16:58:01,482][INFO ][o.e.n.Node               ] [lumisportal-node-2] started\\n[2019-10-22T16:58:11,483][WARN ][o.e.c.c.ClusterFormationFailureHelper] [lumisportal-node-2] master not discovered yet: have discovered []; discovery will continue using [IP_internal:9300] from hosts providers and [{lumisportal-node-2}{TH8cWLD5TQa89m_uydyP2g}{_L8xfQ57Q7e0lU_y0ZjjPA}{IP_internal}{IP_internal:9300}{ml.machine_memory=34359267328, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\\n\\nelasticsearch.yml node-1\\ncluster.name: lumisportal-cluster-1\\nnode.name: lumisportal-node-1\\nnode.master: true\\nnode.data: true\\nbootstrap.memory_lock: true\\nnetwork.host: 10.153.35.133\\nhttp.port: 9200\\ndiscovery.seed_hosts: [\"ip-node-1\", \"ip-node-2\"]\\ncluster.initial_master_nodes: [\"lumisportal-node-1\"]\\n\\nelasticsearch.yml node-2\\ncluster.name: lumisportal-cluster-2\\nnode.name: lumisportal-node-2\\nnode.master: false\\nnode.data: false\\nbootstrap.memory_lock: true\\nnetwork.host: ip-node-2\\nhttp.port: 9200\\ndiscovery.seed_hosts: [\"ip-note-2\", \"ip-node-1\"]\\ncluster.initial_master_nodes: [\"lumisportal-node-1\"]'},\n",
       " {'text_field': 'Rogerio,\\nThe two configurations need to have the same cluster name.'},\n",
       " {'text_field': \"Hi,\\nI have enabled security in elasticsearch and kibana.\\nI have created a user with only the built-in kibana_user role assigned to it.\\nIt seems this user can create other users and assign whatever role he wants to them, also edit its own user roles like for example add the superuser role... this can't be normal?\\nElasticsearch version 6.8.1\\nKibana runs on one of the master nodes\"},\n",
       " {'text_field': 'Hey @gregorys, it\\'s \"expected\" in that it\\'s a known bug we need to fix. https://github.com/elastic/kibana/issues/35613 discusses some aspects of the current behavior, and why Kibana does what it does. Elasticsearch treats the anonymous user as the set of privileges that all users are granted. For example, if you enable the anonymous user in Elasticsearch and then you authenticate and provide the credentials for a user with no roles, they\\'ll get the privileges of the anonymous user. The only way to overcome this behavior is to no longer use the anonymous user in Elasticsearch.\\nIdeally, would Kibana no longer require the end-user to login in your situation and automatically allow them to be authenticated as the anonymous user?'},\n",
       " {'text_field': \"Hello,\\nI can't seem to get Distributed Tracing to work. Im starting to wonder if I don't understand Distributed Tracing correctly.\\nIn my frontend I am creating a Transaction and a few spans to cover some JS functions. One of the JS functions makes a request to a backend Python service. I am sending the traceID from the Transaction created in the frontend as part of the request. The Python service is calling begin_transaction and setting trace_parent to be the traceID passed in. Backend code calls a couple methods with spans, finally ending its transaction with a response back to the frontend. The frontend then ends its transaction.\\nClient Transaction AAA (traceId of XXX) -&gt; Span BBB -&gt; Ajax (traceId of XXX) -&gt; Python begin_transaction CCC (using traceId of XXX) -&gt; Python Span DDD -&gt; End Span DDD -&gt; End Transaction CCC -&gt; End Span BBB -&gt; End Transaction AAA\\nHopefully that makes sense, but that is a rough chain of the calls being done and what I would expect from a distributed trace in Kibana.\\nAm I doing something wrong? Is there a good tutorial some has I should check out?\\nThanks for the help!\\nEDIT:\\nI should mention I see each of these individual transactions and spans in Kibana... I just don't see a single distributed trace.\"},\n",
       " {'text_field': \"Hi James,\\nAs you already figured out, custom transactions does not support automatic instrumentations (XHR, Fetch, Resources) which means the user would have to start span and attach it to the transaction manually.\\nAnd yes by setting { managed: true } in the startTransaction option the transaction would be managed automatically which means the transaction would end as soon as current tasks(api calls) are done.\\n\\nHowever you can hold of the current managed transaction from closing by adding a task and removing the task when you want to end the transaction.\\n\\nconst tr = apm.startTransaction('custom', 'custom', { managed: true })\\n\\nconst id = tr.addTask();\\n// the above call would make sure the transaction cannot be automatically ended till all scheduled tasks are done. \\n\\n\\n// remove task once you are done instrumenting\\ntr.removeTask(id)\\n\\ntr.end()\\n\\nBy this way, you are in full control of the transaction plus also you get the benefits of instrumenting other parts of code.\\n\\nSince the above solution is not what we would like for a custom transaction, we would expose an API in the next versions to allow users to inject headers for the custom transactions/spans to make the DT work as intended. created an issue for the same - https://github.com/elastic/apm-agent-rum-js/issues/468\\n\\n\\nThanks for the feedback and detailed info.\\nCheers,\\nVignesh\"},\n",
       " {'text_field': 'I am using Logstash 7.4 to ingest JSON Lines.\\nIn this particular JSON Lines data, which is from a proprietary source, the event timestamp is the first time value in each incoming line.\\nBy first, I am referring to the serialized JSON Lines input data, which might arrive in a stream over TCP or from a file. I am aware of the following text in the JSON standard (ECMA-404):\\n\\nThe JSON syntax ... does not assign any significance to the ordering of name/value pairs. ... [This] may be defined by JSON processors or in specifications defining specific uses of JSON for data interchange.\\n\\nThe property to use as the event timestamp depends on the code property, which identifies the incoming event type. For example:\\n{\"code\":\"abc-123\",\"system\":\"mysys\",\"tranid\":\"xyz\",\"start\":\"2019-10-22T13:00:00.01Z\",\"cpu\":0.05,\"stop\":\"2019-10-22T13:00:00.02Z\"}\\n{\"code\":\"def-456\",\"collected\":\"2019-10-22T13:15:00Z\",\"errors\":321,\"#tran\":54321}\\n\\nIn the first line, the event timestamp is the start property value, which is the fourth property in the line.\\nIn the second line, the event timestamp is the collected property, which is the second property in the line.\\nIn each case, start and collected are the first properties with a time value.\\nI could use the code property value with conditional (if) statements to identify the name of the JSON property to use as the event timestamp:  if code is this value, then map the timestamp to the value of this property. However, I\\'d prefer to avoid such conditional statements. I\\'d like to avoid any code-specific config.\\nI\\'ve already successfully configured a different analytics platform (rhymes with punk  ) to ingest this data without any code-specific config. This is because that platform uses a text/regex-based method to extract timestamps, before it \"knows\" about the data being JSON Lines.\\nI\\'d appreciate suggestions for a Logstash config that:\\n\\nIdentifies the first value in an incoming line of data that matches an \"ISO8601\" pattern, and uses the date filter to set the event timestamp and (possibly, and then)\\nUses the json_lines codec for the data\\n\\nIt\\'s occurred to me to customize the json_lines codec to output a message field consisting of the entire (unparsed) input line. Then I could extract (e.g. grok) the timestamp from that message text. Then remove the message field before output. However, I\\'d prefer to find a solution that does not involve a custom codec, while still being reasonably performant.\\nThoughts, suggestions welcome.\\nP.S. I\\'ve added a related issue (feature request) in GitHub for json_lines: \"Add option to output unparsed line in message field, in addition to parsing\".'},\n",
       " {'text_field': 'A local colleague supplied me with the following working config:\\ninput {\\n  tcp {\\n    port =&gt; 6789\\n    codec =&gt; line\\n  }\\n}\\nfilter {\\n  grok {\\n    match =&gt; { \"message\" =&gt; \"%{TIMESTAMP_ISO8601:_time}\" }\\n  }\\n  date {\\n    match =&gt; [ _time, ISO8601 ]\\n  }\\n  json {\\n    source =&gt; \"message\"\\n    remove_field =&gt; [ _time, message ]\\n  }\\n}\\n...\\n\\nKudos for using the lines codec. I wish I\\'d thought of that. Also for citing the supplied TIMESTAMP_ISO8601 Grok pattern, which is a perfect fit for this use case. Sweet!'},\n",
       " {'text_field': \"I have a filebeat.yml file, in which I want to define multiple filebeat inputs. I understand each input needs to be configured separately, but that would be sub-optimal in my opinion, since I could re-use a lot of properties that are shared across multiple inputs.\\nAs an example, looking at this link, they have 2 inputs: https://www.elastic.co/guide/en/beats/filebeat/master/configuration-filebeat-options.html\\nMy use case is, I need to define the multiline pattern settings, which happen to be the exact same between all my filebeat inputs. Ideally I do not want to repeat the same thing over and over again.\\nLooking at documentation, I couldn't find my answer, but hopefully I am wrong. It would be a lot of config duplication if this is not supported.\"},\n",
       " {'text_field': \"I don't know how standard yaml is the beats config parser, but I would expect yaml anchors to work.\\nCan you try them?\"},\n",
       " {'text_field': 'Hello,\\nIs it possible to filter the bucket list result of significant term aggregations using multiple fields to be filtered?\\nI am trying to create a recommendation feature using ES based on this article at medium\\nhttps://towardsdatascience.com/how-to-build-a-recommendation-engine-quick-and-simple-aec8c71a823e.\\nI store the search data as array of objects instead of array of strings as the given example, because i need other fields to be filtered to get correct bucket list result. Here is the index mapping:\\n{\\n  \"mapping\": {\\n    \"properties\": {\\n      \"user\": {\\n        \"type\": \"keyword\",\\n        \"ignore_above\": 256\\n      },\\n      \"comic_subscribes\": {\\n        \"properties\": {\\n          \"genres\": {\\n            \"type\": \"keyword\",\\n            \"ignore_above\": 256\\n          },\\n          \"id\": {\\n            \"type\": \"keyword\",\\n            \"ignore_above\": 256\\n          },\\n          \"type\": {\\n            \"type\": \"keyword\",\\n            \"ignore_above\": 256\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nI have 2 conditions to be filtered:\\n\\ncomic_subscribes.type must be \"serial\"\\ncomic_subscribes.genre must not in \"hentai\" or \"echii\"\\n\\nCurrent aggregation query is like this:\\n{\\n    \"size\": 0,\\n    \"query\": {\\n        \"bool\": {\\n            \"should\": [\\n                {\\n                    \"term\": {\\n                        \"comic_subscribes.id\": \"1\"\\n                    }\\n                }\\n            ],\\n            \"minimum_should_match\": 1,\\n            \"filter\": {\\n                \"term\": {\\n                    \"comic_subscribes.type\": \"serial\"\\n                }\\n            },\\n            \"must_not\": [\\n                {\\n                    \"bool\": {\\n                        \"should\": [\\n                            {\\n                                \"term\": {\\n                                    \"comic_subscribes.genres\": \"hentai\"\\n                                }\\n                            },\\n                            {\\n                                \"term\": {\\n                                    \"comic_subscribes.genres\": \"echii\"\\n                                }\\n                            }\\n                        ],\\n                        \"minimum_should_match\": 1\\n                    }\\n                }\\n            ]\\n        }\\n    },\\n    \"aggs\": {\\n        \"recommendations\": {\\n            \"significant_terms\": {\\n                \"field\": \"comic_subscribes.id\",\\n                \"exclude\": [\"1\"],\\n                \"min_doc_count\": 1,\\n                \"size\": 10\\n            }\\n        }\\n    }\\n}\\n\\nI have tried to use filter aggregation but nothing seems to be worked. So is it possible using exclude filtering or is there any other way to achieve my requirements? Thanks'},\n",
       " {'text_field': 'Ok, bros. I think there is no option method to filter aggregation significant terms bucket list result using different field.\\nBased on elasticsearch documentation Significant Terms Aggregation Parameters, which refers to Terms Aggregation Filtering Value. There is no other option than filter using partition expression and filter values with exact values (which i have been using as above, \"exclude\" param).\\nSo i create other way around by getting the comic ids which i want to exclude and store it as excludeComics variable in array. Then use the excludeComics var in exclude param. And boom, there you go. Filtered significant terms aggregation bucket list result.'},\n",
       " {'text_field': 'Hello:\\nIs there a way to update the max_snapshot_bytes_per_sec setting on a repository without deleting and recreating it?\\nWe have an S3 repo configured with a bunch of snapshots. As the daily data volume has grown the time it takes to complete the snapshot has increased. We would like to bump up the max_snapshot_bytes_per_sec setting on the repo. However, updating the repo results in this error:\\n&gt;     \"type\": \"illegal_state_exception\",\\n&gt;     \"reason\": \"trying to modify or unregister repository that is currently used \"\\n\\nIs there a way to update this setting without creating a new repo?'},\n",
       " {'text_field': 'This appears to be a known limitation of Elasticsearch. See #18785\\nBased on the issue description, I waited for the running snapshots to finish (using GET _snapshot/_status) and then deleted the repository and added it back again with the new settings. The repo was live with new settings and all our existing snapshots were available.'},\n",
       " {'text_field': 'I\\'m trying to create Elasticsearch cluster with 9 nodes. i\\'m working on Elastic 7.3.2 version .i have 3 master only nodes , 1 voting only nodes, rest are date node,cordinate node and ingest node. i am getting the error as \"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and this node must discover master-eligible nodes\"\\nThe error i am getting is\\n2019-10-23T04:14:58,906][WARN ][o.e.c.c.ClusterFormationFailureHelper] [master-a] master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and this node must discover master-eligible nodes [master-a, master-b, master-c] to bootstrap a cluster: have discovered [{master-a}{8RPo7rbGTyaImEW7riET6Q}{8PSk648MTJePxNMKHuCcUg}{10.55.50.104}{10.55.50.104:9300}{m}{xpack.installed=true}, {master-b}{8RPo7rbGTyaImEW7riET6Q}{EzAjKx6gQHyMeuwHkIqtAA}{10.55.50.101}{10.55.50.101:9300}{m}{xpack.installed=true}, {vote}{8RPo7rbGTyaImEW7riET6Q}{O442bjqIRZW4h1UC1MPpSw}{10.55.50.108}{10.55.50.108:9300}{mv}{xpack.installed=true}]; discovery will continue using [10.55.50.100:9300, 10.55.50.101:9300, 10.55.50.102:9300, 10.55.50.103:9300, 10.55.50.105:9300, 10.55.50.106:9300, 10.55.50.107:9300, 10.55.50.108:9300] from hosts providers and [{master-a}{8RPo7rbGTyaImEW7riET6Q}{8PSk648MTJePxNMKHuCcUg}{10.55.50.104}{10.55.50.104:9300}{m}{xpack.installed=true}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\\n[2019-10-23T04:15:02,177][WARN ][o.e.c.c.ClusterBootstrapService] [master-a] exception when bootstrapping with VotingConfiguration{{bootstrap-placeholder}-master-c,8RPo7rbGTyaImEW7riET6Q}, rescheduling.'},\n",
       " {'text_field': 'Actually, I see the issue:\\n\\n\\n\\n Ozil:\\n\\nhave discovered [\\n{master-a}{8RPo7rbGTyaImEW7riET6Q}{8PSk648MTJePxNMKHuCcUg}{10.55.50.104}{10.55.50.104:9300}{m}{xpack.installed=true},\\n{master-b}{8RPo7rbGTyaImEW7riET6Q}{EzAjKx6gQHyMeuwHkIqtAA}{10.55.50.101}{10.55.50.101:9300}{m}{xpack.installed=true},\\n{vote}{8RPo7rbGTyaImEW7riET6Q}{O442bjqIRZW4h1UC1MPpSw}{10.55.50.108}{10.55.50.108:9300}{mv}{xpack.installed=true}]\\n\\n\\n\\nYour three nodes all have the same node ID. This means you copied their data paths. You should not do this. Each node should start from a completely empty data path.'},\n",
       " {'text_field': 'I configured elastic to do x-pack security.\\nFollowed the documentation to create pkcs12 certs using the certutil, configured the password, and also added keystore entry to match the password.\\nWhen I start elastic, I get the follwing:\\n\\nCaused by: org.elasticsearch.ElasticsearchException: failed to initialize SSL TrustManager\\nat org.elasticsearch.xpack.core.ssl.StoreTrustConfig.createTrustManager(StoreTrustConfig.java:74) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.SSLService.createSslContext(SSLService.java:384) ~[?:?]\\nat jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]\\nat jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?]\\nat jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]\\nat java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) ~[?:?]\\nat java.lang.reflect.Constructor.newInstance(Constructor.java:481) ~[?:?]\\nat org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:605) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:556) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.4.0.jar:7.4.0]\\n... 6 more\\nCaused by: java.io.IOException: keystore password was incorrect\\nat sun.security.pkcs12.PKCS12KeyStore.engineLoad(PKCS12KeyStore.java:2118) ~[?:?]\\nat sun.security.util.KeyStoreDelegator.engineLoad(KeyStoreDelegator.java:222) ~[?:?]\\nat java.security.KeyStore.load(KeyStore.java:1472) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.TrustConfig.getStore(TrustConfig.java:97) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.StoreTrustConfig.createTrustManager(StoreTrustConfig.java:65) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.SSLService.createSslContext(SSLService.java:384) ~[?:?]\\nat java.util.HashMap.computeIfAbsent(HashMap.java:1138) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.SSLService.loadConfiguration(SSLService.java:446) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.SSLService.lambda$loadSSLConfigurations$2(SSLService.java:426) ~[?:?]\\nat java.util.HashMap.forEach(HashMap.java:1338) ~[?:?]\\nat jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?]\\nat jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]\\nat java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) ~[?:?]\\nat java.lang.reflect.Constructor.newInstance(Constructor.java:481) ~[?:?]\\nat org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.4.0.jar:7.4.0]\\n... 6 more\\nCaused by: java.security.UnrecoverableKeyException: failed to decrypt safe contents entry: javax.crypto.BadPaddingException: Given final block not properly padded. Such issues can arise if a bad key is used during decryption.\\nat sun.security.pkcs12.PKCS12KeyStore.engineLoad(PKCS12KeyStore.java:2118) ~[?:?]\\nat sun.security.util.KeyStoreDelegator.engineLoad(KeyStoreDelegator.java:222) ~[?:?]\\nat java.security.KeyStore.load(KeyStore.java:1472) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.TrustConfig.getStore(TrustConfig.java:97) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.StoreTrustConfig.createTrustManager(StoreTrustConfig.java:65) ~[?:?]\\nat java.util.HashMap.forEach(HashMap.java:1338) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.SSLService.loadSSLConfigurations(SSLService.java:426) ~[?:?]\\nat org.elasticsearch.xpack.core.ssl.SSLService.(SSLService.java:121) ~[?:?]\\nat org.elasticsearch.xpack.core.XPackPlugin.(XPackPlugin.java:142) ~[?:?]\\nat jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]\\nat jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:?]\\nat jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]\\nat java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500) ~[?:?]\\nat java.lang.reflect.Constructor.newInstance(Constructor.java:481) ~[?:?]\\nat org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:605) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:556) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:471) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.plugins.PluginsService.(PluginsService.java:163) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.node.Node.(Node.java:311) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.node.Node.(Node.java:255) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.bootstrap.Bootstrap$5.(Bootstrap.java:221) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:221) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:349) ~[elasticsearch-7.4.0.jar:7.4.0]\\nat org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.4.0.jar:7.4.0]\\n... 6 more\\n\\nHeres my config:\\n\\nxpack.security.enabled: true\\nxpack.monitoring.enabled: true\\ndiscovery.type: single-node\\nxpack.ssl.keystore.password: \\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.transport.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.enabled: true\\nxpack.security.http.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.client_authentication: optional\\n\\nI cant get elastic to start. Am I missing anything?'},\n",
       " {'text_field': \"This is not the correct configuration parameter. xpack.ssl.* default settings were removed in version 7 and even before these wouldn't apply to passwords. see https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html.\\nSince you define the keystore in the following for configuration settings\\nxpack.security.transport.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.transport.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\n\\nyou either need\\nxpack.security.transport.ssl.keystore.password: &lt;your_password_here&gt;\\nxpack.security.transport.ssl.truststore.password: &lt;your_password_here&gt;\\nxpack.security.http.ssl.keystore.password: &lt;your_password_here&gt;\\nxpack.security.http.ssl.truststore.password: &lt;your_password_here&gt;\\n\\nor alternatively add them to the secure settings with\\n./elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password\\n./elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password\\n./elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_password\\n./elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password\\n\"},\n",
       " {'text_field': 'I\\'m unable to find the right permissions to grant a user to let them import CSV files in Kibana.\\n\\n  \\n      \\n      Elastic Blog – 23 Jan 19\\n  \\n  \\n    \\n\\nImport CSV and Log Data into Elasticsearch from Kibana with File Data Visualizer\\n\\nWith the File Data Visualizer, it\\'s never been easier to import CSV, NDJSON, and semi-structured text (like log files) into Elasticsearch directly from Kibana.\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nThe user can load the CSV and visualize the data, but the \"Import\" button is grayed out and can\\'t be clicked:\\n\\nThe button is working for my super user. What permissions or roles are required for the import button to work?'},\n",
       " {'text_field': 'I checked the code and it looks like the user needs cluster:monitor/nodes/info and cluster:admin/ingest/pipeline/put privileges: https://github.com/elastic/kibana/blob/master/x-pack/legacy/plugins/ml/public/datavisualizer/file_based/components/utils/utils.js#L122\\nLet me know whether that helps!'},\n",
       " {'text_field': 'Hello.\\nI believe automatic report generation is available in  GOLD subscription .\\n\\nShare &amp; collaborate\\n\\nEmbeddable dashboards\\nObject export UI &amp; APIs\\nCSV exports\\nPDF and PNG reports\\nSaved queries\\n\\n\\nHowever, would there be any way to implement automatic report generation by PDF or PNG image using other than Report API ?\\nMy goal is to export dashboard and back it up to S3.\\nI appreciate if I could get some help.\\nThanks,\\nYu Watanabe'},\n",
       " {'text_field': \"Hey @YuWatanabe,\\nIs there something lacking in the APIs that make them unusable for you? For an S3 backup, you could write a small script using curl to generate the report on-the-fly, and then subsequently publish to an S3 bucket.\\nOr, are you asking if there is a way to generate these reports without a paid subscription? There unfortunately isn't a way to do that, but you can run Kibana and Elasticsearch on Elastic Cloud, which gives you all of these features and more, for a very reasonable starting price.\"},\n",
       " {'text_field': '\\n  \\n    \\n    \\n    What is [use_field_mapping] format? Elasticsearch\\n  \\n  \\n    Hi, \\nI have a problem with my shards failing so I check es logs. \\nI find this warning : \\n[instance-0000000005] [use_field_mapping] is a special format that was only used to ease the transition to 7.x. It has become the default and shouldn\\'t be set explicitly anymore.\\n\\n\\nWhat is concerned by this format?\\nIs it has a lynk with my shards problem?\\n\\nThanks in advance\\n  \\n\\n\\nRegarding the question about use_field_mapping warnings in the linked post above:\\n\"[use_field_mapping] is a special format that was only used to ease the transition to 7.x. It has become the default and shouldn\\'t be set explicitly anymore.\"\\nI also get these warnings on the nodes in our cluster. I understand that use_field_mapping should no longer be used, but how can I find out where it is used? What should I look for? I have no recollection of ever using this in any scripts that index data to Elasticsearch.'},\n",
       " {'text_field': \"Hello @tomhe,\\nThe use_field_mapping parameter is set in the doc_values section of a search request: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-docvalue-fields.html. You would see this warning if you issued any searches that set this parameter. Unfortunately our logs don't yet give an easy way to track down the client that is causing the deprecation error.\\nIf you are using Kibana, then it's possible that Kibana itself is causing the deprecation warning -- Kibana still includes use_field_mapping parameter in some visualizations (https://github.com/elastic/kibana/issues/47727). It looks like the team is aware of the issue and is working on a fix.\"},\n",
       " {'text_field': 'i have setup EFK stack in k8 cluster, created the index pattern, it crated an extra field with \".keyword\" at the end, this field has aggregateable option enabled.\\n\\nwhy are there 2 same field mappings, one is aggregateable but the other one isn\\'t?\\ncant the original filed have the aggregateable option enabled?\\n\\nhere is a screenshot of what I am seeing:\\n\\nimage.png968×834 64.8 KB\\n'},\n",
       " {'text_field': 'The difference between these fields is how they are indexed - the regular one is indexed as text, which means you can do a full text search on it (suitable for fields that contain lots of text, e.g. an article or a description of a product). The other one is indexed as keyword which means you can only search for the whole value of the field, not individual parts of it (suitable for fields with short values, like a transaction type or an id). This is the blog post explaining it in greater detail: https://www.elastic.co/blog/strings-are-dead-long-live-strings\\nThe keyword field is aggregateable which means you can do things like terms aggregations on it.\\nIf you only want to filter by the whole value of a field and do aggregations on it, you don\\'t need the \"text\" indexed version of it. Having both fields is just the default mapping, you can specify an explicit mapping to just index the fields as keyword under the original field name: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html#create-mapping\\nFor more complex cases it might make sense to look into dynamic templates: https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-templates.html#dynamic-templates'},\n",
       " {'text_field': 'Hi,\\nWe are struggling with ES performance issue, we have daily indexes and minute or two after midnight our cluster is unavailable. Logstash indexer reports time out, after around 10 min everything back to normal. we try few different strategies, create indexer with 0 replica and replica during the day. In pick we creates 150 indexes. After investigation we discovered index create time is around 8 sec, even after high pick still it\\'s 8sec. We believe it\\'s root cause.\\nMy questions:\\n\\n\\nHow to speed up index creation ?\\n\\n\\nDoes it make sense have dedicated servers and VMs ? Does cluster speed depends on slowest node ?\\n\\n\\nOur cluster: 3 masters, 3 hot nodes (dedicated bare-metal serves) 6 cold nodes VMs, 1 ingest node.\\nWe have 20k shards, 15k on cold nodes.\\n\"version\" : {\\n\"number\" : \"6.4.2\",\\n\"build_flavor\" : \"default\",\\n\"build_type\" : \"rpm\",\\n\"build_hash\" : \"04711c2\",\\n\"build_date\" : \"2018-09-26T13:34:09.098244Z\",\\n\"build_snapshot\" : false,\\n\"lucene_version\" : \"7.4.0\",\\nThanks for any suggestions.'},\n",
       " {'text_field': 'The optimal hardware for your cluster very much depends on your needs and you can only really determine the best setup with careful benchmarking of a realistic workload. There are definitely workloads that benefit from 256GB or more of RAM. Elasticsearch will use any spare RAM for the filesystem cache, which can be many times larger than the JVM heap.'},\n",
       " {'text_field': 'Our application is indexing content and is passing the Elasticsearch 7.1 Shards limit of 1000.\\nWhat setting in elasticsearch.yml should we do to increase this limit?'},\n",
       " {'text_field': 'That sounds like a lot of shards for just one node. I recommend you read this blog post and look to reduce the shard count rather than increase the limit. having too many shards per node can lead to performance problems and instability. If you still want to go against the recommendation and increase it, the documentation can be found here.'},\n",
       " {'text_field': \"I'm having a weird issue while trying to compile and package an ingest plugin for ES 7.4.0.\\nI started from @spinscale's cookiecutter project template and was able to properly create the project and implement the business logic.\\nHowever, when trying to compile the project using gradle clean check I get the following error:\\n&gt; Task :generateGlobalBuildInfo FAILED\\n\\nFAILURE: Build failed with an exception.\\n\\n* What went wrong:\\n\\nExecution failed for task ':generateGlobalBuildInfo'.\\n\\n&gt; The compiler java.home must be set to a JDK installation directory for Java 12 but is [/Library/Java/JavaVirtualMachines/jdk-11.0.4.jdk/Contents/Home] corresponding to [11]\\n\\n* Try:\\n\\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\\n\\n* Get more help at https://help.gradle.org\\n\\nBUILD FAILED in 597ms\\n\\n1 actionable task: 1 executed \\n\\nI have Java 11 installed (as can be seen in the error message). Since Java 12 is EOL, it is impossible for me to install it. I have tried with Java 13, but I get a different error message:\\nFAILURE: Build completed with 2 failures.\\n\\n1: Task failed with an exception.\\n\\n-----------\\n\\n* Where:\\n\\nBuild file '/Users/consulthys/workspaces/elasticsearch-ingest-h3/build.gradle' line: 18\\n\\n* What went wrong:\\n\\nA problem occurred evaluating root project 'ingest-h3'.\\n\\n&gt; Failed to apply plugin [class 'de.thetaphi.forbiddenapis.gradle.ForbiddenApisPlugin']\\n\\n&gt; Could not create plugin of type 'ForbiddenApisPlugin'.\\n\\n&gt; Could not initialize class de.thetaphi.forbiddenapis.gradle.ForbiddenApisPlugin\\n\\n* Try:\\n\\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\\n\\n==============================================================================\\n\\n2: Task failed with an exception.\\n\\n-----------\\n\\n* What went wrong:\\n\\nA problem occurred configuring root project 'ingest-h3'.\\n\\n&gt; Must specify license and notice file for project :\\n\\n* Try:\\n\\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.\\n\\n==============================================================================\\n\\n* Get more help at https://help.gradle.org\\n\\nBUILD FAILED in 615ms\\n\\nSo it seems to require me to have Java 12, even though the support matrix explicitly states that Java 12 is not supported with 7.4.0, only Java 11 ad 13.\\nI'd appreciate if anyone could shed some light on this and how to resolve this issue, it's a bit frustrating.\"},\n",
       " {'text_field': 'Small workaround around the installation issue: Using sdkman installing java 12 is just a single command away. sdk install java 12.0.2-open. You can use sdk list java to list possible installation candidates.\\nYou can get sdkman at https://sdkman.io'},\n",
       " {'text_field': 'Hi, before upgrading my elastic stack I want to confirm whether Customize time range in version 7.4 available in basic licence.\\nplease confirm.\\nThanks'},\n",
       " {'text_field': 'Hi @Saurabh_Sharma_IIT! The Customize time range feature is available in the basic license. You can visit our subscriptions page for more information on what features are available in each license.'},\n",
       " {'text_field': 'Hi\\nI have install Logstash, ElasticSearch and Kibana, without any problems. But when I going to rund the Logstash conf file through the Command Prompt or PowerShell I get the same error messages... see picture:\\n\\nimage.png954×645 33.2 KB\\n\\nMy conf file look like this:\\n\\nimage.png918×485 15.3 KB\\n\\nCan someone help me? '},\n",
       " {'text_field': 'Change your elasticsearch output parameter \"host\" to \"hosts\" and change\\nyour sincedb_path to sincedb_path =&gt; \"NUL\"'},\n",
       " {'text_field': 'I\\'m trying to create a Watcher alert to Slack, every time an IP is generating more than 10 logs per minute, of a certain type (for example, ModSecurity).\\nThis is what I have so far:\\n{\\n  \"trigger\": {\\n    \"schedule\": {\\n      \"interval\": \"1m\"\\n    }\\n  },\\n  \"input\": {\\n    \"search\": {\\n      \"request\": {\\n        \"search_type\": \"query_then_fetch\",\\n        \"indices\": [\\n          \"filebeat-*\"\\n        ],\\n        \"rest_total_hits_as_int\": true,\\n        \"body\": {\\n          \"query\": {\\n            \"bool\": {\\n              \"filter\": [\\n                {\\n                  \"range\": {\\n                    \"@timestamp\": {\\n                      \"gte\": \"now-1min\",\\n                      \"lt\": \"now\"\\n                    }\\n                  }\\n                },\\n                {\\n                  \"term\": {\\n                    \"event.dataset\": \"modsecurity.log\"\\n                  }\\n                }\\n              ]\\n            }\\n          },\\n          \"aggs\": {\\n            \"sources\": {\\n              \"terms\": {\\n                \"field\": \"source.ip\"\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n    \"condition\" : {\\n      \"script\": \"return ctx.payload.hits.total &gt; ctx.metadata.min_hits\"\\n    },\\n  \"actions\": {\\n    \"log_hits\": {\\n      \"foreach\": \"ctx.payload.buckets.hits\",\\n      \"max_iterations\": 500,\\n      \"slack\": {\\n        \"account\": \"security\",\\n        \"message\": {\\n          \"from\": \"Watcher - ModSecurity\",\\n          \"text\": \"{{ctx.payload._source.source.ip}} generated *{{ctx.payload.hits.total}}* logs in the last minute\"\\n        }\\n      }\\n    }\\n  },\\n  \"metadata\": {\\n    \"min_hits\": 10\\n  }\\n}\\n\\nSo far, it generates some Slack notifications, but not what I\\'m expecting. Some times the source.ip is empty, and the total is less than 10.\\nThere is any easiest way to do this? This seems to me a pretty common watcher, but I couldn\\'t find anything similar in the docs.'},\n",
       " {'text_field': 'Thanks for the feedback! I managed to build the Watcher:\\n{\\n  \"trigger\": {\\n    \"schedule\": {\\n      \"interval\": \"1m\"\\n    }\\n  },\\n  \"input\": {\\n    \"search\": {\\n      \"request\": {\\n        \"search_type\": \"query_then_fetch\",\\n        \"indices\": [\\n          \"filebeat-*\"\\n        ],\\n        \"rest_total_hits_as_int\": true,\\n        \"body\": {\\n          \"query\": {\\n            \"bool\": {\\n              \"filter\": [\\n                {\\n                  \"range\": {\\n                    \"@timestamp\": {\\n                      \"gte\": \"now-1m\",\\n                      \"lt\": \"now\"\\n                    }\\n                  }\\n                },\\n                {\\n                  \"term\": {\\n                    \"event.dataset\": \"modsecurity.log\"\\n                  }\\n                }\\n              ]\\n            }\\n          },\\n          \"aggs\": {\\n            \"sources\": {\\n              \"terms\": {\\n                \"field\": \"source.ip\",\\n                \"min_doc_count\": 10\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"condition\": {\\n    \"compare\": {\\n      \"ctx.payload.hits.total\": {\\n        \"gt\": 0\\n      }\\n    }\\n  },\\n  \"actions\": {\\n    \"notify-slack\": {\\n      \"slack\": {\\n        \"account\": \"security\",\\n        \"message\": {\\n          \"from\": \"Watcher - ModSecurity\",\\n          \"text\": \"`{{ctx.payload.aggregations.sources.buckets.0.key}}` generated *{{ctx.payload.aggregations.sources.buckets.0.doc_count}}* logs in the last minute\"\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nWhat I\\'m not sure is what will happen if two different IPs are abusing at the same time, as as far as I know, the action will only notify about the first one found in the aggregation. Correct?\\nMaybe a foreach action is needed?'},\n",
       " {'text_field': 'Hello, I\\'m trying to find the duplicate based on a term \"TDID\" in a index.\\nBasically the TDID : \"00000-00000-0000000-00000\"\\n\\nGET ttd-2019-04*/_search\\n{\\n\"size\" : 0,\\n\"_source\": [\"TDID\"],\\n\"query\": {\\n\"term\": {\\n  \"eventName\": \"impressions\"\\n}\\n\\n},\\n\"aggs\":{\\n\"duplicate_aggs\":{\\n\"terms\":{\\n\"field\":\"TDID\",\\n\"min_doc_count\":2\\n}\\n}}\\n}\\n\\nI got result as,\\n\"agg    regations\" : {\\n\"duplicate_aggs\" : {\\n\"doc_count_error_upper_bound\" : 5448,\\n\"sum_other_doc_count\" : 77814191,\\n\"buckets\" : [\\n{\\n\"key\" : \"0000\",\\n\"doc_count\" : 261766\\n},\\n{\\n\"key\" : \"00000000\",\\n\"doc_count\" : 261542\\n},\\n{\\n\"key\" : \"000000000000\",\\n\"doc_count\" : 261542\\n},\\n{\\n\"key\" : \"4cc0\",\\n\"doc_count\" : 3540\\n}}\\nAs you can see, that the TDID is splitted and then aggreation was done. Could you please help me with that. Thanks for the support'},\n",
       " {'text_field': 'Vinothkumar,\\nAre you using any special mappings for these indexes? Which version of Elasticsearch are you running?\\nMy suspicion is that your TDID field is mapped as a text datatype. A text field\\'s contents will be split into tokens for searching. The mapping you want for an identifier like this is probably a keyword datatype, which stores exact values like email addresses or phone numbers efficiently for searching.\\nYou might have a keyword field as a default mapping. If so, you could use the following aggregation:\\n{\\n  [...],\\n  \"aggs\": {\\n    \"duplicate_aggs\": {\\n      \"terms\": {\\n        \"field\": \"key.keyword\",\\n        \"min_doc_count\": 2\\n      }\\n    }\\n  }\\n}\\n\\nIf that doesn\\'t work, the correct answer will depend on which Elasticsearch version you are running and what your index\\'s mappings are.\\n-William'},\n",
       " {'text_field': 'We are using filebeats 7.4.0 in a k8s cluster to ship logs to ES, however when specifying a processor to drop the agent.* fields they are still sent to ES. Config is as follows:\\n    filebeat.inputs:\\n    - type: docker\\n      containers.ids:\\n      - \\'*\\'\\n      processors:\\n      - add_docker_metadata:\\n      - add_kubernetes_metadata:\\n          in_cluster: true\\n      - rename:\\n          fields:\\n          - from: \"log_level\"\\n            to: \"level\"\\n          - from: \"log_tag\"\\n            to: \"tag\"\\n          ignore_missing: true\\n      - drop_fields:\\n          fields: [\"agent.ephemeral_id\", \"agent.hostname\", \"agent.id\", \"agent.type\", \"agent.version\", \"host.name\", \"ecs.version\", \"input.type\"]\\n          ignore_missing: true\\n    output.elasticsearch:\\n      hosts: [\"https://es-host:443\"]\\n      protocol: \"https\"\\n      compression: 3\\n'},\n",
       " {'text_field': 'There is a global Processors section. Please use that. I tired it and it works\\n#================================Processors=====================================\\n#Configure processors to enhance or manipulate events generated by the beat.\\n\\nprocessors:\\n  - drop_fields:\\n       fields: [\"agent.ephemeral_id\", \"agent.hostname\", \"agent.id\", \"agent.type\", \"agent.version\", \"host.name\", \"ecs.version\", \"input.type\"]'},\n",
       " {'text_field': 'Hi Elasticsearch Ninja Masters,\\nI’ve recently spun up an Elasticsearch v7.4.0 Docker container, and I’m using the “elasticsearch-sql-cli” script to run SQL-like queries off my ES instance.  Which is awesome, thank you for implementing that.\\nI have a specific syntax question, however.  I need to run the below query:\\nselect Value1, sum( Value2 * Value3 )\\nfrom \\\\\"myindex2019.10.09\\\\\"\\ngroup by Value1;\\n\\nPretty simple, really.  For every record in the “myindex2019.10.09” index, multiply the values of Value2 and Value3 together.  Then sum up all the (Value2 * Value3)’s together, grouped my Value1.  Should look like this:\\nValue1 | sum(Value2 * Value3)\\n-------+---------------------\\n1      |  100\\n2      |  200\\n3      |  ...etc...\\n\\nBut from experimentation, I can tell you that the “elasticsearch-sql-cli” script likes Value2 * Value3 or sum(Value2), but not sum(Value2 * Value3)  The full output I see when I try this query is below.  The “elasticsearch-sql-cli” looks like it throws a Java exception, but does not crash.\\nIs there a solution here?  Any suggestion would be useful.  Thanks!\\nsql&gt; select Value1, sum( Value2 * Value3 ) from \\\\\"myindex2019.10.09\\\\\";\\nServer error [Server encountered an error [Does not know how to convert argument Mul[] for function Sum[]]. [SqlIllegalArgumentException[Does not know how to convert argument Mul[] for function Sum[]]\\n        at org.elasticsearch.xpack.sql.planner.QueryTranslator.field(QueryTranslator.java:470)\\n        at org.elasticsearch.xpack.sql.planner.QueryTranslator$Sums.toAgg(QueryTranslator.java:854)\\n        at org.elasticsearch.xpack.sql.planner.QueryTranslator$Sums.toAgg(QueryTranslator.java:850)\\n        at org.elasticsearch.xpack.sql.planner.QueryTranslator$SingleValueAggTranslator.asAgg(QueryTranslator.java:971)\\n        at org.elasticsearch.xpack.sql.planner.QueryTranslator$AggTranslator.apply(QueryTranslator.java:961)\\n        at org.elasticsearch.xpack.sql.planner.QueryTranslator.toAgg(QueryTranslator.java:198)\\n        at org.elasticsearch.xpack.sql.planner.QueryFolder$FoldAggregate.addAggFunction(QueryFolder.java:445)\\n        at org.elasticsearch.xpack.sql.planner.QueryFolder$FoldAggregate.rule(QueryFolder.java:359)\\n        at org.elasticsearch.xpack.sql.planner.QueryFolder$FoldAggregate.rule(QueryFolder.java:204)\\n        at org.elasticsearch.xpack.sql.tree.Node.lambda$transformUp$11(Node.java:196)\\n        at org.elasticsearch.xpack.sql.tree.Node.transformUp(Node.java:190)\\n        at org.elasticsearch.xpack.sql.tree.Node.transformUp(Node.java:196)\\n        at org.elasticsearch.xpack.sql.planner.QueryFolder$FoldingRule.apply(QueryFolder.java:593)\\n        at org.elasticsearch.xpack.sql.planner.QueryFolder$FoldingRule.apply(QueryFolder.java:589)\\n        at org.elasticsearch.xpack.sql.rule.RuleExecutor$Transformation.&lt;init&gt;(RuleExecutor.java:82)\\n        at org.elasticsearch.xpack.sql.rule.RuleExecutor.executeWithInfo(RuleExecutor.java:158)\\n        at org.elasticsearch.xpack.sql.rule.RuleExecutor.execute(RuleExecutor.java:130)\\n        at org.elasticsearch.xpack.sql.planner.QueryFolder.fold(QueryFolder.java:82)\\n        at org.elasticsearch.xpack.sql.planner.Planner.foldPlan(Planner.java:38)\\n        at org.elasticsearch.xpack.sql.planner.Planner.plan(Planner.java:28)\\n        at org.elasticsearch.xpack.sql.session.SqlSession.lambda$physicalPlan$4(SqlSession.java:160)\\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62)\\n        at org.elasticsearch.xpack.sql.session.SqlSession.lambda$optimizedPlan$3(SqlSession.java:156)\\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62)\\n        at org.elasticsearch.xpack.sql.session.SqlSession.lambda$preAnalyze$2(SqlSession.java:144)\\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62)\\n        at org.elasticsearch.xpack.sql.analysis.index.IndexResolver.lambda$resolveAsMergedMapping$3(IndexResolver.java:277)\\n        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62)\\n        at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:70)\\n        at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:64)\\n        at org.elasticsearch.action.fieldcaps.TransportFieldCapabilitiesAction.lambda$doExecute$1(TransportFieldCapabilitiesAction.java:88)\\n        at org.elasticsearch.action.fieldcaps.TransportFieldCapabilitiesAction$1.onResponse(TransportFieldCapabilitiesAction.java:101)\\n        at org.elasticsearch.action.fieldcaps.TransportFieldCapabilitiesAction$1.onResponse(TransportFieldCapabilitiesAction.java:97)\\n        at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:70)\\n        at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:64)\\n        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleResponse(TransportSingleShardAction.java:261)\\n        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleResponse(TransportSingleShardAction.java:247)\\n        at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1110)\\n        at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1188)\\n        at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1168)\\n        at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:54)\\n        at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:47)\\n        at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:30)\\n        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.lambda$asyncShardOperation$0(TransportSingleShardAction.java:110)\\n        at org.elasticsearch.action.ActionRunnable$1.doRun(ActionRunnable.java:45)\\n        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773)\\n        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)\\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n        at java.base/java.lang.Thread.run(Thread.java:830)\\n]]\\nsql&gt;'},\n",
       " {'text_field': 'Pete,\\nUnfortunately, I think you\\'re hitting a documented limitation of our SQL support:\\n\\nUsing aggregation functions on top of scalar functions\\nAggregation functions like  MIN ,  MAX , etc. can only be used directly on fields, and so queries like  SELECT MAX(abs(age)) FROM test  are not possible.\\n\\nFor what it\\'s worth, I popped the following documents into an index called myindex:\\n{\\n    \"key\": \"value\",\\n    \"length\": 2,\\n    \"width\": 3\\n}\\n{\\n    \"key\": \"value\",\\n    \"length\": 5,\\n    \"width\": 7\\n}\\n\\nAnd used the Elasticsearch query DSL to get the sum of areas using the following query:\\nGET myindex/_search\\n{\\n  \"query\": {\\n    \"match_all\": {}\\n  },\\n  \"aggs\": {\\n    \"keys\": {\\n      \"terms\": {\\n        \"field\": \"key.keyword\"\\n      }, \\n      \"aggs\": {\\n        \"areas\": {\\n          \"sum\": {\\n            \"script\": \"doc[\\'length\\'].value * doc[\\'width\\'].value\"\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"size\": 0\\n}\\n\\nResult:\\n{\\n  [...],\\n  \"aggregations\" : {\\n    \"keys\" : {\\n      \"doc_count_error_upper_bound\" : 0,\\n      \"sum_other_doc_count\" : 0,\\n      \"buckets\" : [\\n        {\\n          \"key\" : \"value\",\\n          \"doc_count\" : 2,\\n          \"areas\" : {\\n            \"value\" : 41.0\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}\\n\\nI wish I had been able to find a SQL solution for you.\\n-William'},\n",
       " {'text_field': 'Hi Team,\\nI\\'m using type for separating the logs category,  but in the logs file I am getting error (warning) message like below.\\n[2019-10-22T14:48:01,104][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: thetypeevent field won\\'t be used to determine the document _type {:es_version=&gt;7}\\nThe document of logstash says to use the type for apache and other things.\\n_processing_apache_logs\\nBelow is the config snippet.\\ngrep \\'replace =&gt; { \"type\"\\' /etc/logstash/conf.d/logstash.conf\\n  mutate {  replace =&gt; { \"type\" =&gt; \"nginx_access\" }}\\n  mutate {  replace =&gt; { \"type\" =&gt; \"apache_access\" }}\\n  mutate {  replace =&gt; { \"type\" =&gt; \"sm_access\" } }\\n  mutate {  replace =&gt; { \"type\" =&gt; \"smps_logs\" }}\\n  mutate {  replace =&gt; { \"type\" =&gt; \"apigee_logs\" }}\\n\\nLet me know the correct way to categorize the logs.\\nThanks'},\n",
       " {'text_field': \"\\n\\n\\n Senthil_ak:\\n\\nI'm using type for separating the logs category\\n\\n\\nIf you are using it to separate the logstash processing you can continue to do so. If you want to suppress the warning then just rename it docType or something else.\\nIf you want to use type to separate the documents in Elasticsearch into different mapping types then you will not be able to do that in the future, because mapping types are going away.\"},\n",
       " {'text_field': 'In ES 7.4, I have created a simple index with default mapping.\\nThere is a string field \"authors\". I have indexed a doc where \"authors\" is \"Benjamin\".\\nNow I use this query:\\nGET my_index/_search\\n{\\n  \"query\": {\\n    \"fuzzy\": {\\n      \"authors\": {\\n        \"value\":\"&lt;testvalue&gt;\",\\n        \"fuzziness\": 2\\n      }\\n    }\\n  }\\n}\\n\\nI get a match for the following &lt;testvalue&gt;:\\n\\nBenjaman\\nenjaman\\nBenjamni\\n\\nenjamni (!)\\n\\nBut I got NO match for Banjaman!\\nHow does that match with the principle of Levenshtein distance?\\nBanjaman has a distance of 2, just like e.g. enjamni, so it is not logical.\\nBy the way, transposition as in -&gt; ni is not \"pure\" Levenshein distance, but a derived concept (\"Damerau-Levenshtein distance\"), this confused me a bit at first.'},\n",
       " {'text_field': 'The fuzzy query is a term-level query, which means it does not analyze the query terms. As a result, this query is case sensitive.\\nThe author Benjamin, when indexed using the default mapping does get analyzed, with the standard analyzer. As a result, what gets indexed is the all-lower case term benjamin.\\nThe term Banjaman differs 3 characters from benjamin, because of the upper/lower case B. As a result, the fuzzy query with a fuzziness of 2 returns no hits.\\nHow to solve this? Use the match query with fuziness instead. The match query does analyze the query terms, and gives you case insensitivity:\\nGET my_index/_search\\n{\\n  \"query\": {\\n    \"match\": {\\n      \"authors\":{\\n        \"query\": \"Banjaman\",\\n        \"fuzziness\": 2\\n      }\\n    }\\n  }\\n}\\n'},\n",
       " {'text_field': 'I wanting to query one term across multiple indexes, types, and fields. I\\'ve been reading and experimenting with no working solution... yet.\\nIt\\'s time to ask.\\nIt works until I un-comment the second match clause. Thinking I need an OR somewhere, don\\'t know if this is even possible. The \"post\" type is for the wordpress index, the \"oa_basic\" type is for the other three indexes. post_content and content fields map to respective types and indexes. Hope this explanation makes sense...\\nWhat have I missed or where should I be reading? Thank-you\\nMy php is code is as follows;\\nq = _GET[\\'q\\'];\\n$params = array();\\n$params[\\'size\\'] = 50;\\n$params[\\'index\\'] = \\'oa-assn-20oct2019,oa-prtnr-20oct2019,oa-mastr-21oct2019,oa-1382185244wordpress-post-1\\';\\n$params[\\'type\\'] = \\'oa_basic,post\\';\\n$params[\\'body\\'][\\'query\\'][\\'bool\\'][\\'should\\'][\\'match\\'][\\'content\\'] = $q;\\n//    $params[\\'body\\'][\\'query\\'][\\'bool\\'][\\'should\\'][\\'match\\'][\\'post_content\\'] = $q;\\n}\\nprint_r(json_encode($params[\\'body\\']));\\n$response = $es-&gt;search($params);'},\n",
       " {'text_field': \"I answered this myself.  A couple of issues with the code;\\n\\nphp would have been overwriting the contents of the first ['body'] array element with the second. just the way php works...\\nI built the query in two ways; one setting array elements directly and the second where I build the array as the query clauses required. (see query code below)\\nI also needed to add an AND to remove the trashed wordpress posts, you can see that with the 'must_not' syntax (again, see query below)\\n\\nSo there you have it. The query gets results across fields, types, and indexes. And the relevancy score sets the order across these three... all good. Thanks for your eyes!\\nif(isset($_GET['q'])) {\\n    $q = $_GET['q'];\\n    }\\nif ($q != ''){\\n\\n// Prepare to fetch the content of the oa* indexes\\n$params = array();\\n$params['size'] = 50;\\n$params['index'] = 'oa-assn-20oct2019,oa-prtnr-20oct2019,oa-mastr-21oct2019,oa-1382185244wordpress-post-1';\\n$params['type'] = 'oa_basic,post';\\n$params['body'] = array('query' =&gt; array(\\n'bool' =&gt; array(\\n'should' =&gt; array(\\narray('match' =&gt; array('content' =&gt; $q)),\\narray('match' =&gt; array('post_content' =&gt; $q))),\\n'must_not' =&gt; array(\\narray('match' =&gt; array('post_status' =&gt; 'trash')))\\n)\\n)\\n);\\n}\"},\n",
       " {'text_field': 'I have an XML doc being sent to a logstash server/ELK server.\\nThis is the portion of the xml I\\'m trying to work with:\\n\\\\&lt;row refno=\"1\" percent=\"100\"&gt;\\n\\\\&lt;col&gt;OMXEOC0\\\\&lt;/col&gt;\\\\&lt;col&gt;100\\\\&lt;/col&gt;\\\\&lt;col&gt;00E4\\\\&lt;/col&gt;\\n\\\\&lt;/row&gt;\\n\\\\&lt;row refno=\"2\" percent=\"100\"&gt;\\n\\\\&lt;col&gt;PWXLST02\\\\&lt;/col&gt;\\\\&lt;col&gt;100\\\\&lt;/col&gt;\\\\&lt;col&gt;00D3\\\\&lt;/col&gt;\\n\\\\&lt;/row&gt;\\n\\\\&lt;row refno=\"3\" percent=\"100\"&gt;\\n\\\\&lt;col&gt;OMXEDSST\\\\&lt;/col&gt;\\\\&lt;col&gt;100\\\\&lt;/col&gt;\\\\&lt;col&gt;0097\\\\&lt;/col&gt;\\n\\\\&lt;/row&gt;\\n\\\\&lt;row refno=\"4\" percent=\"100\"&gt;\\n\\\\&lt;col&gt;BHA5947\\\\&lt;/col&gt;\\\\&lt;col&gt;100\\\\&lt;/col&gt;\\\\&lt;col&gt;026C\\\\&lt;/col&gt;\\n\\\\&lt;/row&gt;\\n\\\\&lt;row refno=\"5\" percent=\"100\"&gt;\\n\\\\&lt;col&gt;MAN3333G\\\\&lt;/col&gt;\\\\&lt;col&gt;100\\\\&lt;/col&gt;\\\\&lt;col&gt;00F7\\\\&lt;/col&gt;\\n\\\\&lt;/row&gt;\\n\\n^^^I had to use \\\\ before each tag so that it wasn\\'t read as actual XML^^^\\nWhat I want to do is split this single event into mutiple events each containing the information within each &lt;row&gt; tag (all the info in the row tag as well as teh embedded &lt;col&gt; tags).\\nSo all of this would be in it\\'s own separate message/event\\n\\\\&lt;row refno=\"5\" percent=\"100\"&gt;\\n\\\\&lt;col&gt;MAN3333G\\\\&lt;/col&gt;\\\\&lt;col&gt;100\\\\&lt;/col&gt;\\\\&lt;col&gt;00F7\\\\&lt;/col&gt;\\n\\\\&lt;/row&gt;\\n\\nThis is my current code (which doens\\'t work):\\nsplit {\\n      field =&gt; \"[ddsml][report][row]\"\\n}\\nxml {\\n      source =&gt; \"message\"\\n      target =&gt; \"parsed\"\\n      xpath =&gt; [\\n              \"/ddsml/report/row[1]/col[1]/text()\",\"Job\",\\n              \"/ddsml/report/row[1]/@percent\",\"Percent_Delay\"\\n      ]\\n}\\n\\nAny idea how I can do that? I\\'ve asked before but the guy wasn\\'t able to help. I\\'ve been working on this for a couple days now. I know it\\'s possible. I\\'ve read everything about the split filter but that doesn\\'t give any examples for XML.'},\n",
       " {'text_field': '\\n\\n\\n Jim_Thunder:\\n\\nHowever, when my target option is set to \"parsed\". I get two different fields, one called \"parsed.report\"\\n\\n\\nI think you will find it is [parsed][report], so you want to split [parsed][report][row]'},\n",
       " {'text_field': 'Hello Elasticsearch Team,\\nIn our use case, we want to get only document IDs of the matched documents as a result of search API /_search.\\nBut when I call search API with a query including \"sort\" field, the response contains unnecessary hits.hits.sort field as follows.\\n{\\n  \"took\": 6,\\n  \"timed_out\": false,\\n  \"_shards\": {\\n    \"total\": 6,\\n    \"successful\": 6,\\n    \"skipped\": 0,\\n    \"failed\": 0\\n  },\\n  \"hits\": {\\n    \"total\": {\\n      \"value\": 1,\\n      \"relation\": \"eq\"\\n    },\\n    \"max_score\": null,\\n    \"hits\": [\\n      {\\n        \"_index\": \"example1\",\\n        \"_type\": \"_doc\",\\n        \"_id\": \"12345\",\\n        \"_score\": null,\\n        \"sort\": [\\n          \"Unnecessary keyword value\",\\n          \"12345\"\\n        ]\\n      }\\n    ]\\n  }\\n}\\n\\n\"_source\":false was ignored for the hits.hits.sort field.\\nIn our use case, the keyword field for sort may contain long keyword. So it consumes Java heap space uselessly.\\nCan I remove hits.hits.sort field itself or the keyword text in the field by any option?\\nThanks.'},\n",
       " {'text_field': 'May be but the filtering will not happen early though. So big objects will still being built in memory.'},\n",
       " {'text_field': 'Hi,\\nI have configured elasticsearch, logstash and kibana in Server Cent OS which IP is (10.200.14.36) and both working perfectly.\\nAlso, with filebeat in another Client Cent OS machine, I can able to get index and logs into Server Cent OS machine.\\nI have setup winlogbeat 7.4 in windows 10 and want to ship logs to elasticsearch in Server Cent OS. However, I am receiving \"No indices match pattern \" error in kibana as well in kibana it enlisted winlogbeat in index pattern.\\nPlease find attached screenshots for more clarification....\\n\\nScreenshot_3.png724×377 27.4 KB\\n \\nFollowing is configuration of my winlogbeat:\\n#======================= Winlogbeat specific options ===========================\\nhttps://go.es.io/WinlogbeatConfig\\nwinlogbeat.event_logs:\\n\\n\\nname: Application\\nignore_older: 72h\\n\\n\\nname: System\\n\\n\\nname: Security\\nprocessors:\\n\\nscript:\\nlang: javascript\\nid: security\\nfile: ${path.home}/module/security/config/winlogbeat-security.js\\n\\n\\n\\nname: Microsoft-Windows-Sysmon/Operational\\nprocessors:\\n\\nscript:\\nlang: javascript\\nid: sysmon\\nfile: ${path.home}/module/sysmon/config/winlogbeat-sysmon.js\\n\\n\\n\\n#==================== Elasticsearch template settings ==========================\\nsetup.template.settings:\\nindex.number_of_shards: 1\\n#index.codec: best_compression\\n#_source.enabled: false\\n#================================ General =====================================\\nThe name of the shipper that publishes the network data. It can be used to group\\nall the transactions sent by a single shipper in the web interface.\\n#name:\\nThe tags of the shipper are included in their own field with each\\ntransaction published.\\n#tags: [\"service-X\", \"web-tier\"]\\nOptional fields that you can specify to add additional information to the\\noutput.\\n#fields:\\nenv: staging\\n#============================== Dashboards =====================================\\nThese settings control loading the sample dashboards to the Kibana index. Loading\\nthe dashboards is disabled by default and can be enabled either by setting the\\noptions here or by using the setup command.\\nsetup.dashboards.enabled: true\\nThe URL from where to download the dashboards archive. By default this URL\\nhas a value which is computed based on the Beat name and version. For released\\nversions, this URL points to the dashboard archive on the artifacts.elastic.co\\n\\nwebsite.\\n#setup.dashboards.url:\\n#============================== Kibana =====================================\\nStarting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.\\nThis requires a Kibana endpoint configuration.\\nsetup.kibana:\\nKibana Host\\nScheme and port can be left out and will be set to the default (http and 5601)\\nIn case you specify and additional path, the scheme is required: http://localhost:5601/path\\n\\nIPv6 addresses should always be defined as: https://[2001:db8::1]:5601\\n#host: \"localhost:5601\"\\nhost: \"10.200.14.36:5601\"\\nKibana Space ID\\nID of the Kibana Space into which the dashboards should be loaded. By default,\\nthe Default Space will be used.\\n#space.id:\\n#================================ Outputs =====================================\\nConfigure what output to use when sending the data collected by the beat.\\n#-------------------------- Elasticsearch output ------------------------------\\noutput.elasticsearch:\\nArray of hosts to connect to.\\n#hosts: [\"localhost:9200\"]\\nhosts: [\"10.200.14.36:9200\"]\\nOptional protocol and basic auth credentials.\\n#protocol: \"https\"\\n#username: \"elastic\"\\n#password: \"changeme\"\\n#----------------------------- Logstash output --------------------------------\\n#output.logstash:\\nThe Logstash hosts\\n#hosts: [\"localhost:5044\"]\\nOptional SSL. By default is off.\\nList of root certificates for HTTPS server verifications\\n#ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"]\\nCertificate for SSL client authentication\\n#ssl.certificate: \"/etc/pki/client/cert.pem\"\\nClient Certificate Key\\n#ssl.key: \"/etc/pki/client/cert.key\"\\n#================================ Processors =========================\\nConfigure processors to enhance or manipulate events generated by the beat.\\nprocessors:\\n\\nadd_host_metadata: ~\\nadd_cloud_metadata: ~\\n\\n#================================ Logging =================================\\nSets log level. The default log level is info.\\nAvailable log levels are: error, warning, info, debug\\n#logging.level: debug\\nAt debug level, you can selectively enable logging only for some components.\\nTo enable all selectors use [\"*\"]. Examples of other selectors are \"beat\",\\n\"publish\", \"service\".\\n#logging.selectors: [\"*\"]\\nPlease help me..try to resolve it but don\\'t understand the actual problem.....'},\n",
       " {'text_field': 'Hi Christian,\\nIn firewall port 9200 is already open, same for CentOS and Windows firewall...\\nHowever, Problem resolved, I need to add \"http.host: 10.200.14.36  &amp;  http.port: 9200\" in my elasticsearch YAML file which can receive winloagbeat  logs input (due to it connect to elasticsearch with http://10.200.14.36:9200 as per the logs...)\\nThank you for your help'},\n",
       " {'text_field': 'Hi there,\\nThere is an index, each element of which contains:\\n\\ngeneral information about a player\\'s session in tutorial\\na few arrays with information for each tutorial stage\\n\\nIt looks like this:\\n&gt; id:206 duration:35 created_at:Oct 23, 2019 @ 16:55:11.000 build:0.2.3.889 player_id:450 is_tester:true stages:{ \"id\": 1475, \"stage_id\": 0, \"duration\": 11, \"completed\": true }, { \"id\": 1476, \"stage_id\": 1, \"duration\": 6, \"completed\": true }, { \"id\": 1477, \"stage_id\": 2, \"duration\": 2, \"completed\": true }, { \"id\": 1478, \"stage_id\": 3, \"duration\": 2, \"completed\": true }, { \"id\": 1479, \"stage_id\": 4, \"duration\": 3, \"completed\": true }, { \"id\": 1480, \"stage_id\": 5, \"duration\": 3, \"completed\": true }, { \"id\": 1481, \"stage_id\": 6, \"duration\": 4, \"completed\": true } _id:Hdrq-G0BSOdvtuXl5PqJ _type:data _index:tutorial_data _score: -\\n\\n\\n(Or same in JSON)\\n{\\n  \"_index\": \"tutorial_data\",\\n  \"_type\": \"data\",\\n  \"_id\": \"Sdoi-W0BSOdvtuXlEfvi\",\\n  \"_version\": 1,\\n  \"_score\": null,\\n  \"_source\": {\\n    \"id\": 207,\\n    \"duration\": 28,\\n    \"created_at\": \"2019-10-23T14:26:28.000+00:00\",\\n    \"build\": \"0.2.3.889\",\\n    \"player_id\": 450,\\n    \"is_tester\": true,\\n    \"stages\": [\\n      {\\n        \"id\": 1482,\\n        \"stage_id\": 0,\\n        \"duration\": 4,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1483,\\n        \"stage_id\": 1,\\n        \"duration\": 5,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1484,\\n        \"stage_id\": 2,\\n        \"duration\": 2,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1485,\\n        \"stage_id\": 3,\\n        \"duration\": 7,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1486,\\n        \"stage_id\": 4,\\n        \"duration\": 3,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1487,\\n        \"stage_id\": 5,\\n        \"duration\": 6,\\n        \"completed\": true\\n      }\\n    ]\\n  },\\n  \"fields\": {\\n    \"created_at\": [\\n      \"2019-10-23T14:26:28.000Z\"\\n    ]\\n  },\\n  \"sort\": [\\n    1571840788000\\n  ]\\n}\\n\\n\\nThe task is to create a table/ a diagram of an average time spent on each stage of the tutorial.\\nWhen I aggregate through the \"stages.duration\" field (not just the \"duration\" !) and split the table by \"stages.stage_id\", it calculates an average time basing on ALL stages in sessions that include a record about the given stages.stages_id (e.g., a stage №2).\\n\\n\\nThis is my request\\n{\\n  \"aggs\": {\\n\"3\": {\\n  \"terms\": {\\n    \"field\": \"stages.stage_id\",\\n    \"order\": {\\n      \"_key\": \"desc\"\\n    },\\n    \"size\": 9\\n  },\\n  \"aggs\": {\\n    \"1\": {\\n      \"avg\": {\\n        \"field\": \"stages.duration\"\\n      }\\n    }\\n  }\\n}\\n  },\\n  \"size\": 0,\\n  \"_source\": {\\n\"excludes\": []\\n  },\\n  \"stored_fields\": [\\n\"*\"\\n  ],\\n  \"script_fields\": {},\\n  \"docvalue_fields\": [\\n{\\n  \"field\": \"created_at\",\\n  \"format\": \"date_time\"\\n}\\n  ],\\n  \"query\": {\\n\"bool\": {\\n  \"must\": [\\n    {\\n      \"range\": {\\n        \"created_at\": {\\n          \"format\": \"strict_date_optional_time\",\\n          \"gte\": \"2019-10-11T20:30:00.000Z\",\\n          \"lte\": \"2019-10-24T08:10:54.837Z\"\\n        }\\n      }\\n    }\\n  ],\\n  \"filter\": [\\n    {\\n      \"match_all\": {}\\n    }\\n  ],\\n  \"should\": [],\\n  \"must_not\": []\\n}\\n  }\\n}\\n\\n\\n\\n\\nResponse\\n{\\n  \"took\": 4,\\n  \"timed_out\": false,\\n  \"_shards\": {\\n    \"total\": 1,\\n    \"successful\": 1,\\n    \"skipped\": 0,\\n    \"failed\": 0\\n  },\\n  \"hits\": {\\n    \"total\": 203,\\n    \"max_score\": null,\\n    \"hits\": []\\n  },\\n  \"aggregations\": {\\n    \"3\": {\\n      \"doc_count_error_upper_bound\": 0,\\n      \"sum_other_doc_count\": 0,\\n      \"buckets\": [\\n        {\\n          \"1\": {\\n            \"value\": 8.141304347826088\\n          },\\n          \"key\": 8,\\n          \"doc_count\": 147\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.164634146341463\\n          },\\n          \"key\": 7,\\n          \"doc_count\": 150\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.108289768483944\\n          },\\n          \"key\": 6,\\n          \"doc_count\": 154\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.036390101892286\\n          },\\n          \"key\": 5,\\n          \"doc_count\": 160\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.02030456852792\\n          },\\n          \"key\": 4,\\n          \"doc_count\": 161\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 7.935256861365236\\n          },\\n          \"key\": 3,\\n          \"doc_count\": 172\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.189151599443672\\n          },\\n          \"key\": 2,\\n          \"doc_count\": 179\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.189151599443672\\n          },\\n          \"key\": 1,\\n          \"doc_count\": 179\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 9.656871218668972\\n          },\\n          \"key\": 0,\\n          \"doc_count\": 160\\n        }\\n      ]\\n    }\\n  },\\n  \"status\": 200\\n}\\n\\n\\nSo how can I \"pick out\" and aggregate through the data, e.g. just for the 2nd stage in all appropriate index elements?'},\n",
       " {'text_field': \"Hi @Anne_Kim,\\nunfortunately this is currently not possible in Kibana because nested fields are not supported: https://github.com/elastic/kibana/issues/1084\\nFor Kibana, stages.duration and stages.stage_id look like two separate arrays that don't have any relationship, so it can't associate one value from one array with a single value in the other array.\\nFor your use case it makes sense to denormalize and ingest one document per individual stage - then you can create the diagram exactly like you described it.\"},\n",
       " {'text_field': \"Hi guys,\\nJust wondering how you guys will find out the reason if one query does not return the result as expected? I know you can use 'explain' api to find out how the match score is calculated, but how can I use this if that document does not show up in the query?\\nI have tried to use min_score but it seems like it is for a different purpose.\\nPlease help to point out where it is documented, and if it is not in place, may I request to add this somewhere in the online documentation? I thought it would be a general question for everybody.\"},\n",
       " {'text_field': 'You can use the Explain API by hitting _explain on a specific document. That will tell you exactly why that specific document does not match or has a much lower score that you may have expected. The documentation has an example:\\nGET /twitter/_explain/0\\n{\\n      \"query\" : {\\n        \"match\" : { \"message\" : \"elasticsearch\" }\\n      }\\n}\\n\\nThat example will explain exactly what would happen for that given query on document 0 in the twitter index.'},\n",
       " {'text_field': 'Есть индекс, каждый элемент которого содержит следующую информацию:\\n\\nобщая информация о прохождении туториала игры\\nнесколько массивов с информацией о прохождении каждого из этапов туториала\\n\\nОдин элемент выглядит так:\\nid:206 duration:35 created_at:Oct 23, 2019 @ 16:55:11.000 build:0.2.3.889 player_id:450 is_tester:true stages:{ \"id\": 1475, \"stage_id\": 0, \"duration\": 11, \"completed\": true }, { \"id\": 1476, \"stage_id\": 1, \"duration\": 6, \"completed\": true }, { \"id\": 1477, \"stage_id\": 2, \"duration\": 2, \"completed\": true }, { \"id\": 1478, \"stage_id\": 3, \"duration\": 2, \"completed\": true }, { \"id\": 1479, \"stage_id\": 4, \"duration\": 3, \"completed\": true }, { \"id\": 1480, \"stage_id\": 5, \"duration\": 3, \"completed\": true }, { \"id\": 1481, \"stage_id\": 6, \"duration\": 4, \"completed\": true } _id:Hdrq-G0BSOdvtuXl5PqJ _type:data _index:tutorial_data _score: -\\n\\n\\n(Или то же самое на JSON)\\n{\\n  \"_index\": \"tutorial_data\",\\n  \"_type\": \"data\",\\n  \"_id\": \"Sdoi-W0BSOdvtuXlEfvi\",\\n  \"_version\": 1,\\n  \"_score\": null,\\n  \"_source\": {\\n    \"id\": 207,\\n    \"duration\": 28,\\n    \"created_at\": \"2019-10-23T14:26:28.000+00:00\",\\n    \"build\": \"0.2.3.889\",\\n    \"player_id\": 450,\\n    \"is_tester\": true,\\n    \"stages\": [\\n      {\\n        \"id\": 1482,\\n        \"stage_id\": 0,\\n        \"duration\": 4,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1483,\\n        \"stage_id\": 1,\\n        \"duration\": 5,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1484,\\n        \"stage_id\": 2,\\n        \"duration\": 2,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1485,\\n        \"stage_id\": 3,\\n        \"duration\": 7,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1486,\\n        \"stage_id\": 4,\\n        \"duration\": 3,\\n        \"completed\": true\\n      },\\n      {\\n        \"id\": 1487,\\n        \"stage_id\": 5,\\n        \"duration\": 6,\\n        \"completed\": true\\n      }\\n    ]\\n  },\\n  \"fields\": {\\n    \"created_at\": [\\n      \"2019-10-23T14:26:28.000Z\"\\n    ]\\n  },\\n  \"sort\": [\\n    1571840788000\\n  ]\\n}\\n\\n\\nЗадача - визуализировать (пока в виде таблицы, но потом и в виде диаграммы) среднее время прохождения каждого из этапов туториала.\\nКогда я агрегируюсь по  \"stages.duration\" (не по просто \"duration\" !) и далее разбиваю строки таблицы по \"stages.stage_id\", то высчитывается среднее время на основе данных со ВСЕХ этапов сессий, которые содержат данный stages.stages_id (например, сессии с этапом №2).\\n\\n\\nТак выглядит запрос\\n{\\n  \"aggs\": {\\n\"3\": {\\n  \"terms\": {\\n    \"field\": \"stages.stage_id\",\\n    \"order\": {\\n      \"_key\": \"desc\"\\n    },\\n    \"size\": 9\\n  },\\n  \"aggs\": {\\n    \"1\": {\\n      \"avg\": {\\n        \"field\": \"stages.duration\"\\n      }\\n    }\\n  }\\n}\\n  },\\n  \"size\": 0,\\n  \"_source\": {\\n\"excludes\": []\\n  },\\n  \"stored_fields\": [\\n\"*\"\\n  ],\\n  \"script_fields\": {},\\n  \"docvalue_fields\": [\\n{\\n  \"field\": \"created_at\",\\n  \"format\": \"date_time\"\\n}\\n  ],\\n  \"query\": {\\n\"bool\": {\\n  \"must\": [\\n    {\\n      \"range\": {\\n        \"created_at\": {\\n          \"format\": \"strict_date_optional_time\",\\n          \"gte\": \"2019-10-11T20:30:00.000Z\",\\n          \"lte\": \"2019-10-24T08:10:54.837Z\"\\n        }\\n      }\\n    }\\n  ],\\n  \"filter\": [\\n    {\\n      \"match_all\": {}\\n    }\\n  ],\\n  \"should\": [],\\n  \"must_not\": []\\n}\\n  }\\n}\\n\\n\\n\\n\\nТак выглядит ответ\\n{\\n  \"took\": 4,\\n  \"timed_out\": false,\\n  \"_shards\": {\\n    \"total\": 1,\\n    \"successful\": 1,\\n    \"skipped\": 0,\\n    \"failed\": 0\\n  },\\n  \"hits\": {\\n    \"total\": 203,\\n    \"max_score\": null,\\n    \"hits\": []\\n  },\\n  \"aggregations\": {\\n    \"3\": {\\n      \"doc_count_error_upper_bound\": 0,\\n      \"sum_other_doc_count\": 0,\\n      \"buckets\": [\\n        {\\n          \"1\": {\\n            \"value\": 8.141304347826088\\n          },\\n          \"key\": 8,\\n          \"doc_count\": 147\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.164634146341463\\n          },\\n          \"key\": 7,\\n          \"doc_count\": 150\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.108289768483944\\n          },\\n          \"key\": 6,\\n          \"doc_count\": 154\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.036390101892286\\n          },\\n          \"key\": 5,\\n          \"doc_count\": 160\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.02030456852792\\n          },\\n          \"key\": 4,\\n          \"doc_count\": 161\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 7.935256861365236\\n          },\\n          \"key\": 3,\\n          \"doc_count\": 172\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.189151599443672\\n          },\\n          \"key\": 2,\\n          \"doc_count\": 179\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 8.189151599443672\\n          },\\n          \"key\": 1,\\n          \"doc_count\": 179\\n        },\\n        {\\n          \"1\": {\\n            \"value\": 9.656871218668972\\n          },\\n          \"key\": 0,\\n          \"doc_count\": 160\\n        }\\n      ]\\n    }\\n  },\\n  \"status\": 200\\n}\\n\\n\\nКак в итоге можно \"выцепить\" информацию по каждому из этапов туториала в отдельности?'},\n",
       " {'text_field': '\\n\\n\\n Anne_Kim:\\n\\nКак в итоге можно \"выцепить\" информацию по каждому из этапов туториала в отдельности?\\n\\n\\nВ elasticsearch это делается с помощью объектов с типом nested. К сожалению, kibana c такими объектами не работает.\\nЕсли с ними надо работать в Kibana, то надо каждый этап индексировать, как отдельную запись, либо создавать отдельные поля для каждого этапа. То есть duration_1, completed_1, duration_2, completed_2 и т.д.'},\n",
       " {'text_field': \"Hi There,\\nI upgraded the whole ELK stack from 7.3.x to 7.4.0 yesterday. After upgrade, I am unable to open Kibana from the browser after authentication. It goes into a loop refreshing the page with errors all the time. Below is the screenshot of the error that I get.\\n\\nimage.png1374×790 51.3 KB\\n\\nQuoiting from this thread: Kibana did not load properly. Check the server output for more information - Kibana not working\\nIn the above thread, @Brandon_Kobel had asked the poster to execute the below command and check the downloaded file size.\\ncurl -so /dev/null http://*********:5601/built_assets/dlls/vendors.bundle.dll.js -w '%{size_download}'\\nI executed the same curl command, and I got the file size as 22868674. But the actual file size is of the vendors.bundle.dll.js is 22868683.\\nI am unable to download the whole file even though I am executing the curl command on the same host where kibana is installed. But Kibana is setup to run on the private IP address of the host rather than the localhost.\\nPlease do help me out in resolving this.\\nThank You.\\nRegards,\\nZulfiqar\"},\n",
       " {'text_field': '\\n\\n\\n zulfiqar4891:\\n\\n\\nkibana.defaultAppId: \"tapro2\"\\n\\n\\n\\nIf you remove the kibana.defaultAppId setting in your kibana.yml,  you should see this problem fixed.\\nThis setting should only really be set to home, discover, dashboard or visualize. When this is set to  a non-existent app, it\\'s causing Kibana to go into an endless loop in 7.4.x. I created kibana.defaultAppId non-existent application, endless redirect loop · Issue #49495 · elastic/kibana · GitHub to track us fixing this behavior.'},\n",
       " {'text_field': 'wonder if there are any way to call ES SQL API, say like\\nhttp://localhost:9200/_sql?format=json&amp;source={“query”: “SELECT 1” }\\nsimiler to\\nhttp://localhost:9200//hockey/_search?q=johnny\\nThis gives error. \"source and source_content_type parameters are required\"\\nNB:\\nthe following curl script works fine.\\ncurl -X GET \"localhost:9200/_sql\" -H \\'Content-Type: application/json\\' -d\\'SELECT 1\\'\\nminimal script to create the index\\nPUT hockey/_bulk?refresh\\n{\"index\":{\"_id\":1}}\\n{\"first\":\"johnny\",\"last\":\"gaudreau\",\"goals\":[9,27,1],\"assists\":[17,46,0],\"gp\":[26,82,1],\"born\":\"1993/08/13\"}'},\n",
       " {'text_field': '@madhusoodanan yes, this is possible:\\nhttp://localhost:9200/_sql?source={\"query\":\"SELECT 1\"}&amp;source_content_type=application/json&amp;format=txt\\n'},\n",
       " {'text_field': 'Hello,\\nis it possible to have a complex type (like a geopoint) dinamically recognized  using the dynamic_templates section of a template/index mapping?\\nSort of:\\nPUT test_loc/doc/1\\n{\\n  \"position\" : [-64, -19.4]\\n}\\n\\nresulting in position field recognized as a geopoint, instead of float.'},\n",
       " {'text_field': 'Sorry it took so long.\\nHere is how you can fix it:\\nDELETE test-loc\\nPUT test-loc\\n{\\n  \"mappings\": {\\n    \"dynamic_templates\": [\\n      {\\n        \"geo\": {\\n          \"match_mapping_type\": \"object\",\\n          \"match\": \"*location\",\\n          \"mapping\": {\\n            \"type\": \"geo_point\"\\n          }\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nPUT test-loc/_doc/1\\n{\\n  \"start_location\": {\\n    \"lon\": 4.8995,\\n    \"lat\": 52.3824\\n  }\\n}\\n\\nGET test-loc/_mapping\\n\\nTested on 7.5.'},\n",
       " {'text_field': 'We have 4 node elasticsearch ecs cluster. now problem is two nodes keep restarting elastic search containers and other two stable containers.\\nbut unable to get masternode.\\nbash-4.1# cat /usr/share/elasticsearch/config/elasticsearch.yml\\nscript.inline: true\\ncloud.aws.access_key: AKIAJXFMPDJ4GTC63CXQ\\ncloud.aws.secret_key: 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\\ncloud.aws.region: us-east-1\\nrepositories.s3.bucket: \"mpt-elk-snapshots\"\\nnode.master: true\\nnode.data: true\\nxpack.security.enabled: false\\ncluster.name: cluster-prod\\nnetwork.host: 0.0.0.0\\nnetwork.publish_host: ec2:privateIp\\ndiscovery.type: ec2\\ndiscovery.ec2.any_group: false\\ndiscovery.ec2.groups: sg-*****\\ndiscovery.zen.hosts_provider: ec2\\ndiscovery.zen.minimum_master_nodes: 3\\n#discovery.zen.ping.multicast.enabled: false\\nworking two nodes getting this output on health.\\ncurl -XGET \\'localhost:9200/_cluster/health?pretty\\'\\n{\\n\"error\" : {\\n\"root_cause\" : [\\n{\\n\"type\" : \"master_not_discovered_exception\",\\n\"reason\" : null\\n}\\n],\\n\"type\" : \"master_not_discovered_exception\",\\n\"reason\" : null\\n},\\n\"status\" : 503\\n}\\nplease help me to resolve this issue\\ncurl -XGET \\'http://localhost:9200\\'\\n{\\n\"name\" : \"casXLJFLD\",\\n\"cluster_name\" : \"prod-cluster\",\\n\"cluster_uuid\" : \"QbooN9HaTnSRJClksjdf4Zy1aw\",\\n\"version\" : {\\n\"number\" : \"5.5.1\",\\n\"build_hash\" : \"23c4swdo\",\\n\"build_date\" : \"2017-07-18T20:44:24.823Z\",\\n\"build_snapshot\" : false,\\n\"lucene_version\" : \"6.6.0\"\\n},\\n\"tagline\" : \"You Know, for Search\"\\n}'},\n",
       " {'text_field': 'Elasticsearch service with in the docker container and it is taking more time to start service.  As part of ECS setup ELB/ALB will look for health check of container and if container is not health with in the time frame it will automatically stops the container and try to start new container.\\nBefore we have 30 sec internal for 2 times.  If container is not health with 60 secs (1 mins) it stops running container and start a new one.\\nSo health checks are failing all the time due to elastic search service is taking around 2 mins to start.  and containers constantly rebuilding all the time.\\nwe increased health check time in ELB to check 5 times (2 1/2 mins) before stopping container. Issue resolved.'},\n",
       " {'text_field': 'Hi I am trialing ECE and I have setup secure route to a new cluster and that works fine.  I get a access denied when I try and access it from anywhere except the host I allowed.\\nBut I can hit the frc-services-forwarders-services-forwarder (port 9244) on a server that is only a allocator. If I spoof the address for the cluster ie. add the cluster-id.ece-address.local to the allocator ip in the host file I can access the cluster anywhere I do this change.\\nThis raise some security concerns, also that communication looks to be  frc-services-forwarders-services-forwarder http only. Does the poxy terminate the tls connection and its http to the elasticsearch cluster or is the something missing.'},\n",
       " {'text_field': \"\\n\\n\\n robrotheram:\\n\\nSo if I understand correctly Services Forwarder is used for services on that host to send data to other clusters.\\nThe internal cluster transport e.g for shard moving is all done via TLS.\\n\\n\\nThat's exactly right yes\\n\\n\\n\\n robrotheram:\\n\\nDoes the proxy do any TLS termination or is the termination of my connection done at the elastic cluster?\\n\\n\\nThe proxy terminates TLS and has its own internal certification for proxy&lt;-&gt;ES/Kibana communications.\\n\\n\\n\\n robrotheram:\\n\\nIs there any documentation for security hardening ECE, I saw nothing about needing to iptable off 9244?\\n\\n\\nThe list of which ports you should allow (the implication being that all others should be blocked) is here: https://www.elastic.co/guide/en/cloud-enterprise/2.3/ece-prereqs-networking.html\\n(It doesn't mention 9244 because it's intended to be blocked) It would probably be good if we linked to this from https://www.elastic.co/guide/en/cloud-enterprise/2.3/ece-security-considerations.html and also were more explicit on that page that other ports should be blocked (at least inbound), I think?\\nAlex\"},\n",
       " {'text_field': 'I have an entire ELK stack in docker compose but only Kibana and Elasticsearch are shown. Logs show no errors and it seems, that Logstash communicates with the rest of stack. Here\\'s my logstash.yml:\\nxpack.monitoring.elasticsearch.hosts: [ \"http://elasticsearch:9200\" ]\\nxpack.monitoring.enabled: true\\nxpack.monitoring.elasticsearch.username: elastic\\nxpack.monitoring.elasticsearch.password: changeme'},\n",
       " {'text_field': \"So it appears, that thing in Kibana called 'Centralized management' and it's available only in Gold and higher subscriptions. I suggest to rename 'Pipelines' in Kibana UI to 'Centralized pipelines' to avoid any misunderstandings.\"},\n",
       " {'text_field': 'Hello , I\\'ve got a kibana vega vizualization embeded in a webpage, but the message \"This visualization is marked as experimental. Have feedback? Please create an issue in GitHub\" is displayed at the top. I would like to remove this message for cosmetic reasons.\\nHow can I do this ?\\nThanks in advance, Hubert.'},\n",
       " {'text_field': \"Hey @HubertBonisseur, unfortunately this isn't currently possible, I'd recommend creating a feature request to add this possibility.\"},\n",
       " {'text_field': 'Hello all,\\nI keep encountering a CircuitBreakingException on my ES cluster\\nThe following stack trace seems to suggest that the CircuitBreakingException is taking place during a seemingly lightweight call (cluster:monitor/stats). But I have seen similar issues during indices:data/write/bulk calls as well\\n\"org.elasticsearch.transport.RemoteTransportException: [elasticsearch-data01-srv][10.210.146.44:9300][cluster:monitor/stats[n]]\"\\n\"Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [&lt;transport_request&gt;] would be [14592176820/13.5gb], which is larger than the limit of [14280766259/13.2gb], real usage: [14592173056/13.5gb], new bytes reserved: [3764/3.6kb], usages [request=0/0b, fielddata=0/0b, in_flight_requests=3764/3.6kb, accounting=384737042/366.9mb]\",\\n\"at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:342) ~[elasticsearch-7.3.1.jar:7.3.1]\"\\n\\nI am running ES 7.3 using the bundled JDK. My cluster setup is as follows:\\n\\n3 dedicated master nodes (t2.xlarge EC2 instances with 16GB RAM. The only non-default jvm.options on these nodes is -Xms7g -Xmx7g)\\n3 data-only nodes (r4.xlarge EC2 instances with 30.5GB RAM. The only non-default jvm.options on these nodes is -Xms14g -Xmx14g)\\n\\nOther relevant modifications to my elasticsearch.yml file\\n\\nbootstrap.memory_lock: true\\nhttp.max_content_length: 1g\\nindices.memory.index_buffer_size: 30%\\nthread_pool.write.queue_size: -1\\n\\nOther stats:\\n\\nTotal number of indices: 174\\nNumber of replicas: 1\\nTotal shards: 1676 (90% indices are configured to have 5 shards per index. Largest shard is 814.6mb)\\nTotal docs count: 20329064\\nStore size in bytes: 88GB\\n\\nI would really appreciate any insights on what the problem and solution could be and also pointers on how to approach such issues\\nThanks!'},\n",
       " {'text_field': 'You have way too many shards.\\n\\nTotal shards: 1676 (90% indices are configured to have 5 shards per index. Largest shard is 814.6mb)\\n\\nYou should re-index every index that has 5 primary shards down to a single primary shard. That will make the cluster much more stable.\\nIf you do that and the circuit-breaker issue persists, further possible causes can be explored, but the re-sharding needs to happen anyway.'},\n",
       " {'text_field': \"Hello ,\\nI am using kibana 7.4 . Can i change or remove euiSideNavItem euiSideNavItem--trunk labels. I changed a lot of things in /usr/share/kibana/ path but it doesn't work.\\nThanks.\"},\n",
       " {'text_field': \"Hey @Burak_Cayir, unfortunately that won't be possible until we address https://github.com/elastic/kibana/issues/35965.\"},\n",
       " {'text_field': 'Hi everyone,\\nI started studying the operation of Kibana (7.4) now and I have a problem that has not been answered in the many discussions on the subject.\\nThe current time in the filter is 2 hours back.\\n\\nI\\'m running Kibana on my machine and I changed the \"tz\" setting with UTC, I tried to set my time zone but nothing.\\n\\nimg2.JPG721×163 10.4 KB\\n\\nThe change from \"default\" to \"UTC\" solved the problem of the timestamp in the data that were displayed with 2 more hours.\\nHas anyone solved it?\\nThanks'},\n",
       " {'text_field': '@dome which version of Kibana/Elasticsearch are you using? I\\'m currently in the PST timezone with a minus seven hours offset. Using DevTools, I\\'m able to insert two documents, one using my current timezone offset and one using UTC:\\nPOST dome/_doc\\n{\\n  \"message\": \"hello\",\\n  \"@timestamp\": \"2019-10-29T09:22:00.000-0700\"\\n}\\n\\nPOST dome/_doc\\n{\\n  \"message\": \"hello\",\\n  \"@timestamp\": \"2019-10-29T16:22:00.000Z\"\\n}\\n\\nAnd then when I view these documents within Discover, I see both being treated properly by the date-filter and displayed correctly:\\n\\nScreen Shot 2019-10-29 at 9.35.45 AM.png2788×1326 318 KB\\n'},\n",
       " {'text_field': 'Hi experts,\\nWould you please provide some materials or WEB Link for me that include the introduction of grammar of ruby code which apply in logstash script.\\nSuch like this:\\n\\nruby {\\n    code =&gt; \\'event.get(\"message\").scan(/loc[A-Z]{4}/).each {|loc| event.tag(loc)}\\'\\n}\\n\\nruby {\\n event.set(\\'except_fcn_trace\\', event.get(\\'except_fcn_trace\\').delete(\\' \\').scan(/.{16}/).join(\\' \\'))\\n}\\n\\nI just want to know how many event function(.get .set .scan . delete) defined and how to use them in logstash.\\nThank you.'},\n",
       " {'text_field': '.get and .set are part of the logstash event API. .scan and .delete are functions of a Ruby string.'},\n",
       " {'text_field': 'I am unable to visualize a long value using any chart type (line, bar...) in Kibana. Only if I choose \"Count\" do I see my data: Max, Average all empty.\\nMapping\\n{\\n    \"msaperf-all\": {\\n        \"mappings\": {\\n            \"dynamic\": \"false\",\\n            \"properties\": {\\n                \"env\": {\\n                    \"type\": \"keyword\"\\n                },\\n                \"name\": {\\n                    \"type\": \"keyword\"\\n                },\\n                \"port\": {\\n                    \"type\": \"keyword\"\\n                },\\n                \"timeTakenMS\": {\\n                    \"type\": \"long\"\\n                },\\n                \"timestamp\": {\\n                    \"type\": \"date\",\\n                    \"format\": \"date_time||epoch_millis\"\\n                }\\n            }\\n        }\\n    }\\n}\\n\\nVisualization totally empty:\\n\\nimage.png1523×777 39 KB\\n\\nRequest\\n{\\n  \"aggs\": {\\n    \"2\": {\\n      \"date_histogram\": {\\n        \"field\": \"timestamp\",\\n        \"fixed_interval\": \"30m\",\\n        \"time_zone\": \"Europe/Zurich\",\\n        \"min_doc_count\": 1\\n      },\\n      \"aggs\": {\\n        \"1\": {\\n          \"avg\": {\\n            \"field\": \"timeTakenMS\"\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"size\": 0,\\n  \"_source\": {\\n    \"excludes\": []\\n  },\\n  \"stored_fields\": [\\n    \"*\"\\n  ],\\n  \"script_fields\": {},\\n  \"docvalue_fields\": [\\n    {\\n      \"field\": \"timestamp\",\\n      \"format\": \"date_time\"\\n    }\\n  ],\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"match_all\": {}\\n        },\\n        {\\n          \"range\": {\\n            \"timestamp\": {\\n              \"format\": \"strict_date_optional_time\",\\n              \"gte\": \"2019-10-13T22:00:00.000Z\",\\n              \"lte\": \"2019-10-14T21:59:59.999Z\"\\n            }\\n          }\\n        }\\n      ],\\n      \"filter\": [],\\n      \"should\": [],\\n      \"must_not\": []\\n    }\\n  }\\n}\\n\\nResponse\\n{\\n  \"took\": 0,\\n  \"timed_out\": false,\\n  \"_shards\": {\\n    \"total\": 1,\\n    \"successful\": 1,\\n    \"skipped\": 0,\\n    \"failed\": 0\\n  },\\n  \"hits\": {\\n    \"total\": 112,\\n    \"max_score\": null,\\n    \"hits\": []\\n  },\\n  \"aggregations\": {\\n    \"2\": {\\n      \"buckets\": [\\n        {\\n          \"1\": {\\n            \"value\": null\\n          },\\n          \"key_as_string\": \"2019-10-14T06:30:00.000+02:00\",\\n          \"key\": 1571027400000,\\n          \"doc_count\": 16\\n        },\\n        {\\n          \"1\": {\\n            \"value\": null\\n          },\\n          \"key_as_string\": \"2019-10-14T07:00:00.000+02:00\",\\n          \"key\": 1571029200000,\\n          \"doc_count\": 28\\n        },\\n        {\\n          \"1\": {\\n            \"value\": null\\n          },\\n          \"key_as_string\": \"2019-10-14T07:30:00.000+02:00\",\\n          \"key\": 1571031000000,\\n          \"doc_count\": 24\\n        },\\n        {\\n          \"1\": {\\n            \"value\": null\\n          },\\n          \"key_as_string\": \"2019-10-14T08:00:00.000+02:00\",\\n          \"key\": 1571032800000,\\n          \"doc_count\": 24\\n        },\\n        {\\n          \"1\": {\\n            \"value\": null\\n          },\\n          \"key_as_string\": \"2019-10-14T08:30:00.000+02:00\",\\n          \"key\": 1571034600000,\\n          \"doc_count\": 20\\n        }\\n      ]\\n    }\\n  },\\n  \"status\": 200\\n}\\n\\nSample Data\\n{\\n    \"took\": 0,\\n    \"timed_out\": false,\\n    \"_shards\": {\\n        \"total\": 1,\\n        \"successful\": 1,\\n        \"skipped\": 0,\\n        \"failed\": 0\\n    },\\n    \"hits\": {\\n        \"total\": {\\n            \"value\": 112,\\n            \"relation\": \"eq\"\\n        },\\n        \"max_score\": 1.0,\\n        \"hits\": [\\n            {\\n                \"_index\": \"msaperf-all\",\\n                \"_type\": \"_doc\",\\n                \"_id\": \"32SfyG0B7cpF8onKBrWu\",\\n                \"_score\": 1.0,\\n                \"_source\": {\\n                    \"timeTaken\": \"785\",\\n                    \"type\": \"INFO\",\\n                    \"server\": \"app02\",\\n                    \"env\": \"dev\",\\n                    \"port\": \"8082\",\\n                    \"priority\": \"1\",\\n                    \"timestamp\": \"2019-10-14T04:55:22.000Z\",\\n                    \"source\": \"msaperf\",\\n                    \"name\": \"parts\",\\n                    \"app\": \"%{[fields][app]}\"\\n                }\\n            },\\n            {\\n                \"_index\": \"msaperf-all\",\\n                \"_type\": \"_doc\",\\n                \"_id\": \"JGSfyG0B7cpF8onKO8OP\",\\n                \"_score\": 1.0,\\n                \"_source\": {\\n                    \"timeTaken\": \"227\",\\n                    \"type\": \"INFO\",\\n                    \"server\": \"app02\",\\n                    \"env\": \"dev\",\\n                    \"port\": \"8083\",\\n                    \"priority\": \"1\",\\n                    \"timestamp\": \"2019-10-14T04:55:37.000Z\",\\n                    \"source\": \"msaperf\",\\n                    \"name\": \"salesdocs\",\\n                    \"app\": \"%{[fields][app]}\"\\n                }\\n            },\\n            ...'},\n",
       " {'text_field': '\\n\\n\\n cawoodm:\\n\\nAny ideas anyone?\\n\\n\\nYour doc has field timeTaken not timeTakenMS ?'},\n",
       " {'text_field': 'The following function execution throws an error. The same command in bash works like a charm.\\ndef set_kibana_password():\\nheaders = {\"Content-Type\": \"application/json\"}\\npayload = {\"password\": os.getenv(\\'ELASTIC_PASSWORD\\')}\\nresponse = requests.put(\\n    url=\\'https://localhost:9200/_xpack/security/user/kibana/_password\\',\\n    auth=HTTPBasicAuth(\\'elastic\\', os.getenv(\\'ELASTIC_PASSWORD\\')),\\n    cert=\\'/certs/ca/ca.crt\\',\\n    headers=headers,\\n    data=payload\\n)\\nlogging.info(str(response))\\n\\nThe output:\\nDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): localhost:9200\\nTraceback (most recent call last):\\n  File \"/usr/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\\n    chunked=chunked)\\n  File \"/usr/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 343, in _make_request\\n    self._validate_conn(conn)\\n  File \"/usr/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 839, in _validate_conn\\n    conn.connect()\\n  File \"/usr/lib/python3.7/site-packages/urllib3/connection.py\", line 344, in connect\\n    ssl_context=context)\\n  File \"/usr/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 335, in ssl_wrap_socket\\n    context.load_cert_chain(certfile, keyfile)\\nssl.SSLError: [SSL] PEM lib (_ssl.c:3845)\\n\\nBash:\\ncurl -u elastic:$ELASTIC_PASSWORD -X PUT -H \\'Content-Type: application/json\\' \\\\\\n  --cacert /certs/ca/ca.crt \\\\\\n  \\'https://localhost:9200/_xpack/security/user/kibana/_password\\' \\\\\\n  -d \\'{ \"password\": \"$ELASTIC_PASSWORD\" }\\''},\n",
       " {'text_field': \"Hi,\\nYou are using a wrong parameter with python-requests. The cert parameter is for passing in a client certificate to do authentication, and when you do that you also need to pass the respective private key ( which you don't and this is why urllib3 throws an error )\\nWhat you should be passing instead is the verify parameter that controls the CA certificates that are used to verify the server certificates and is the logical equivalent of using --cacert in curl. See https://requests.kennethreitz.org/en/master/user/advanced/#ssl-cert-verification for more details.\"},\n",
       " {'text_field': 'So we restarted our Elastic-Cluster last week, today i noticed that there are a lot unassigned Shards.\\nI thought my colleagues forgot to turn Shard - Allocation back on, which i can confirm by now.\\nI now i have to change it back in Order to get my Unassigned Shards away. But now if i try to do\\n curl -XPUT \"http://YY.XX.ZZZ.YXZ:9200/_cluster/settings\" -d \\'{\\n    \"transient\" : {\\n        \"cluster.routing.allocation.enable\" : \"all\"\\n    }\\n}\\'\\n\\ni get the following Error :  {\"error\":\"Content-Type header [application/x-www-form-urlencoded] is not supported\",\"status\":406}\\nThanks for you help in advance guys!'},\n",
       " {'text_field': \"You need to add:\\n-H 'Content-Type: application/json'\"},\n",
       " {'text_field': 'Hi I hope someone here can help.\\ni\\'ve recently setup a three node cluster on AKS using the elasticsearch helm chart, using pretty much all of the default values (aprat from the obvious persistent volumes etc..) and everything works great for a few hours but then I get a error\\n{\"type\": \"server\", \"timestamp\": \"2019-10-11T15:50:20,748Z\", \"level\": \"INFO\", \"component\": \"o.e.c.c.Coordinator\", \"cluster.name\": \"d2c-es-cluster-nonprod\", \"node.name\": \"d2c-es-cluster-nonprod-master-1\", \"message\": \"master node [{d2c-es-cluster-nonprod-master-2}{IeYJlsrSQ3KFxNBdn8M7sA}{qlURXoZ7SW6uKlBCoB1j5A}{10.253.156.134}{10.253.156.134:9300}{dilm}{ml.machine_memory=21474836480, ml.max_open_jobs=20, xpack.installed=true}] failed, restarting discovery\", \"cluster.uuid\": \"XPDaEguqRnWjluTh-Y6UVA\", \"node.id\": \"kyTywCrARMO1TGRDaWkoPA\"\\nthe three node setup is configure with three master, ingest and data nodes (as per the default)  i\\'ve checked each node and as far as I can tell each node is able to communicate with each other over port 9300 and 9200\\nas other posts suggest, i\\'ve tried to increase the timeouts, i\\'ve turned of Authentication and SSL but after a few hours I get the error mentioned above and the service discovery fails - only way to fix it is to restar the entire cluster.\\ni\\'ve attached my values yaml file here https://pastebin.com/S59buPPw\\ni\\'ve been pulling my hair out on this one for days!.'},\n",
       " {'text_field': \"Thanks for your help David,\\nIncase anyone stumbles across this with the same issue, i've managed to work out what the issue is.\\nbasically, due to our connections between kubernetes nodes going via an enterprise firewall - the firewall was terminating tcp keep alive connections and basically timing them out after a period of inactivity.\\nto fix this I set in the elasticsearch config\\ntransport.ping_schedule: 5s\\nthis, as far as I understand it keeps the tcp connection alive by sending ping requests every 5 seconds.\"},\n",
       " {'text_field': 'All is in the title I want to filter with the first data in my grok filter to use an other grok filter'},\n",
       " {'text_field': 'Searching alone'},\n",
       " {'text_field': 'Hi,\\nhow can i know the status of Filebeats across my organization in an elastic cloud deployment?\\nChecking into the documentation, looks like is for hosted cluster.'},\n",
       " {'text_field': 'The documentation here: https://www.elastic.co/guide/en/beats/filebeat/current/monitoring-internal-collection.html should work for ESS or ECE deployments also - just configure the URL/user/password to point at the ESS cluster\\nAlex'},\n",
       " {'text_field': 'Hello,\\nFollowing scenario:\\n\\ncreate ml job from saved search\\nedit this saved search\\n\\nWill the ml job continue with the data from the updated saved search?\\nGrtz\\nWillem'},\n",
       " {'text_field': 'Hi Willem,\\nI\\'m afraid the ML job will not be updated if you make any subsequent edits to the saved search.\\nThe elasticsearch query from saved search is copied to the job\\'s datafeed and exists separately.\\nHowever, it is possible to update the datafeed query directly from the Jobs Management page.\\nFor the job in question, select Edit Job from the menu at the end of the row in the table. Under the datafeed tab you can then edit the query.\\nNote, the datafeed must be in a \"stopped\" state in order for the query to be updated. Once the edits are saved you can start the datafeed again.\\nJames'},\n",
       " {'text_field': 'Hi\\nThe Health of my Index has turned yellow, seemingly because a replica shard cannot be allocated. This index has been created as part of the \"shrink\" action as defined in my ILM policy. The desired result is that the Index would have been shrunk and moved to the \"warm\" datanodes. I don\\'t think that this can complete until all shards are assigned.\\nI\\'ll post the output of _cluster/allocation/explain, _cat/shards/&lt;index&gt;, ILM policy, Index template and Elasticsearch logs below. Please let me know if you have any idea at all why this replica is now unassigned.\\n_cat/shards/shrink-filebeat-haproxy-production-2019.10.08-000001?v\\nindex                                                shard prirep state           docs  store ip          node\\nshrink-filebeat-haproxy-production-2019.10.08-000001 0     p      STARTED    132097605   90gb 10.0.16.212 es-dn-warm-3.core.ld5.phg.io\\nshrink-filebeat-haproxy-production-2019.10.08-000001 0     r      STARTED    132097605 90.1gb 10.0.16.210 es-dn-warm-1.core.ld5.phg.io\\nshrink-filebeat-haproxy-production-2019.10.08-000001 0     r      UNASSIGNED                              \\n\\n_cluster/allocation/explain\\nMoved to comment as hit max body limit.\\n\\nILM policy:\\n{\\n    \"policy\": {\\n        \"phases\": {\\n            \"hot\": {\\n                \"min_age\": \"0ms\",\\n                \"actions\": {\\n                    \"rollover\": {\\n                        \"max_age\": \"30d\",\\n                        \"max_size\": \"90gb\"\\n                    },\\n                    \"set_priority\": {\\n                        \"priority\": 100\\n                    }\\n                }\\n            },\\n            \"warm\": {\\n                \"min_age\": \"30d\",\\n                \"actions\": {\\n                    \"allocate\": {\\n                        \"include\": {},\\n                        \"exclude\": {},\\n                        \"require\": {\\n                            \"data\": \"warm\"\\n                        }\\n                    },\\n                    \"forcemerge\": {\\n                        \"max_num_segments\": 1\\n                    },\\n                    \"set_priority\": {\\n                        \"priority\": 50\\n                    },\\n                    \"shrink\": {\\n                        \"number_of_shards\": 1\\n                    }\\n                }\\n            }\\n        }\\n    }\\n}\\n\\nIndex Template (some fields removed):\\n{\\n  \"settings\": {\\n    \"index\": {\\n      \"mapping\": {\\n        \"total_fields\": {\\n          \"limit\": \"10000\"\\n        }\\n      },\\n      \"refresh_interval\": \"5s\",\\n      \"blocks\": {\\n        \"write\": \"true\"\\n      },\\n      \"provided_name\": \"filebeat-haproxy-production-2019.10.08-000001\",\\n      \"query\": {\\n      ...\\n      }\\n      \"creation_date\": \"1570537372676\",\\n      \"priority\": \"50\",\\n      \"number_of_replicas\": \"2\",\\n      \"uuid\": \"***\",\\n      \"version\": {\\n        \"created\": \"7030099\"\\n      },\\n      \"lifecycle\": {\\n        \"name\": \"filebeat-haproxy-production-ilm-policy\",\\n        \"rollover_alias\": \"filebeat-haproxy-production-ilm-alias\",\\n        \"indexing_complete\": \"true\"\\n      },\\n      \"codec\": \"best_compression\",\\n      \"routing\": {\\n        \"allocation\": {\\n          \"require\": {\\n            \"data\": \"warm\",\\n            \"_id\": \"***\"\\n          }\\n        }\\n      },\\n      \"number_of_shards\": \"3\",\\n      \"shard\": {\\n        \"check_on_startup\": \"checksum\"\\n      }\\n    }\\n  },\\n\\nElasticsearch Logs (From datanode which is failing to allocated replica)\\nMoved to comment as hit max body limit.\\n\\nThings that I\\'ve tried:\\n\\nRan POST /_cluster/reroute?retry_failed=true to try and retry the shard allocation. Shard turns to INITIALIZATION state then moves back to UNASSIGNED after a short period of time. Above Elasticsearch log error is noticed once INITIALIZATION has failed.\\nSet cluster.routing.allocation.enable\": \"none\". Tried to manually allocate the replica using the /_cluster/reroute API. Then renabled shard allocation. Same failure.\\n\\nPlease let me know if you need any further logs/information'},\n",
       " {'text_field': 'It looks like you have index.shard.check_on_startup configured. This performs a (very expensive) check of your index when it is starting up. I suggest removing this setting.'},\n",
       " {'text_field': 'Hi ,\\nI using filebeat to send mysql data to elasticsearch, but i notice there are no result for mysql.error data in elasticsearch even though mysql.slowlog data are coming correctly. I check the data using kibana discover\\ni\\'m using filebeat 7.4 and elasticsearch 7.4\\nfilebeat module conf:\\n# cat /etc/filebeat/modules.d/mysql.yml\\n# Module: mysql\\n# Docs: https://www.elastic.co/guide/en/beats/filebeat/7.4/filebeat-module-mysql.html\\n\\n- module: mysql\\n  # Error logs\\n  error:\\n    enabled: true\\n\\n    # Set custom paths for the log files. If left empty,\\n    # Filebeat will choose the paths depending on your OS.\\n    var.paths: [\"/var/log/mariadb/mariadb.log\"]\\n\\n  # Slow logs\\n  slowlog:\\n    enabled: true\\n\\n    # Set custom paths for the log files. If left empty,\\n    # Filebeat will choose the paths depending on your OS.\\n    var.paths: [\"/var/log/mariadb/mariadb-slow.log\"]\\n\\nI have check filebeat stdout using command:  filebeat -e -d \"*\" ... no error found. The stdout show that the mysql.error and mysql.slowlog data has been posted to elasticsearch. But still there is nothing from elasticsearch. The only data appear in elasticsearch is mysql.slowlog.\\nfilebeat -e -d \"*\" output:\\n2019-10-14T02:19:23.785+0800    DEBUG   [modules]       fileset/pipelines.go:67 Required processors: []\\n2019-10-14T02:19:23.785+0800    DEBUG   [elasticsearch] elasticsearch/client.go:761     GET http://localhost:9200/_ingest/pipeline/filebeat-7.4.0-mysql-error-pipeline  &lt;nil&gt;\\n2019-10-14T02:19:23.786+0800    DEBUG   [modules]       fileset/pipelines.go:120        Pipeline filebeat-7.4.0-mysql-error-pipeline already loaded\\n2019-10-14T02:19:23.786+0800    DEBUG   [modules]       fileset/pipelines.go:67 Required processors: []\\n2019-10-14T02:19:23.786+0800    DEBUG   [elasticsearch] elasticsearch/client.go:761     GET http://localhost:9200/_ingest/pipeline/filebeat-7.4.0-mysql-slowlog-pipeline  &lt;nil&gt;\\n2019-10-14T02:19:23.788+0800    DEBUG   [modules]       fileset/pipelines.go:120        Pipeline filebeat-7.4.0-mysql-slowlog-pipeline already loaded\\n2019-10-14T02:19:23.788+0800    INFO    input/input.go:114      Starting input of type: log; ID: 1124530994825458434\\n2019-10-14T02:19:23.788+0800    INFO    input/input.go:114      Starting input of type: log; ID: 7616194281951100578\\n2019-10-14T02:19:23.788+0800    DEBUG   [input] log/input.go:191        Start next scan\\n2019-10-14T02:19:23.788+0800    DEBUG   [input] log/input.go:191        Start next scan\\n2019-10-14T02:19:23.789+0800    DEBUG   [input] log/input.go:421        Check file for harvesting: /var/log/mariadb/mariadb.log\\n2019-10-14T02:19:23.789+0800    DEBUG   [input] log/input.go:511        Update existing file for harvesting: /var/log/mariadb/mariadb.log, offset: 411223\\n2019-10-14T02:19:23.789+0800    DEBUG   [input] log/input.go:520        Resuming harvesting of file: /var/log/mariadb/mariadb.log, offset: 411223, new size: 412093\\n2019-10-14T02:19:23.789+0800    DEBUG   [input] log/input.go:421        Check file for harvesting: /var/log/mariadb/mariadb-slow.log\\n2019-10-14T02:19:23.789+0800    DEBUG   [input] log/input.go:511        Update existing file for harvesting: /var/log/mariadb/mariadb-slow.log, offset: 4600\\n2019-10-14T02:19:23.789+0800    DEBUG   [input] log/input.go:520        Resuming harvesting of file: /var/log/mariadb/mariadb-slow.log, offset: 4600, new size: 4861\\n2019-10-14T02:19:23.789+0800    DEBUG   [harvester]     log/harvester.go:489    Set previous offset for file: /var/log/mariadb/mariadb.log. Offset: 411223\\n2019-10-14T02:19:23.789+0800    DEBUG   [harvester]     log/harvester.go:480    Setting offset for file: /var/log/mariadb/mariadb.log. Offset: 411223\\n2019-10-14T02:19:23.789+0800    DEBUG   [harvester]     log/harvester.go:182    Harvester setup successful. Line terminator: 1\\n2019-10-14T02:19:23.790+0800    DEBUG   [publisher]     pipeline/client.go:220  Pipeline client receives callback \\'onFilteredOut\\' for event: {Timestamp:0001-01-01 00:00:00 +0000 UTC Meta:null Fields:null Private:{Id: Finished:false Fileinfo:0xc0007564e0 Source:/var/log/mariadb/mariadb.log Offset:411223 Timestamp:2019-10-14 02:19:23.78908156 +0800 +08 m=+3.366934399 TTL:-1ns Type:log Meta:map[] FileStateOS:415273452-64768} TimeSeries:false}    \\n\\ni also tried to post the mysql.error data manually using curl to elasticsearch, found out that the data coming out correctly\\n# curl -u elastic:elasticpwd -H \\'Content-Type: application/json\\' -X POST \"localhost:9200/filebeat-7.4.0-2019.10.11-000001/_doc/\" -d \\'{\"@timestamp\":\"2019-10-13T00:18:35.510Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.4.0\",\"pipeline\":\"filebeat-7.4.0-mysql-error-pipeline\"},\"input\":{\"type\":\"log\"},\"message\":\"191013  8:18:32\\\\t     8 Query\\\\tSHOW /*!50002 GLOBAL */ STATUS\",\"event\":{\"dataset\":\"mysql.error\",\"module\":\"mysql\"},\"fileset\":{\"name\":\"error\"},\"agent\":{\"type\":\"filebeat\",\"ephemeral_id\":\"16835ee0-9321-476a-802b-c61f0640c370\",\"hostname\":\"nttdisbdb02\",\"id\":\"97520869-259a-4d60-bb46-5eecf412f37d\",\"version\":\"7.4.0\"},\"ecs\":{\"version\":\"1.1.0\"},\"log\":{\"file\":{\"path\":\"/var/log/mariadb/mariadb.log\"},\"offset\":11778},\"service\":{\"type\":\"mysql\"},\"host\":{\"id\":\"9d4ba8aa77aa49ccba4f96e8d89eb7c3\",\"name\":\"nttdisbdb02\",\"containerized\":false,\"hostname\":\"nttdisbdb02\",\"architecture\":\"x86_64\",\"os\":{\"kernel\":\"3.10.0-862.3.2.el7.x86_64\",\"codename\":\"Core\",\"platform\":\"centos\",\"version\":\"7 (Core)\",\"family\":\"redhat\",\"name\":\"CentOS Linux\"}}}\\'\\n{\"_index\":\"filebeat-7.4.0-2019.10.11-000001\",\"_type\":\"_doc\",\"_id\":\"BDTTyW0BJGi3-aJ4nxKF\",\"_version\":1,\"result\":\"created\",\"_shards\":{\"total\":2,\"successful\":1,\"failed\":0},\"_seq_no\":7348502,\"_primary_term\":1}\\n\\ni check the mysql.error data in elasticsearch the data came out normally as expected.\\n# curl -u elastic:elasticpwd -X GET \"nttdisblb01:9200/filebeat-7.4.0-2019.10.11-000001/_source/BDTTyW0BJGi3-aJ4nxKF\"\\n{\"@timestamp\":\"2019-10-13T00:18:35.510Z\",\"@metadata\":{\"beat\":\"filebeat\",\"type\":\"_doc\",\"version\":\"7.4.0\",\"pipeline\":\"filebeat-7.4.0-mysql-error-pipeline\"},\"input\":{\"type\":\"log\"},\"message\":\"191013  8:18:32\\\\t     8 Query\\\\tSHOW /*!50002 GLOBAL */ STATUS\",\"event\":{\"dataset\":\"mysql.error\",\"module\":\"mysql\"},\"fileset\":{\"name\":\"error\"},\"agent\":{\"type\":\"filebeat\",\"ephemeral_id\":\"16835ee0-9321-476a-802b-c61f0640c370\",\"hostname\":\"nttdisbdb02\",\"id\":\"97520869-259a-4d60-bb46-5eecf412f37d\",\"version\":\"7.4.0\"},\"ecs\":{\"version\":\"1.1.0\"},\"log\":{\"file\":{\"path\":\"/var/log/mariadb/mariadb.log\"},\"offset\":11778},\"service\":{\"type\":\"mysql\"},\"host\":{\"id\":\"9d4ba8aa77aa49ccba4f96e8d89eb7c3\",\"name\":\"nttdisbdb02\",\"containerized\":false,\"hostname\":\"nttdisbdb02\",\"architecture\":\"x86_64\",\"os\":{\"kernel\":\"3.10.0-862.3.2.el7.x86_64\",\"codename\":\"Core\",\"platform\":\"centos\",\"version\":\"7 (Core)\",\"family\":\"redhat\",\"name\":\"CentOS Linux\"}}}'},\n",
       " {'text_field': 'Hi @Admiraludon,\\nI have an update. I think that your log files are fine, and they should be actually collected. Even the line with the different pattern should be captured because we have a fallback pattern to capture any line.\\nPlease check if this query returns any document:\\nGET /filebeat-*/_search?q=event.dataset:mysql.error\\n\\nI think that the problem may be in the date parsing. Your logs are probably there, but not at the time that you expect. If the previous query returns any document, please check what is the timestamp, and then select this time in kibana discover. You should be able to see your documents there.\\nI have prepared some fixes to improve timestamp handling in this pull request: https://github.com/elastic/beats/pull/14130'},\n",
       " {'text_field': 'Hi!\\nHow can I increase number of symbols in one row in Discovery search?\\nBy default if shows 5-7 strings, but I want to see the whole message without expanding each row by clicking \"&gt;\".\\nScreenshot attached\\n\\n2019-10-14 14_32_55-2019-10-14 14_29_40-Параметры-.png1908×872 114 KB\\n'},\n",
       " {'text_field': 'There is no option to configure that at the moment, but do open an issue on https://github.com/elastic/kibana for it please.'},\n",
       " {'text_field': 'Hello everyone\\ni have a simple filebeat.yml which send some log from specific path to logstash\\nmy problem is how can i activate some filebeat modules such as system on this host while the output of filebeat.yml is logstash and the modules used elasticsearch as output??\\nshould i config multiple config file for this purpose?or should i have create multiple filebeat service ?'},\n",
       " {'text_field': '@Mohammad.ali,\\nWhich version are you using? Yes, you can use filebeat modules with logstash output. You need to use logstash pipeline for parsing. Please refer the below documentation:\\nLogstash pipeline for parsing\\nThanks.'},\n",
       " {'text_field': \"Hi,\\nI have the problem with reindexing my documents, I have index with 900K documents but RestHighLevelClient always returns   java.net.SocketTimeoutException, I increase the socket timeout, set the batch size to 500 but still problem occurs. How I should resolve this problem ?\\nmy code looks like\\n public Mono&lt;BulkByScrollResponse&gt; reindex(String sourceIndex, String destIndex) {\\n\\n    ReindexRequest request = new ReindexRequest();\\n    request.setSourceIndices(sourceIndex);\\n    request.setDestIndex(destIndex);\\n    request.setSourceBatchSize(500);\\n\\n    return Mono.create(\\n        sink -&gt;\\n            elasticClient\\n                .reindexAsync(request, RequestOptions.DEFAULT, listenerToSink(sink)));\\n  }\\n\\nIs this socket opened each batch or it waits to the end of reindexing?  I add also more slices but this didn't solve the issue. My documents are pretty simple, just 15 text fields (&lt; 30 signs)\"},\n",
       " {'text_field': 'Hi @Dawid_Macura,\\nusing reindex or reindexAsync, it is likely that the entire reindex request need to complete within the socket timeout. The request is sent in its entirety to the server and thus batching does not interact with socket timeout here.\\nYou can use submitReindexTask instead, which will return information about the started task that you can then use task().get(...) to interrogate.'},\n",
       " {'text_field': 'Hi\\nI\\'m new with Kibana. I have download elasticsearch, extracted the fil and install it with cmd. Now I have to do the same for Kibana, but I have problems to extract the zip file. When I\\'m trying, the Folder program just crash and doent react...\\nI\\'m trying to extract \"kibana-7.4.0-windows-x86_64\"'},\n",
       " {'text_field': 'I solved it! I just had to install 7-zip '},\n",
       " {'text_field': 'Всем привет.\\nПоднял с нуля ELK.\\nНастроил его на мониторинг\\nВ /etc/logstash/conf.d/postfix.conf\\nhttp://pastebin.calculate-linux.ru/ru/show/127900\\nВ /etc/logstash/patterns/postfix\\nhttp://pastebin.calculate-linux.ru/ru/show/127901\\nНо в логах logstash\\n\\nBlockquote\\n\\n[2019-10-14T10:39:13,494][WARN ][logstash.outputs.elasticsearch][main] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"postfix10-2019.10.14\", :_type=&gt;\"_doc\", :routing=&gt;nil}, #LogStash::Event:0x65c32d89], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"postfix10-2019.10.14\", \"_type\"=&gt;\"_doc\", \"_id\"=&gt;\"MBm1ym0Br-QV79CHeMxH\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"Could not dynamically add mapping for field [postfix.client_ip]. Existing mapping for [postfix] must be of type object but found [boolean].\"}}}}\\nКак починить это ?'},\n",
       " {'text_field': 'Вместо blockquote лучше использовать ```. Я поправил ваш пост для примера.\\n\\n\\n\\n beren:\\n\\nОбъясните, пожалуйста, что они делают ?\\n\\n\\n\\n  \\n      \\n\\n      elastic.co\\n  \\n\\n  \\n    \\n\\nLog input | Filebeat Reference [7.15] | Elastic\\n\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n'},\n",
       " {'text_field': 'Hello,\\nI am working to set-up a hot-warm-cold-delete policy for my ELK cluster that has multiple indices. I basically have 4 questions and as much data following them up as I thought needed.\\nMany thanks to any help!\\nAll data ingested into ES comes through Logstash first. Example LS config:\\noutput {\\n if [type] == \\'syslog\\' {\\n  elasticsearch {\\n        hosts =&gt; [ \"10.15.1.108:9200\" ]\\n        ilm_enabled =&gt; true\\n        ilm_rollover_alias =&gt; \"wincollect-ilm\"\\n        index =&gt; \"wincollect-%{+YYYY.MM.dd}\"\\n        user =&gt; \"****\"\\n        password =&gt; \"****\"\\n\\n                 }\\n\\n                       }\\n       }\\n\\nI am trying to follow the many guides and blog posts out there. Before implementing ILM I did not have either of the ilm_ options and thus an index titled wincollect-date was created. I have learned that date formatting is difficult with ILM so I created the alias. I don\\'t need date formatting anyway.\\n\\nFor creating the index template, do I need to create a template for each index and then specifiy the template in each different logstash config?\\n\\nFor the above LS config, would I create this template:\\nPUT _template/wincollect-template\\n{\\n  \"index_patterns\": [\"wincollect-\"], \\n  \"settings\": {\\n    \"index.lifecycle.name\": \"hot-warm-cold\", \\n    \"index.lifecycle.rollover_alias\": \"wincollect\" \\n  }\\n}\\n\\n\\nAnd then add this to the LS config?\\n\\n\\nilm_policy =&gt; \"wincollect-template\"\\n\\n\\nAnd, do I need to do anything with POST /_aliases? ie\\n\\n\\nPOST /_aliases\\n{\\n  \"actions\": [\\n    {\\n      \"add\": {\\n        \"index\": \"wincollect-*\",\\n        \"alias\": \"wincollect-ilm\"\\n      }\\n    }\\n  ]\\n  \\n}\\n\\n\\n\\nOR, is ilm_policy supposed to be the actual policy I created? In this case, \"hot-warm-cold\" as seen below:\\n\\n\\n{\\n  \"hot-warm-cold\" : {\\n    \"version\" : 2,\\n    \"modified_date\" : \"2019-10-14T13:40:50.345Z\",\\n    \"policy\" : {\\n      \"phases\" : {\\n        \"warm\" : {\\n          \"min_age\" : \"60d\",\\n          \"actions\" : {\\n            \"allocate\" : {\\n              \"include\" : { },\\n              \"exclude\" : { },\\n              \"require\" : {\\n                \"data\" : \"warm\"\\n              }\\n            },\\n            \"forcemerge\" : {\\n              \"max_num_segments\" : 1\\n            },\\n            \"set_priority\" : {\\n              \"priority\" : 50\\n            },\\n            \"shrink\" : {\\n              \"number_of_shards\" : 1\\n            }\\n          }\\n        },\\n        \"cold\" : {\\n          \"min_age\" : \"365d\",\\n          \"actions\" : {\\n            \"allocate\" : {\\n              \"include\" : { },\\n              \"exclude\" : { },\\n              \"require\" : {\\n                \"data\" : \"cold\"\\n              }\\n            },\\n            \"freeze\" : { },\\n            \"set_priority\" : {\\n              \"priority\" : 0\\n            }\\n          }\\n        },\\n        \"hot\" : {\\n          \"min_age\" : \"0ms\",\\n          \"actions\" : {\\n            \"rollover\" : {\\n              \"max_size\" : \"100gb\",\\n              \"max_age\" : \"30d\"\\n            },\\n            \"set_priority\" : {\\n              \"priority\" : 100\\n            }\\n          }\\n        },\\n        \"delete\" : {\\n          \"min_age\" : \"2190d\",\\n          \"actions\" : {\\n            \"delete\" : { }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n'},\n",
       " {'text_field': 'Figured it out.\\nFollowing\\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index-lifecycle-management.html\\nand\\n\\n  \\n      \\n      Elastic Blog – 10 Apr 19\\n  \\n  \\n    \\n\\nImplementing Hot-Warm-Cold in Elasticsearch with Index Lifecycle Management\\n\\nIndex lifecycle management (ILM) is part of Elasticsearch and is designed to help you manage your indexes. In this blog, we will explore how to implement a hot-warm-cold architecture using ILM. Hot-warm-cold architectures are common for time series...\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nI first created my policy using Kibana. Cross checked with the devconsole using\\n\\nGET  /_ilm/policy\\n\\nIn the devconsole, I made a template for each of my indexes:\\n\\nPUT _template/wincollect-template\\n{\\n\"index_patterns\": [\"wincollect-\"],\\n\"settings\": {\\n\"index.routing.allocation.require.data\": \"hot\",\\n\"index.lifecycle.name\": \"hot-warm-cold\",\\n\"index.lifecycle.rollover_alias\":\"wincollect-ilm\"\\n}\\n}\\n\\nI made 5 such templates.\\nI then had to delete some preexisting indexes. I could have \"renamed\" them, but the loss of data did not matter at this time. Next I did the whole bootstrapping thing.\\n\\nPUT wincollect-ilm-000001\\n{\\n\"aliases\": {\\n\"wincollect-ilm\":{\\n\"is_write_index\": true\\n}\\n}\\n}\\n\\nI removed all aliases that i had created manually and let them be created by the template. I think there was one index that didn\\'t fit the template that I forced an alias on.\\n2 weeks later, still working as expected.'},\n",
       " {'text_field': 'I\\'m trying to write a simple Elastic SQL that polls the temperature value in the last 3 minutes but i\\'m not able to select such a small timeframe. It should be as simple as NOW() - INTERVAL 3 MINUTES but that returns nothing.\\nIf I increase the INTERVAL to 250 minutes, that somehow equates to the last 10 minutes and I don\\'t understand how that can be.\\nWhat could I be doing wrong here?\\nsql&gt; SELECT timestamp AS TIME, round(TEMPERATURE_A,1) AS TEMP FROM \\\\\"test*\\\\\" WHERE agent.hostname = \\'b-1-1\\' AND TEMP is not null AND TIME &lt; NOW() AND TIME &gt; NOW() - INTERVAL 250 MINUTES ORDER BY TIME DESC LIMIT 1;\\n          TIME          |     TEMP\\n------------------------+---------------\\n2019-10-14T11:42:00.000Z|-166.9\\n\\nsql&gt; SELECT timestamp AS TIME, round(TEMPERATURE_A,1) AS TEMP FROM \\\\\"test*\\\\\" WHERE agent.hostname = \\'b-1-1\\' AND TEMP is not null AND TIME &lt; NOW() AND TIME &gt; NOW() - INTERVAL 250 MINUTES ORDER BY TIME ASC LIMIT 1;\\n          TIME          |     TEMP\\n------------------------+---------------\\n2019-10-14T11:33:00.000Z|-166.9\\n\\n241 seems like it represents the last minute but again, I am at a loss\\nsql&gt; SELECT timestamp AS TIME, round(TEMPERATURE_A,1) AS TEMP FROM \\\\\"test*\\\\\" WHERE agent.hostname = \\'b-1-1\\' AND TEMP is not null AND TIME &lt; NOW() AND TIME &gt; NOW() - INTERVAL 240 MINUTES ORDER BY TIME ASC;\\n     TIME      |     TEMP\\n---------------+---------------\\n\\nsql&gt; SELECT timestamp AS TIME, round(TEMPERATURE_A,1) AS TEMP FROM \\\\\"test*\\\\\" WHERE agent.hostname = \\'b-1-1\\' AND TEMP is not null AND TIME &lt; NOW() AND TIME &gt; NOW() - INTERVAL 241 MINUTES ORDER BY TIME ASC;\\n          TIME          |     TEMP\\n------------------------+---------------\\n2019-10-14T11:50:00.000Z|-166.9\\n2019-10-14T11:50:00.000Z|-166.9\\n2019-10-14T11:50:00.000Z|-166.9\\n\\nsql&gt;\\n\\nAny ideas?'},\n",
       " {'text_field': 'Hi, Rob.\\nI\\'m not able to replicate this on Elasticsearch 7.4. I used the following steps in the Kibana dev console.\\n\\nPost mappings to get the right datatypes:\\n\\nPUT sql_testing2\\n{\\n  \"mappings\": {\\n    \"properties\" : {\\n        \"TEMPERATURE_A\" : {\\n          \"type\" : \"float\"\\n        },\\n        \"timestamp\" : {\\n          \"type\" : \"date\"\\n        }\\n      }\\n  }\\n}\\n\\n\\nPost a document for testing (it\\'s 17:19 UTC time, as I test):\\n\\nPUT sql_testing2/_doc/2\\n{\\n  \"timestamp\": \"2019-10-14T17:18:00.000Z\",\\n  \"TEMPERATURE_A\": 312.2\\n}\\n\\n\\nRun a slightly modified version of your query against the REST API:\\n\\nPOST /_sql?format=txt\\n{\\n  \"query\": \"SELECT timestamp AS TIME, round(TEMPERATURE_A,1) AS TEMP FROM sql_testing2 WHERE TEMP is not null AND TIME &lt; NOW() AND TIME &gt; NOW() - INTERVAL 400 MINUTES\"\\n}\\n\\n          TIME          |     TEMP      \\n------------------------+---------------\\n2019-10-14T17:18:00.000Z|312.2     \\n\\nHave you checked your logic around timezones? You can see what now() is returning by running SELECT now() AS RIGHT_NOW;.\\nI hope this is helpful!\\n-William'},\n",
       " {'text_field': \"Is it possible to create index patterns with the help of Elasticsearch JS client in Elkstack 7.3+?\\nOr, for example, from Kibana plugin?\\nI know it's possible from the API side, but is it achievable using the client?\\nThanks a lot\"},\n",
       " {'text_field': \"Yes, it's possible to use SavedObject client for this, but with nuances. If you don't have a lot of the logic on the server it would be easier to call API endpoint directly.\\nOperations with SavedObjects performed on behalf of a user. It means that whenever you access SavedObject client on the server, it should be associated with the incoming request objects. Thus you need to register your endpoint to get SavedObjects client and create index-pattern:\\nconst client  = request.getSavedObjectsClient();\\nclient.create('index-pattern', ...);\\n\\nexample from the Kibana codebase:\\n\\n  \\n      github.com\\n  \\n  \\n    elastic/kibana/blob/92917c1c19040c47daca3be10b280ce08d96a135/src/legacy/server/saved_objects/routes/create.ts#L82\\n\\n            .default([]),\\n        }).required(),\\n      },\\n      handler(request: CreateRequest) {\\n        const { savedObjectsClient } = request.pre;\\n        const { type, id } = request.params;\\n        const { overwrite } = request.query;\\n        const { migrationVersion, references } = request.payload;\\n        const options = { id, overwrite, migrationVersion, references };\\n\\n\\n        return savedObjectsClient.create(type, request.payload.attributes, options);\\n      },\\n    },\\n  };\\n};\\n\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\"},\n",
       " {'text_field': 'I have an index which documents contain a date field, a PackageName field (with keywords) and Quantity field. Here follows a doc sample:\\n{\\n\"Quantity\" : 1054,\\n\"PackageName\" : \"PackageName1\",\\n\"Date\" : \"2019-09-04\",\\n}\\nI have one document for each day, for each PackageName. I need to create a table (or any visualization)  in Kibana that present me, for each package name, de difference of the quantity at days Yesterday and Today.\\nThis looks very simple, but unfortunately I was not able to create any visualization or table for that.\\nI plotted in the attached image that table that I\\'d like to create.\\nCould anyone help me?\\nThanks,\\nAlex\\n'},\n",
       " {'text_field': 'Hmm, there\\'s sort of a way to accomplish what you want.\\n\\nCreate a data table visualization.\\nUnder Metrics, Choose your metric you want to display in the chart.\\nChoose another metric, the \"derivative\", making sure to select the same custom metric you selected in (2).\\nUnder Aggregations, split rows by terms for package name.\\nUnder Aggregations, split table (rows) by data histogram with an interval of \"daily\".\\n\\nThe end result will end up looking something like this:\\n\\nimage.png2026×788 28.2 KB\\n\\nWhere the table on the left is yesterday and the table on the right is today.'},\n",
       " {'text_field': 'We have a 4 node cluster using elasticsearch-6.5.2 and recently we did a synonyms update which caused the cluster state to turn red for two indexes out of four. The initial error we discovered was due to the shard failing to allocate:\\n \"Failed shard on node [x] : failed to create index, failure illegaArgumentException[Failed to build synonyms]; nested NotSerializableExceptionWrapper[parse_exception: Invalid synonyms rule at line 2; nest IllegalArgumentException[term: termination of pregnancy anazlyed to a token (pregnancy) with position increment != 1 (got: 2)]; the allocate_explanation was \"cannot allocate because allocation is not permitted to any of the nodes that hold an in-sync shard copy\"\\n\\nWe understand why the above error occurred because the analysis chain has the stopwords filter first and then at the end has the synonyms filter and we had agreed a procedure whereby no stopwords were to be entered into the synonyms file, however accidentally a stopword was entered.\\nWhat we don\\'t understand is why the synonyms update caused the state to turn to red and not recover. Since this happend on the live instance we quickly rebuilt the indexes and deleted the old ones when were realised they weren\\'t recoverable. Looking at the logs there is some interesting information about failed to list shard for shard_store on node:\\nplease see: https://gist.github.com/imranazad/7436c43bb7ca87a1ce1f64b988d22a83\\nJust to add context when we update the synonyms file we restart the elasticsearch service via the command line .\\nSo what caused the unrecoverability of the indexes? I\\'m not convinced it was the direct result of the synonyms update although yes that would have stopped the shard allocation but it shouldn\\'t have made the indexes unrecoverable.'},\n",
       " {'text_field': 'Hey Christoph,\\nThanks so much for getting back to me, really appreciate you trying to reproduce the issue.\\nWe figured out in the end what the issue was. In a panic we decided to rebuild the indexes by rolling back the synonyms file. Whereas as all we really needed to do was roll back the synonyms file and then wait for the cluster to recover by itself.\\nThanks again, this can be closed.'},\n",
       " {'text_field': 'Kibana version: 7.3.2\\nElasticsearch version:  7.3.2\\nAPM Server version:  7.3.2\\nAPM Agent language and version: java 1.9.0\\nFresh install or upgraded from other version? fresh install\\nHi\\nI am trying to use open tracing API in my application to trace events. Can Kibana visualize the events captured by the open API spans ? I was able to trace my application using the native elastic API , but I want to make it work with open tracing API.\\nI have used the ElasticApmTracer but when I try to use the below line\\nspan = tracer.buildSpan(eventName).startActive(false).span();\\nI am able to get SpanBuilder , scope manager but I am not able to start the span as start method is an inner interface of the main Tracer inteface .\\nHow can i start my span after using the elasticAPm bridge\\nThere is an example also provided in the elastic website as below\\nTracer tracer = new ElasticApmTracer();\\ntry (Scope scope = tracer.buildSpan(\"say-hello\").startActive(true)) {\\nscope.span().setTag(\"hello-to\", helloTo);\\n}\\nBut the startActive method always throws the below error.\\nPls assist\\nError java.lang.AbstractMethodError: co.elastic.apm.opentracing.ApmSpanBuilder.startActive(Z)Lio/opentracing/Scope;'},\n",
       " {'text_field': 'Using it through -javaagent is what I meant by \"install\" \\nOK, so you should see agent messages being logged alongside the application logs. If you see errors- please share. Otherwise, try to follow them and see whether there are activated transactions/spans not being deactivated (meaning- unclosed scopes) or spans/transactions not being ended (through OpenTracing finish).'},\n",
       " {'text_field': \"Hi ELK Wise Ones…\\nI recently spun up Logstash 7.4.0 as a Docker container. My host machine is a Ubuntu 16.04.4, and my Docker is 17.09.0-ce, build afdb6d4.\\nI was following the procedure to set automatic config reloading (here), as I need a graceful way to tweak the config file. (I’m referring to the config file located in /usr/share/logstash/pipeline/logstash.conf)\\nThe documentation says to run this command: “bin/logstash –f apache.config --config.reload.automatic”. But for me, it did not go well. (See below)\\nThe command runs the logstash script with the -f option.  But the error message suggests the -f option is not viable.  That seems really weird to me...  is it possible I'm doing something wrong?  Is there another way to set auto config reload?  Would love to know. Thanks!\\nbash-4.2$ pwd\\n/usr/share/logstash\\nbash-4.2$\\nbash-4.2$\\nbash-4.2$ bin/logstash –f apache.config --config.reload.automatic\\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\\nWARNING: An illegal reflective access operation has occurred\\nWARNING: Illegal reflective access by com.headius.backport9.modules.Modules (file:/usr/share/logstash/logstash-core/lib/jars/jruby-complete-9.2.8.0.jar) to field java.io.FileDescriptor.fd\\nWARNING: Please consider reporting this to the maintainers of com.headius.backport9.modules.Modules\\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\\nWARNING: All illegal access operations will be denied in a future release\\nThread.exclusive is deprecated, use Thread::Mutex\\nERROR: Unknown command '–f'\\nSee: 'bin/logstash --help'\\n[ERROR] 2019-10-14 20:02:33.485 [main] Logstash - java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit\\nbash-4.2$\"},\n",
       " {'text_field': '\\n\\n\\n redapplesonly:\\n\\n–f\\n\\n\\nThe character before the letter f is a unicode character for \"non-breaking hyphen\" U+8211, and not an ASCII hyphen (\\'-\\').\\nEDIT: I see that the source documentation had the wrong character there  and have opened up a pull-request to fix it here doc: replace unicode non-breaking hyphen U+8211 with ASCII hyphen by yaauie · Pull Request #11217 · elastic/logstash · GitHub'},\n",
       " {'text_field': 'I am currently trying to get SAML auth working with OneLogin .\\nVersion  : Elastic Cloud Cluster 7.4\\n{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"[security_exception] unable to authenticate user [&lt;unauthenticated-saml-user&gt;] for action [cluster:admin/xpack/security/saml/authenticate], with { header={ WWW-Authenticate={ 0=\\\\\"Bearer realm=\\\\\\\\\\\\\"security\\\\\\\\\\\\\"\\\\\" &amp; 1=\\\\\"ApiKey\\\\\" &amp; 2=\\\\\"Basic realm=\\\\\\\\\\\\\"security\\\\\\\\\\\\\" charset=\\\\\\\\\\\\\"UTF-8\\\\\\\\\\\\\"\\\\\" } } }\"}\\n\\nElasticsearch config:\\nxpack:\\n  security:\\n    authc:\\n      realms:\\n        saml: \\n          cloud-saml: \\n            order: 2\\n            attributes.principal: \"nameid:persistent\" \\n            attributes.groups: \"groups\" \\n            idp.metadata.path: \"https://app.onelogin.com/saml/metadata/xxx\" \\n            idp.entity_id: \"https://app.onelogin.com/saml/metadata/xxx\" \\n            sp.entity_id: \"https://8e1d.us-east-1.aws.found.io:9243/\" \\n            sp.acs: \"https://8e1d.us-east-1.aws.found.io:9243/api/security/v1/saml\"\\n            sp.logout: \"https://8e1d.us-east-1.aws.found.io:9243/logout\"\\n\\nKibana config:\\nxpack.security.authc.providers: [saml, basic]\\nserver.xsrf.whitelist: [/api/security/v1/saml]\\nxpack.security.authc.saml.realm: cloud-saml\\n\\nSAML Stuff for /api/security/v1/saml\\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;samlp:Response xmlns:samlp=\"urn:oasis:names:tc:SAML:2.0:protocol\" xmlns:saml=\"urn:oasis:names:tc:SAML:2.0:assertion\" ID=\"Rf6dd186d758d8bba33ea4d110d8990b2958ce5a8\" Version=\"2.0\" IssueInstant=\"2019-10-14T22:02:47Z\" Destination=\"{recipient}\" InResponseTo=\"_f39ff7cc60b3fb475bc0d4d1c51e8369fe3b99c4\"&gt;\\n   &lt;saml:Issuer&gt;https://app.onelogin.com/saml/metadata/13e12235-b801-4ff4-85c6-5cadc0ee3890&lt;/saml:Issuer&gt;\\n   &lt;samlp:Status&gt;\\n      &lt;samlp:StatusCode Value=\"urn:oasis:names:tc:SAML:2.0:status:Success\" /&gt;\\n   &lt;/samlp:Status&gt;\\n   &lt;saml:Assertion xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" Version=\"2.0\" ID=\"pfxd6935572-7039-d89c-e643-c11bbd71491d\" IssueInstant=\"2019-10-14T22:02:47Z\"&gt;\\n      &lt;saml:Issuer&gt;https://app.onelogin.com/saml/metadata/13e12235-b801-4ff4-85c6-5cadc0ee3890&lt;/saml:Issuer&gt;\\n      &lt;ds:Signature xmlns:ds=\"http://www.w3.org/2000/09/xmldsig#\"&gt;\\n         &lt;ds:SignedInfo&gt;\\n            &lt;ds:CanonicalizationMethod Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\" /&gt;\\n            &lt;ds:SignatureMethod Algorithm=\"http://www.w3.org/2000/09/xmldsig#rsa-sha1\" /&gt;\\n            &lt;ds:Reference URI=\"#pfxd6935572-7039-d89c-e643-c11bbd71491d\"&gt;\\n               &lt;ds:Transforms&gt;\\n                  &lt;ds:Transform Algorithm=\"http://www.w3.org/2000/09/xmldsig#enveloped-signature\" /&gt;\\n                  &lt;ds:Transform Algorithm=\"http://www.w3.org/2001/10/xml-exc-c14n#\" /&gt;\\n               &lt;/ds:Transforms&gt;\\n               &lt;ds:DigestMethod Algorithm=\"http://www.w3.org/2000/09/xmldsig#sha1\" /&gt;\\n               &lt;ds:DigestValue&gt;xxx&lt;/ds:DigestValue&gt;\\n            &lt;/ds:Reference&gt;\\n         &lt;/ds:SignedInfo&gt;\\n         &lt;ds:SignatureValue&gt;&lt;/ds:SignatureValue&gt;\\n         &lt;ds:KeyInfo&gt;\\n            &lt;ds:X509Data&gt;\\n               &lt;ds:X509Certificate&gt;ccccc&lt;/ds:X509Certificate&gt;\\n            &lt;/ds:X509Data&gt;\\n         &lt;/ds:KeyInfo&gt;\\n      &lt;/ds:Signature&gt;\\n      &lt;saml:Subject&gt;\\n         &lt;saml:NameID Format=\"urn:oasis:names:tc:SAML:2.0:nameid-format:transient\"&gt;jakobant@gmail.com&lt;/saml:NameID&gt;\\n         &lt;saml:SubjectConfirmation Method=\"urn:oasis:names:tc:SAML:2.0:cm:bearer\"&gt;\\n            &lt;saml:SubjectConfirmationData NotOnOrAfter=\"2019-10-14T22:05:47Z\" Recipient=\"{recipient}\" InResponseTo=\"_f39ff7cc60b3fb475bc0d4d1c51e8369fe3b99c4\" /&gt;\\n         &lt;/saml:SubjectConfirmation&gt;\\n      &lt;/saml:Subject&gt;\\n      &lt;saml:Conditions NotBefore=\"2019-10-14T21:59:47Z\" NotOnOrAfter=\"2019-10-14T22:05:47Z\"&gt;\\n         &lt;saml:AudienceRestriction&gt;\\n            &lt;saml:Audience&gt;{audience}&lt;/saml:Audience&gt;\\n         &lt;/saml:AudienceRestriction&gt;\\n      &lt;/saml:Conditions&gt;\\n      &lt;saml:AuthnStatement AuthnInstant=\"2019-10-14T22:02:46Z\" SessionNotOnOrAfter=\"2019-10-15T22:02:47Z\" SessionIndex=\"_47fbf670-d0fc-0137-c88e-31b1babba584\"&gt;\\n         &lt;saml:AuthnContext&gt;\\n            &lt;saml:AuthnContextClassRef&gt;urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport&lt;/saml:AuthnContextClassRef&gt;\\n         &lt;/saml:AuthnContext&gt;\\n      &lt;/saml:AuthnStatement&gt;\\n      &lt;saml:AttributeStatement&gt;\\n         &lt;saml:Attribute Name=\"groups\" NameFormat=\"urn:oasis:names:tc:SAML:2.0:attrname-format:basic\"&gt;\\n            &lt;saml:AttributeValue xsi:type=\"xs:string\"&gt;asdf&lt;/saml:AttributeValue&gt;\\n         &lt;/saml:Attribute&gt;\\n      &lt;/saml:AttributeStatement&gt;\\n   &lt;/saml:Assertion&gt;\\n&lt;/samlp:Response&gt;\\n'},\n",
       " {'text_field': 'After various trial and error, I gave up on the Cloud... for a while.\\nInstalled elasticsearch and kibana with docker-compose to get the debugging logs that I needed.\\n(in basic started from : http://codingfundas.com/setting-up-elasticsearch-6-8-with-kibana-and-x-pack-security-enabled/index.html with few tweeks)\\nEnable trial license... .i.e. https://www.elastic.co/guide/en/elasticsearch/reference/7.4/start-trial.html\\ndocker-compose.yml\\nversion: \\'3\\'\\nservices:\\n  elasticsearch:\\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.0\\n    environment:\\n      - discovery.type=single-node\\n      - logger.level=DEBUG\\n    ports:\\n      - 9200:9200\\n    volumes:\\n      - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml\\n      - ./elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12\\n      - ./docker-data-volumes/elasticsearch:/usr/share/elasticsearch/data\\n\\n  kibana:\\n    depends_on:\\n      - elasticsearch\\n    image: docker.elastic.co/kibana/kibana:7.4.0\\n    ports:\\n      - 5601:5601\\n    volumes:\\n      - ./kibana.yml:/usr/share/kibana/config/kibana.yml\\n\\nelasticsearch.yml\\ncluster.name: my-elasticsearch-cluster\\nnetwork.host: 0.0.0.0\\n#xpack.security.enabled: true\\nxpack.security.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.keystore.type: PKCS12\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.keystore.path: elastic-certificates.p12\\nxpack.security.transport.ssl.truststore.path: elastic-certificates.p12\\nxpack.security.transport.ssl.truststore.type: PKCS12\\nxpack.security.authc.token.enabled: true\\nxpack.security.authc.realms:\\n  saml:\\n    saml1:\\n      order: 2\\n      idp.metadata.path: \"https://app.onelogin.com/saml/metadata/xxxxx\"\\n      idp.entity_id: \"https://app.onelogin.com/saml/metadata/xxxxx\"\\n      sp.entity_id:  \"http://localhost:5601/\"\\n      sp.acs: \"http://localhost:5601/api/security/v1/saml\"\\n      sp.logout: \"http://localhost:5601/logout\"\\n      attributes.principal: \"nameid\"\\n      attributes.groups: \"memberOf\"\\n      attributes.name: \"name\"\\n\\nkibana.yml\\nserver.name: kibana\\nserver.host: \"0\"\\nelasticsearch.hosts: [ \"http://elasticsearch:9200\" ]\\nelasticsearch.username: kibana\\nelasticsearch.password: xxxxxxxxxxx\\n\\nxpack.security.authc.providers: [saml, basic]\\nserver.xsrf.whitelist: [/api/security/v1/saml]\\nxpack.security.authc.saml.realm: saml1\\n\\nOneLogin setup:\\n\\nScreenshot 2019-10-15 at 21.54.53.png1005×499 30.3 KB\\n\\n\\nScreenshot 2019-10-15 at 21.49.52.png708×1141 63.1 KB\\n\\nThe basic error that I was getting was because of the missing/lacking \"Audience\" and \"Recipient\" with the OneLogin configuration\\nThe lack of debugging from the cloud is not good.\\nImplemented for the elastic.co Cloud Elasticsearch .....\\nEnd results for the cloud:\\nxpack:\\n  security:\\n    authc:\\n      realms:\\n        saml: \\n          cloud-saml: \\n            order: 2\\n            attributes.principal: \"nameid\"\\n            attributes.groups: \"memberOf\"\\n            attributes.name: \"name\"\\n            idp.metadata.path: \"https://app.onelogin.com/saml/metadata/xxxxx\"\\n            idp.entity_id: \"https://app.onelogin.com/saml/metadata/xxxxx\"\\n            sp.entity_id: \"https://kibana....us-east-1.aws.found.io:9243/\"\\n            sp.acs: \"https://kibana.....us-east-1.aws.found.io:9243/api/security/v1/saml\"\\n            sp.logout: \"https://kibana....us-east-1.aws.found.io:9243/logout\"\\n\\nxpack.security.authc.providers: [saml, basic]\\nserver.xsrf.whitelist: [/api/security/v1/saml]\\nxpack.security.authc.saml.realm: cloud-saml\\n\\n'},\n",
       " {'text_field': \"Hi,\\nIn the Kibana Visualize field picker, I'm seeing hundreds of metricbeat fields that we have never used. We only use the system module and yet we see fields from every single metricbeat module. Is there any way to limit the fields that Kibana shows here? Where is Kibana getting this list from?\\nThanks.\\n\\nScreenshot from 2019-10-14 23-22-45.png1024×418 68.6 KB\\n\"},\n",
       " {'text_field': \"If that doesn't work, you'd have to update your mapping for your metricbeat indices and remove the fields that you don't use, and then go to Management &gt; Index Patterns and refresh your index pattern fields.\"},\n",
       " {'text_field': 'Hi.\\nI\\'m trying to parse the id filed in my message column so that I can group by id to have a number of unique transactions in my dashboard.\\nCurrently, the filter is:\\n    if [type] == \"web_industry_log\" {\\n        mutate {\\n            replace =&gt; { \\'host\\' =&gt; \\'appserver.datacentre.example.com\\' }\\n            add_field =&gt; { \\'environment\\' =&gt; \\'production\\'\\n                           \\'service\\' =&gt; \\'web_industry\\'\\n            }\\n        }\\n\\n        grok {\\n            match =&gt; { \\n                \"message\" =&gt; \"(?m)%{LOGLEVEL:log-level}%{SPACE}%{SPACE}%{TIMESTAMP_ISO8601:timestamp }%{SPACE}%{DATA:class}%{SPACE}-%{SPACE}%{GREEDYDATA:message}\"\\n            }\\n            overwrite =&gt; [ \\'message\\' ]\\n        }\\n\\n        date {\\n            match =&gt; [\"timestamp\", \"ISO8601\"]\\n            target =&gt; \"@timestamp\"\\n        }\\n    }\\n\\nLog output:\\n{\\n  \"_index\": \"elk-log-index-2019.w42\",\\n  \"_type\": \"_doc\",\\n  \"_id\": \"PM92zG0oBL5xOADC\",\\n  \"_version\": 1,\\n  \"_score\": null,\\n  \"_source\": {\\n    \"@timestamp\": \"2019-10-14T22:49:58.357Z\",\\n    \"@version\": \"1\",\\n    \"environment\": \"production\",\\n    \"timestamp\": \"2019-10-15 11:49:58,357\",\\n    \"host\": \"appserver.datacentre.example.com\",\\n    \"type\": \"web_industry_log\",\\n    \"message\": \"Transaction traderStockMonitor a19d5f | id: 1930808 timestamp: 2019-10-15T11:49:58.000+13:00 user: Customer name here Ltd (5648575) request address: null login id: null\",\\n    \"log-level\": \"INFO\",\\n    \"class\": \"transaction.TransactionDocumentPersisterImpl\",\\n    \"path\": \"/mnt/logs/app/industry/industry.log\",\\n    \"service\": \"web_industry\"\\n  },\\n  \"fields\": {\\n    \"@timestamp\": [\\n      \"2019-10-14T22:49:58.357Z\"\\n    ]\\n  },\\n  \"highlight\": {\\n    \"environment\": [\\n      \"@kibana-highlighted-field@production@/kibana-highlighted-field@\"\\n    ],\\n    \"service\": [\\n      \"@kibana-highlighted-field@web_industry@/kibana-highlighted-field@\"\\n    ],\\n    \"class.keyword\": [\\n      \"@kibana-highlighted-field@transaction.TransactionDocumentPersisterImpl@/kibana-highlighted-field@\"\\n    ],\\n    \"log-level.keyword\": [\\n      \"@kibana-highlighted-field@INFO@/kibana-highlighted-field@\"\\n    ]\\n  },\\n  \"sort\": [\\n    1571093398357\\n  ]\\n}\\n\\nSo, the message column \"message\": \"Transaction traderStockMonitor a19d5f | id: 1930808 timestamp: 2019-10-15T11:49:58.000+13:00 user: Customer name here Ltd (5648575) request address: null login id: null\",. I need to filter the id: number as well, so it is searchable and then I can group by later on.\\nHow to do that, please?\\nThanks!'},\n",
       " {'text_field': 'I\\'ve found the problem.\\nThis final syntax works:\\n\"message\" =&gt; \"(?m)%{LOGLEVEL:log-level}%{SPACE}%{SPACE}%{TIMESTAMP_ISO8601:timestamp } %{DATA:class} - Transaction %{WORD:transaction-type} %{WORD:transaction-reference} \\\\| id: %{NUMBER:transaction-id}%{GREEDYDATA:message}\"'},\n",
       " {'text_field': 'I have an elastic search schema as below.\\n{\\n  \"mappings\": {\\n    \"properties\": {\\n      \"process_id\": {\\n        \"type\": \"keyword\"\\n      },\\n      \"user_info\": {\\n        \"type\": \"nested\",\\n        \"properties\": {\\n          \"first_name\": {\\n            \"type\": \"text\",\\n            \"fields\": {\\n              \"keyword\": {\\n                \"type\": \"keyword\",\\n                \"ignore_above\": 256\\n              }\\n            }\\n          },\\n          \"last_name\": {\\n            \"type\": \"text\",\\n            \"fields\": {\\n              \"keyword\": {\\n                \"type\": \"keyword\",\\n                \"ignore_above\": 256\\n              }\\n            }\\n          },\\n          \"place\": {\\n            \"type\": \"text\",\\n            \"fields\": {\\n              \"keyword\": {\\n                \"type\": \"keyword\",\\n                \"ignore_above\": 256\\n              }\\n            }\\n          }\\n      \\n\\n2. I have added the below documents to the index.\\nPOST processes/_bulk \\n  {\"index\":{\"_id\":1}}\\n{\"process_id\": \"123\", \"user_info\": [{\"first_name\": \"A\", \"last_name\": \"B\",\"place\":\"London\"},{\"first_name\": \"C\", \"last_name\": \"D\",\"place\":\"Moscow\"}]}\\n{\"index\":{\"_id\":2}}\\n{\"process_id\": \"123\", \"user_info\": [{\"first_name\": \"C\", \"last_name\": \"B\",\"Place\":\"Delhi\"},{\"first_name\": \"A\", \"last_name\": \"D\",\"Place\":\"Bangalore\"}]}\\n\\n3. I wanted to search for process_id as 123,  first_name as A and last_name as B, and and aggregate on Place of the user_info. I am trying to do it as below.\\n{   \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"query_string\": {\\n            \"query\": \"process_id:123\"\\n          }\\n        },\\n        {\\n          \"nested\": {\\n            \"path\": \"user_info\",\\n            \"query\": {\\n              \"query_string\": {\\n                \"query\": \"user_info.first_name:A AND user_info.last_name:B\"\\n              }\\n            }\\n          }\\n        }\\n      ]\\n    }   },   \"aggs\": {\\n    \"user_info\": {\\n      \"nested\": {\\n        \"path\": \"user_info\"\\n      },\\n      \"aggs\": {\\n        \"user_info.place\": {\\n          \"terms\": {\\n            \"field\": \"user_info.place\"\\n          }\\n        }\\n      }\\n    }   } }\\n\\nThe result returns the first document correctly. However, the aggregate on the field place returns both London and Moscow. However, I want London to be returned as the output. I am not sure how to proceed with this. I have tried Nested Filter Search Aggregation as well but no success on the same.'},\n",
       " {'text_field': 'Pls check\\n\\n  \\n      stackoverflow.com\\n  \\n  \\n      \\n    \\n  \\n\\n  Elasticsearch - How to filter nested aggregation bucket?\\n\\n\\n\\n  elasticsearch\\n\\n\\n\\n  \\n  answered by\\n  \\n    Val\\n  \\n  on 04:21PM - 21 Dec 16 UTC\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n'},\n",
       " {'text_field': 'Hi.\\nI need to parse the customer name, but there are cases when the customer name has one, two or even three words.\\nExample 1: Customer name with 3 words\\nINFO 2019-10-15 16:34:41,964 transaction.TransactionDocumentPersisterImpl - Transaction chassisCheckPlusRedbook bda19842 | id: 1938978 timestamp: 2019-10-15T16:34:41.000+13:00 user: Luke test name (13756439) request address: 8.8.8.8 login id: 193897\\nExample 2: Customer name with 2 words\\nINFO 2019-10-15 16:34:41,964 transaction.TransactionDocumentPersisterImpl - Transaction chassisCheckPlusRedbook bda19842 | id: 1938978 timestamp: 2019-10-15T16:34:41.000+13:00 user: Luke test (13756439) request address: 8.8.8.8 login id: 193897\\nUsing the option NOTSPACE works.. but only for the times specified.\\ntimestamp: %{TIMESTAMP_ISO8601:timestamp} user: %{NOTSPACE:customer} %{NOTSPACE:customer} %{NOTSPACE:customer} \\\\(%{NUMBER:customer-id:int}\\\\)\\nThe above string will only work when the customer name has 3 words.\\nHow can I have something smart enough that, everything between user: and \\\\(%{NUMBER:customer-id:int}\\\\) is the customer name?'},\n",
       " {'text_field': 'You could use\\n(?&lt;customer&gt;[a-zA-Z ]+)'},\n",
       " {'text_field': 'Hi,\\nLogstash ingest value per 120s aprox and when i do it a visualize and put poor timestamp i see only \"dots\" (picture B) per 120s (obviusly) but when i choose big timestamp points don´t see (picture A).\\nI would like \"merge dots\" and see lines, it is possible¿?\\n\\na.png918×255 30.9 KB\\n\\n\\nb.png998×273 16.2 KB\\n'},\n",
       " {'text_field': 'Solved.\\nChange TSVB interval timestamp \"auto\" to \"2m\".'},\n",
       " {'text_field': 'Hey,\\nwe sitting here in a Study-Project in Germany and want to work with packetbeat and ELK to find security issues in network-traffic. The live-capturing works fine.\\nNow, we want to import our old PCAP-files from the last 3 years (approx. 3GB/day). If we use\\n\"packetbeat run -I \"PCAP-FILE\" -t \"\\nwe see captured packets in Kibana, but only 700 packets per 30 minutes. There should be much more! It seems that packetbeat stops after this 700 packets.\\nCan somebody help us to solve this problem?\\nThanks!!!!\\n---Update ----\\nThe only logs we see in Kibana are ICMP-Packets\\nIf we use live-capturing, we see all the traffic. We use the same config-file.'},\n",
       " {'text_field': 'The timestamp issue is a known problem. This is one of the reasons why the PCAP processing is marked as only useful for testing Packetbeat itself.\\nWe have a pending issue to improve pcap file processing to make it more useful:\\n\\n  \\n      github.com/elastic/beats\\n  \\n  \\n    \\n\\n\\n\\nIssue: [meta] pcap file improvements\\n\\n\\n\\t\\n\\topened by adriansr\\n\\ton 2018-09-06\\n\\t\\n\\t\\n\\t\\n\\n\\nUsers regularly use .pcap file ingestion in packetbeat even though it is described as \"useful only for testing Packetbeat\" in the...\\n\\n\\n \\tPacketbeat\\n \\tdiscuss\\n \\tgood first issue\\n \\tmeta\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n'},\n",
       " {'text_field': 'Hi, my output config is:\\noutput {\\nelasticsearch {\\nhosts =&gt; [\"localhost\"]\\nmanage_template =&gt; true\\nindex =&gt; \"logstash-%{+YYYY.MM.dd}\"\\n}\\n}\\nIt creates a index like \"logstash-2019.10.14-000001\" and doesn\\'t create new index per day.\\nwhat is the -000001? and why it doesn\\'t create new index per day?\\nI didn\\'t set any Rollup job and I just wanna have index per day.\\nI use elk container with logstash and elasticsearch version 7.4.0\\nThanx'},\n",
       " {'text_field': \"Does setting the 'ilm_enabled =&gt; false' option help?\"},\n",
       " {'text_field': 'i can\\'t use elasticsearch\\ncheck service elasticsearch and kibana status active\\n{\"error\":{\"root_cause\":[{\"type\":\"circuit_breaking_exception\",\"reason\":\"[parent] Data too large, data for [&lt;http_request&gt;] would be [2985537059/2.7gb], which is larger than the limit of [2982071500/2.7gb], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, accounting=2985537059/2.7gb]\",\"bytes_wanted\":2985537059,\"bytes_limit\":2982071500}],\"type\":\"circuit_breaking_exception\",\"reason\":\"[parent] Data too large, data for [&lt;http_request&gt;] would be [2985537059/2.7gb], which is larger than the limit of [2982071500/2.7gb], usages [request=0/0b, fielddata=0/0b, in_flight_requests=0/0b, accounting=2985537059/2.7gb]\",\"byt'},\n",
       " {'text_field': 'Thank You for reply\\nI solved by adjusting up jvm.option'},\n",
       " {'text_field': 'got a working 6.8 cluster ingesting data into indicies with templates that defines some date fields like this:\\n  \"os_installed\": {\\n    \"type\": \"date\",\\n    \"format\": \"MM/DD/YYYY||MM\\\\/DD\\\\/YYYY||MM\\\\\\\\/DD\\\\\\\\/YYYY||strict_year_month_day||strict_date_optional_time||epoch_millis\"\\n  },\\n\\nAs prep for upgrading to 7.x I\\'m trying to alter templates like this:\\n  \"os_installed\": {\\n    \"type\": \"date\",\\n    \"format\": \"8MM/DD/yyyy||MM\\\\/DD\\\\/yyyy||MM\\\\\\\\/DD\\\\\\\\/yyyy||strict_year_month_day||strict_date_optional_time||epoch_millis\"\\n  },\\n\\nbut only gets a fraction of doc indexed giving dates like this in the index:\\n\"os_installed\": \"01\\\\\\\\/03\\\\\\\\/2018\"\\n\\nwhile majority of docs fails to get indexed generating error like this:\\n[2019-10-15T10:12:48,589][DEBUG][o.e.a.b.TransportShardBulkAction] [d1r1n1] [tanium_basic_inventory-2019.10.15][2] failed to execute bulk item (index) index {[tanium_basic_inventory-2019.10.15][_doc][Zxx6zm0BLyt5VBipDnsa], source[{...redacted...,\"os_installed\":\"03\\\\\\\\/26\\\\\\\\/2014\",...redacted...\"@timestamp\":\"2019-10-15T08:12:32.92+00:00\"}]}\\norg.elasticsearch.index.mapper.MapperParsingException: failed to parse field [os_installed] of type [date] in document with id \\'Zxx6zm0BLyt5VBipDnsa\\'\\n...\\nCaused by: java.lang.IllegalArgumentException: failed to parse date field [03\\\\/26\\\\/2014] with format [8MM/DD/yyyy||MM/DD/yyyy||MM\\\\/DD\\\\/yyyy||strict_year_month_day||strict_date_optional_time||epoch_millis]\\n        at org.elasticsearch.common.time.JavaDateFormatter.parse(JavaDateFormatter.java:116) ~[elasticsearch-6.8.1.jar:6.8.1]\\n        at org.elasticsearch.common.time.DateFormatter.parseMillis(DateFormatter.java:54) ~[elasticsearch-6.8.1.jar:6.8.1]\\n        at org.elasticsearch.index.mapper.DateFieldMapper$DateFieldType.parse(DateFieldMapper.java:246) ~[elasticsearch-6.8.1.jar:6.8.1]\\n        at org.elasticsearch.index.mapper.DateFieldMapper.parseCreateField(DateFieldMapper.java:454) ~[elasticsearch-6.8.1.jar:6.8.1]\\n        at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:297) ~[elasticsearch-6.8.1.jar:6.8.1]\\n\\nI\\'m also confused about howto define Java data format specifications in my templates and if such format will continue to work in ES 7.x directly or needs changing in 7.x and if to what \\'y\\' -&gt; \\'u\\' and \\'8\\' -&gt; \\'\\'?\\nTried also this with similar bad ratio between indexed/non-indexed docs:\\nCaused by: java.lang.IllegalArgumentException: failed to parse date field [09\\\\/02\\\\/2016] with format [8MM/DD/uuuu||MM/DD/uuuu||MM\\\\/DD\\\\/uuuu||strict_year_month_day||strict_date_optional_time||epoch_millis]\\n\\nTIA for any hints!'},\n",
       " {'text_field': 'Using DD means you want to extract the current day of the year (sth between 1 and 366) and not the current day of the month (which would have been dd). The day of the year overlaps with specifying the month, as both set the month of the year and this is why this error occurs.\\nfor the first example, can you provide a couple of formats for testing to dig into this?'},\n",
       " {'text_field': 'Hello,\\nI am getting gateway timeout error while discovering the index in kibana, but for the same index, I am able to see dashboard and visualization.\\n\\nThanks in advance.'},\n",
       " {'text_field': \"Hey @Ranjith_B_K,\\nMaybe Kibana has a very slow connection to Elasticsearch and can't retrieve all the records Discover wants to return (500 by default). Have you tried reducing the value of discover:sampleSize  in Kibana Advanced Settings?\\nBest,\\nOleg\"},\n",
       " {'text_field': 'Hello there,\\nI have a log file (given by the SLURM workload manager) that looks something like this:\\n2019-07-15 13:26:40 Started - job id: V3gNDm1MD7unkvVVEmSjiJLoABFKDmABFKDmtdGKDmABFKDmN8Erlm, unix user: 45001:45000, name: \"org.nordugrid.ARC-CE-result-ops\", owner: \"/DC=EU/DC=EGI/C=GR/O=Robots/O=Greek Research and Technology Network/CN=Robot:argo-egi@grnet.gr\", lrms: SLURM, queue: parallel1\\n2019-07-15 13:31:10 Finished - job id: V3gNDm1MD7unkvVVEmSjiJLoABFKDmABFKDmtdGKDmABFKDmN8Erlm, unix user: 45001:45000, name: \"org.nordugrid.ARC-CE-result-ops\", owner: \"/DC=EU/DC=EGI/C=GR/O=Robots/O=Greek Research and Technology Network/CN=Robot:argo-egi@grnet.gr\", lrms: SLURM, queue: parallel1, lrmsid: 66918\\n\\nBasically pairs of lines with \"Started\" - \"Finished\" status which correspond to a unique job_id.\\nI want to parse the data into a JSON format and also I want to add a new field for each \"Finished\" line with the job duration (which is given by subtracting the time in Finished line and the time from Started line.\\nI\\'ve started to use logstash filtering last week, so I am quite new to this. From my research, I found that ruby plug-in could help me, but so far I didn\\'t manage to get a non-zero value for the job duration calculus.\\nThis is my code:\\nfilter {  #filtering process for SLURM log file       \\n        grok {  #DEFINED PATTERN FOR TXT FILE              \\n                match =&gt; { \"message\" =&gt; [\"%{TIMESTAMP_ISO8601:[JOB][job_Details][Date1]} %{WORD:[JOB][job_status]} [-] %{DATA}[:] %{DATA:[JOB][job_id]}[,] %{DATA:[JOB][job_Details][user_Type]}[:] %{DATA:[JOB][job_Details][user_ID]}[,] %{DATA}[:] \\\\\"%{DATA:[JOB][job_Details][user_Name]}\\\\\"[,] %{DATA}: \\\\\"%{DATA:[JOB][job_Details][owner]}\\\\\"[,] %{DATA}[:] %{DATA:[JOB][job_Details][Workload_Manager_Type]}[,] %{DATA}[:] %{DATA:[JOB][job_Details][queue_Type]}[,] %{DATA}[:] %{NUMBER:[JOB][job_Details][Workload_Manager_id]}\",\"%{TIMESTAMP_ISO8601:[JOB][job_Details][Date2]} %{WORD:[JOB][job_status]} [-] %{DATA}[:] %{DATA:[JOB][job_id]}[,] %{DATA:[JOB][job_Details][user_Type]}[:] %{DATA:[JOB][job_Details][user_ID]}[,] %{DATA}[:] \\\\\"%{DATA:[JOB][job_Details][user_Name]}\\\\\"[,] %{DATA}: \\\\\"%{DATA:[JOB][job_Details][owner]}\\\\\"[,] %{DATA}[:] %{DATA:[JOB][job_Details][Workload_Manager_Type]}[,] %{DATA}[:] %{GREEDYDATA:[JOB][job_Details][queue_Type]}\" ]\\n                }\\n                remove_field =&gt; [\"message\",\"host\",\"@version\",\"path\"]\\n        }\\n        date {\\n                match =&gt; [\"[JOB][job_Details][Date1]\", \"yyyy-MM-dd HH:mm:ss\"]\\n                target =&gt; \"[JOB][job_Details][Finish_Time]\"\\n            }\\n        date {      \\n                match =&gt; [\"[JOB][job_Details][Date2]\", \"yyyy-MM-dd HH:mm:ss\"]\\n                target =&gt; \"[JOB][job_Details][Start_Time]\"\\n            }\\nruby {\\n        init =&gt; \"require \\'time\\'\"\\n        code =&gt; \"duration=(Time.parse(event.get(\\'[JOB][job_Details][Date1]\\'))-Time.parse(event.get(\\'[JOB][job_Details][Date2]\\'))) rescue nil; event.set(\\'[JOB][job_Details][PROCESS_DURATION]\\',duration);\"\\n}\\n\\nAnd my output:\\n{\\n    \"@timestamp\": \"2019-10-15T09:07:29.057Z\",\\n    \"JOB\": {\\n        \"job_id\": \"V3gNDm1MD7unkvVVEmSjiJLoABFKDmABFKDmtdGKDmABFKDmN8Erlm\",\\n        \"job_Details\": {\\n            \"user_ID\": \"45001:45000\",\\n            \"owner\": \"/DC=EU/DC=EGI/C=GR/O=Robots/O=Greek Research and Technology Network/CN=Robot:argo-egi@grnet.gr\",\\n            \"user_Type\": \"unix user\",\\n            \"PROCESS_DURATION\": null,\\n            \"queue_Type\": \"parallel1\",\\n            \"Workload_Manager_Type\": \"SLURM\",\\n            \"Start_Time\": \"2019-07-15T10:26:40.000Z\",\\n            \"user_Name\": \"org.nordugrid.ARC-CE-result-ops\"\\n        },\\n        \"job_status\": \"Started\"\\n    }\\n} {\\n    \"@timestamp\": \"2019-10-15T09:07:29.057Z\",\\n    \"JOB\": {\\n        \"job_id\": \"V3gNDm1MD7unkvVVEmSjiJLoABFKDmABFKDmtdGKDmABFKDmN8Erlm\",\\n        \"job_Details\": {\\n            \"user_ID\": \"45001:45000\",\\n            \"owner\": \"/DC=EU/DC=EGI/C=GR/O=Robots/O=Greek Research and Technology Network/CN=Robot:argo-egi@grnet.gr\",\\n            \"Workload_Manager_id\": \"66918\",\\n            \"user_Type\": \"unix user\",\\n            \"PROCESS_DURATION\": null,\\n            \"queue_Type\": \"parallel1\",\\n            \"Workload_Manager_Type\": \"SLURM\",\\n            \"Finish_Time\": \"2019-07-15T10:31:10.000Z\",\\n            \"user_Name\": \"org.nordugrid.ARC-CE-result-ops\"\\n        },\\n        \"job_status\": \"Finished\"\\n    }\\n}\\n\\nAs you can see, I can add the \"PROCESS_DURATION field to the lines (although I would like to add it only to the \"Finished\" line) but the result is null.\\nWhat am I doing wrong here?\\nThank you in advance:)'},\n",
       " {'text_field': 'I cannot see what you did wrong there, but I would do it in a different way, using aggregate.\\n    dissect { mapping =&gt; { \"message\" =&gt; \"%{[@metadata][timestamp]} %{+[@metadata][timestamp]} %{event} - %{[@metadata][restOfLine]}\" } }\\n    date { match =&gt; [ \"[@metadata][timestamp]\", \"YYYY-MM-dd HH:mm:ss\" ] }\\n    kv { source =&gt; \"[@metadata][restOfLine]\" target =&gt; \"[@metadata][keys]\" field_split =&gt; \",\" value_split =&gt; \":\" remove_char_key =&gt; \" \" }\\n    aggregate {\\n        task_id =&gt; \"%{[@metadata][keys][jobid]}\"\\n        code =&gt; \\'\\n            keys = event.get(\"[@metadata][keys]\")\\n            if keys\\n                keys.each { |k, v|\\n                    map[k] = v\\n                }\\n            end\\n\\n            e = event.get(\"event\")\\n            map[\"time#{e}\"] = event.get(\"@timestamp\")\\n\\n            event.cancel\\n        \\'\\n        push_map_as_event_on_timeout =&gt; true\\n        timeout =&gt; 3\\n        timeout_code =&gt; \\'\\n            event.set(\"duration\", event.get(\"timeFinished\").to_f - event.get(\"timeStarted\").to_f)\\n        \\'\\n    }\\n\\nwill produce\\n    \"@version\" =&gt; \"1\",\\n       \"owner\" =&gt; \"/DC=EU/DC=EGI/C=GR/O=Robots/O=Greek Research and Technology Network/CN=Robot:argo-egi@grnet.gr\",\\n      \"lrmsid\" =&gt; \"66918\",\\n       \"queue\" =&gt; \"parallel1\",\\n \"timeStarted\" =&gt; 2019-07-15T13:26:40.000Z,\\n  \"@timestamp\" =&gt; 2019-10-15T15:22:13.905Z,\\n        \"name\" =&gt; \"org.nordugrid.ARC-CE-result-ops\",\\n\"timeFinished\" =&gt; 2019-07-15T13:31:10.000Z,\\n    \"unixuser\" =&gt; \"45001:45000\",\\n        \"lrms\" =&gt; \"SLURM\",\\n       \"jobid\" =&gt; \"V3gNDm1MD7unkvVVEmSjiJLoABFKDmABFKDmtdGKDmABFKDmN8Erlm\",\\n    \"duration\" =&gt; 270.0\\n\\nObviously you can move the keys under [JOB][job_Details] using string interpolation in the keys.each loop as I do in storing the timestamps.'},\n",
       " {'text_field': 'I want to create a table in the Visualize tool in Kibana where I show several metrics with condition and calculcation. I have created a generic example in excel, see below. I know the basics. On how to produce the first two columns, be the other ones are harder. I tried looking into adding JSON input with adding another Count and adding a script, but i dont get it to work unfortunately. Any ideas?\\n\\nimage.png1073×111 3.47 KB\\n'},\n",
       " {'text_field': \"Hi. I tried Canvas with ES-SQL before but it has limitations in the queries (ORDER BY gives timeouts for example).\\nI did actually solve it in Visualize by myself by using the table in Visual Builder, utilizing Filter Ratio for the fourth column and a basic Filter for third column (using multiple Metrics just to be clear). Then, I just added the visualization in a dashboard.It's not perfect, but it suit my needs as of now.\"},\n",
       " {'text_field': '以下のdocker-compose.ymlとDockerfile、elasticsearch.ymlの設定で\\nコンテナを作成しました。(使用するポート番号は9600)\\nそこでlocalhost:9600にアクセスしてもERR_EMPTY_RESPONSEとなってしまいます。\\n設定ファイルのパラメータに何か不足があるのでしょうか？\\ndocker-compose.yml\\n\\nversion: \\'2.2\\'\\nservices:\\nelasticsearch7-4:\\nbuild: elasticsearch-ver7.4.0-kuromoji\\ncontainer_name: elasticsearch7-4\\nenvironment:\\n- node.name=es01\\n- cluster.initial_master_nodes=es01\\n- cluster.name=docker-cluster-es7-4\\n- bootstrap.memory_lock=true\\n- \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\\nulimits:\\nmemlock:\\nsoft: -1\\nhard: -1\\nvolumes:\\n- esdata7-4:/usr/share/elasticsearch-7-4/data\\nports:\\n- 9600:9600\\nvolumes:\\nesdata7-4:\\ndriver: local\\n\\nDockerfile\\n\\nFROM docker.elastic.co/elasticsearch/elasticsearch:7.4.0\\nRUN elasticsearch-plugin install analysis-kuromoji\\nADD config/elasticsearch.yml /usr/share/elasticsearch-7-4/config/\\nRUN chown elasticsearch:elasticsearch config/elasticsearch.yml\\n\\nelasticsearch.yml\\n\\ncluster.name: \\'docker-cluster-es7-4\\'\\nnetwork.host: 0.0.0.0\\ndiscovery.zen.minimum_master_nodes: 1\\nxpack.security.enabled: false\\n'},\n",
       " {'text_field': \"\\nこの設定が反映されるのはelasticsearch.ymlではなくdocker-compose.ymlなのはなぜなのでしょうか？\\n\\nhttp.portの設定をelasticsearch.ymlに設定した場合でも、もちろん設定は反映されます。\\n以前提示された内容ですと、elasticsearch.ymlが意図しない場所に置かれてしまっていますので正しい場所に配置するようにします。\\n（configファイルの読み先を変えるような設定をしていない前提です）\\nそうすれば、設定ファイルで指定したhttp.portの値が有効になっていることが確認できるかと思います。\\n\\nDockerfile例（COPY行を見てください。 usr/share/elasticsearch/config/elasticsearch.ymlに設定ファイルをおいています）\\n\\nFROM docker.elastic.co/elasticsearch/elasticsearch:7.4.0\\n\\nRUN elasticsearch-plugin install analysis-kuromoji\\nCOPY config/elasticsearch.yml /usr/share/elasticsearch/config/elasticsearch.yml\\nRUN chown elasticsearch:elasticsearch config/elasticsearch.yml\\n\\n次に、docker-composeでの環境変数の指定とelasticsearch.ymlの使い分けについてです。\\nどういう使い方をされたいのかという用途や目的によって考え方は変わるかと思います。\\n例えば、ベースとなるような共通的な設定はelasticsearch.ymlにおいておき、\\n個々のノードごとにちょっとずつ違う設定をしたい場合は、個々にymlファイルを用意するのも面倒と考えれば、環境変数で動的に変更できるようにしておくことも考えられます。\\nもちろん環境変数ではなく、volume mountでymlファイルを渡すという考え方もあるかと思います。\\nversion: '2'\\nservices:\\n  elasticsearch:\\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.4.0\\n    container_name: elasticsearch\\n    environment:\\n      - cluster.name=docker-cluster\\n      - discovery.type=single-node\\n      - bootstrap.memory_lock=true\\n    ulimits:\\n      memlock:\\n        soft: -1\\n        hard: -1\\n    volumes:\\n      # ↓ イメージに含めるのではなく、コンテナ起動時に外から渡す形にする\\n      - ./config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml``\\n\\nこれも、どのぐらいのコンテナ数があるのか、どのぐらいelasticsearch.ymlのパターンがありそうなのかによってどれが良いのかは変わってくるかと思うので、\\nどうすべき、ということは一概には言えないかと。\\nご参考になれば幸いです。\"},\n",
       " {'text_field': 'Всем привет.\\nВ /etc/logstash/conf.d/postfix.conf\\nhttp://pastebin.calculate-linux.ru/ru/show/127900\\nВ /etc/logstash/patterns/postfix\\nhttp://pastebin.calculate-linux.ru/ru/show/127901\\nНо в логах logstash\\nCould not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"buildlog4\", :_type=&gt;\"_doc\", :routing=&gt;nil}, #&lt;LogStash::Event:0x49522875&gt;], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"buildlog4\", \"_type\"=&gt;\"_doc\", \"_id\"=&gt;\"tmgaz20Bp3jq-MOqGvqp\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"failed to parse field [host] of type [text] in document with id \\'tmgaz20Bp3jq-MOqGvqp\\'. Preview of field\\'s value: \\'{name=elk.mydomain.com}\\'\", \"caused_by\"=&gt;{\"type\"=&gt;\"illegal_state_exception\", \"reason\"=&gt;\"Can\\'t get text on a START_OBJECT at 1:357\"}}}}}\\n\\nВ настройках filebeat\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n      - /var/log/maillog*\\n  exclude_files: [\".gz$\"]\\noutput.logstash:\\n  hosts: [\"10.50.11.8:5044\"]\\n\\nТо есть postifx лезет в во все индексы. Как настроить, чтобы postfix был только в своём индексе ?\\nЕсли output конфига logstash сделать:\\noutput {\\nif \"postfix\" in [tags]{\\n        elasticsearch {\\n            hosts    =&gt; \"localhost:9200\"\\n            index    =&gt; \"postfix-%{+YYYY.MM.dd}\"\\n        }\\n}\\n}\\n\\nИ в filebeat.yml сделать :\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n      - /var/log/maillog*\\n  exclude_files: [\".gz$\"]\\ntags: [\"postfix\"]\\noutput.logstash:\\n  hosts: [\"10.50.11.8:5044\"]\\n\\nЭто не поможет. Можете помочь ?'},\n",
       " {'text_field': 'Спасибо. Я прочитав доку, понял что это делается так\\nmutate {\\n   rename =&gt; [\"host\", \"server\"]\\n   convert =&gt; {\"server\" =&gt; \"string\"} \\n}\\n\\nТам индексов с полями host больше десятка.\\nКак это использовать в моём случае и почему не стоит этого делать ?'},\n",
       " {'text_field': 'Hi All,\\nI´veen facing issue with an elasticsearch cluster, that first out of the blue, and later after a restart failed to allocate an index.\\nThe cluser has a single node atm and replication is disabled.\\nWhen I check allocation status I see:\\nGET {{elastic}}:9200/_cluster/allocation/explain?pretty&amp;include_yes_decisions=true\\n&gt; {\\n&gt;   \"index\": \"logstash-2019.10.04\",\\n&gt;   \"shard\": 0,\\n&gt;   \"primary\": true,\\n&gt;   \"current_state\": \"unassigned\",\\n&gt;   \"unassigned_info\": {\\n&gt;     \"reason\": \"ALLOCATION_FAILED\",\\n&gt;     \"at\": \"2019-10-15T12:25:27.278Z\",\\n&gt;     \"failed_allocation_attempts\": 5,\\n&gt;     \"details\": \"failed shard on node [gjegRSM1Rbi22HYJOFYINw]: failed recovery, failure RecoveryFailedException[[logstash-2019.10.04][0]: Recovery failed on {3730edbdef97}{gjegRSM1Rbi22HYJOFYINw}{QP5gwwEgTKm44DzJisNfaQ}{xxx.17.0.2}{xxx.17.0.2:9300}{ml.machine_memory=4294967296, xpack.installed=true, ml.max_open_jobs=20}]; nested: IndexShardRecoveryException[failed to recover from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/nodes/0/indices/np3g96ylRuGrsJKZ4Zo2LA/0/index/_17i.fdt]; \",\\n&gt;     \"last_allocation_status\": \"no\"\\n&gt;   },\\n&gt;   \"can_allocate\": \"no\",\\n&gt;   \"allocate_explanation\": \"cannot allocate because allocation is not permitted to any of the nodes that hold an in-sync shard copy\",\\n&gt;   \"node_allocation_decisions\": [\\n&gt;     {\\n&gt;       \"node_id\": \"gjegRSM1Rbi22HYJOFYINw\",\\n&gt;       \"node_name\": \"3730edbdef97\",\\n&gt;       \"transport_address\": \"xxx.17.0.2:9300\",\\n&gt;       \"node_attributes\": {\\n&gt;         \"ml.machine_memory\": \"4294967296\",\\n&gt;         \"xpack.installed\": \"true\",\\n&gt;         \"ml.max_open_jobs\": \"20\"\\n&gt;       },\\n&gt;       \"node_decision\": \"no\",\\n&gt;       \"store\": {\\n&gt;         \"in_sync\": true,\\n&gt;         \"allocation_id\": \"Xs0xcNPyQdORV_hS5JUkHg\"\\n&gt;       },\\n&gt;       \"deciders\": [\\n&gt;         {\\n&gt;           \"decider\": \"max_retry\",\\n&gt;           \"decision\": \"NO\",\\n&gt;           \"explanation\": \"shard has exceeded the maximum number of retries [5] on failed allocation attempts - manually call [/_cluster/reroute?retry_failed=true] to retry, [unassigned_info[[reason=ALLOCATION_FAILED], at[2019-10-15T12:25:27.278Z], failed_attempts[5], delayed=false, details[failed shard on node [gjegRSM1Rbi22HYJOFYINw]: failed recovery, failure RecoveryFailedException[[logstash-2019.10.04][0]: Recovery failed on {3730edbdef97}{gjegRSM1Rbi22HYJOFYINw}{QP5gwwEgTKm44DzJisNfaQ}{xxx.17.0.2}{xxx.17.0.2:9300}{ml.machine_memory=4294967296, xpack.installed=true, ml.max_open_jobs=20}]; nested: IndexShardRecoveryException[failed to recover from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/usr/share/elasticsearch/data/nodes/0/indices/np3g96ylRuGrsJKZ4Zo2LA/0/index/_17i.fdt]; ], allocation_status[deciders_no]]]\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"replica_after_primary_active\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"shard is primary and can be allocated\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"enable\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"all allocations are allowed\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"node_version\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"the primary shard is new or already existed on the node\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"snapshot_in_progress\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"no snapshots are currently running\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"restore_in_progress\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"ignored as shard is not being recovered from a snapshot\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"filter\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"node passes include/exclude/require filters\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"same_shard\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"the shard does not exist on the same node\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"disk_threshold\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"there is only a single data node present\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"throttling\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"below primary recovery limit of [4]\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"shards_limit\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"total shard limits are disabled: [index: -1, cluster: -1] &lt;= 0\"\\n&gt;         },\\n&gt;         {\\n&gt;           \"decider\": \"awareness\",\\n&gt;           \"decision\": \"YES\",\\n&gt;           \"explanation\": \"allocation awareness is not enabled, set cluster setting [cluster.routing.allocation.awareness.attributes] to enable it\"\\n&gt;         }\\n&gt;       ]\\n&gt;     }\\n&gt;   ]\\n&gt; }\\n\\nI tried running:\\n\\nPOST {{elastic}}:9200/_cluster/reroute?retry_failed\\n\\nAnd when I run:\\n\\n{{elastic}}:9200/_cat/shards\\n\\nI see the indices as initilizing for a big, but then they change to unasigned again\\nI also tried using lucene check to fix the index like this:\\n\\n/usr/share/elasticsearch/jdk/bin/java -cp \"*\" -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex /usr/share/elasticsearch/data/nodes/0/indices/np3g96ylRuGrsJKZ4Zo2LA/0/index/\\n\\nAnd it runs reporting no problems.\\nAny ideas?\\nThanks in advance.'},\n",
       " {'text_field': \"Ok, interesting, we're hitting this exception trying to delete these files which at least means they don't contain any data that isn't held elsewhere.\\nHowever I think this filesystem is in an inconsistent state and I wouldn't recommend trusting it with anything important. It's probably a good idea to take a snapshot ASAP in case it starts behaving even worse. You must at least run a fsck to try and fix any inconsistencies, although this might lose some other data. Alternatively move all your data onto a known-good filesystem and wipe this one.\"},\n",
       " {'text_field': \"Hi everyone,\\nI have setting up an ELK server, here it is my configuration :\\n\\nLXC container under Proxmox 6.0-7\\nOS : Debian 10 Buster\\nElasticsearch / logstash / kibana release 7.4.0\\nRAM : 64 Gb\\nJVM Heap ( elasticsearch and logstash jvm.option ) : 32 GB\\ndocs : about 82 000 000 per day\\n5 indices with 1 shard / 0 replicas per doc\\n\\nWhen I want to search log in a time range which is more than 24H, I have this error message : Data might be incomplete because your request timed out\\nI try differents settings, like increase kibana timeout from 3000 to 120000 ms, but the time out error appears before 30 seconds so I don't thinks it's the problem.\\nI try to increase JVM ressources, elasticsearch use more RAM but I still have the same error if I try to look for more than 24 hours.\\nI'm looking for people who meet the same issues, but each time this is on old versions.\\nIs anyone know why this timeout happend ? May I have to add more ressource on my server ?\\nThanks for your help. Regards.\"},\n",
       " {'text_field': \"Hi,\\nFor information, I find whats could be the problem ...\\nI just increase the availible RAM of elasticsearch JVM ( /etc/elasticsearch/jvm.options ). By default, it's seting ip to 4Gb, which is absolutly not enough for requestion several billions of docs. I increase it to 48 Gb. Now, I can request for few days, if there is no more 7 days, I can finalise my request without errors. If it can help someone...\\nThanks for the answers.\\nRegards.\"},\n",
       " {'text_field': 'I can set size and age, to remove documents. But I want to skip someone if has special field, for example\\n\\nPostpayment: true\\n\\nnever remove docs with field Postpayment, It`s possible?'},\n",
       " {'text_field': \"You couldn't call it from the internal ILM scheduling, but you could do it from a Cron job. You may want to configure an additional part of the query to not delete data from the newer indices (that is dependent on your retention needs)\"},\n",
       " {'text_field': 'Hi ELK Sensei (s),\\nI recently upgraded to Elasticsearch 7.4.0 because I want to take advantage of the SQL-like queries. But I’m hitting issues with the CLI interface script.\\nFirst off: I’m on an Ubuntu box, 16.04.4. My ES is the Docker variety; my version of Docker is 17.09.0-ce, build afdb6d4. My instance of Elasticsearch is running smoothly, with data populating very nicely.\\nOkay, I went to run the “elasticsearch-sql-cli” script, and here’s what happened:\\n[root@1234567890ab bin]# pwd\\n/usr/share/elasticsearch/bin\\n[root@1234567890ab bin]#\\n[root@1234567890ab bin]# ./elasticsearch-sql-cli https://elastic@127.0.0.1:9200\\n./elasticsearch-sql-cli: line 9: ./x-pack-env: No such file or directory\\n[root@1234567890ab bin]#\\n\\nWeird thing is, the “x-pack-env” file IS in this directory:\\n[root@1234567890ab bin]# pwd\\n/usr/share/elasticsearch/bin\\n[root@1234567890ab bin]#\\n[root@1234567890ab bin]# ls -l *x-pack-env*\\n-rwxr-xr-x 1 elasticsearch root 346 Sep 27 08:40 x-pack-env\\n [root@1234567890ab bin]#\\n\\nWhen I look inside that file, here’s what I see:\\n[root@1234567890ab bin]# more x-pack-env\\n#!/bin/bash\\n# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\\n# or more contributor license agreements. Licensed under the Elastic License;\\n# you may not use this file except in compliance with the Elastic License.\\n# include x-pack-core jars in classpath\\nES_CLASSPATH=\"$ES_CLASSPATH:$ES_HOME/modules/x-pack-core/*\"\\n[root@1234567890ab bin]#\\n\\nAnd just to be complete, here’s the “elasticsearch-sql-cli” script:\\n[root@1234567890ab bin]# more ./elasticsearch-sql-cli\\n#!/bin/bash\\n# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one\\n# or more contributor license agreements. Licensed under the Elastic License;\\n# you may not use this file except in compliance with the Elastic License.\\nsource \"`dirname \"$0\"`\"/elasticsearch-env\\nsource \"`dirname \"$0\"`\"/x-pack-env\\nCLI_JAR=$(ls $ES_HOME/bin/elasticsearch-sql-cli-*.jar)\\nexec \\\\\\n\"$JAVA\" \\\\\\n-jar \"$CLI_JAR\" \\\\\\n\"$@\"\\n[root@1234567890ab bin]#\\n\\nSo forgive me, but it seems like this might be an environment variable issue? The “elasticsearch-sql-cli” is in the same directory as “x-pack-env”, but “elasticsearch-sql-cli” doesn’t see it because its looking in \"dirname \"$0\"`\"/x-pack-env.\"  And I don’t see that env variable set within the Docker container:\\n[root@1234567890ab bin]# printenv\\nHOSTNAME=1234567890ab\\nTERM=xterm\\nELASTIC_CONTAINER=true\\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:\\nPATH=/usr/share/elasticsearch/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\nPWD=/usr/share/elasticsearch/bin\\ndiscovery.type=single-node\\nSHLVL=1\\nHOME=/root\\n_=/usr/bin/printenv\\nOLDPWD=/usr/share/elasticsearch\\n[root@1234567890ab bin]#\\n\\n(This seems like it may also be a problem with the “x-pack-env” script too, as that guy needs $ES_CLASSPATH, $ES_CLASSPATH, and $ES_HOME… which don’t seem to be set, either.)\\nA commented-out line in \"x-pack-env\" says:  \"include x-pack-core jars in classpath\"  Does that mean I have to set the ENV manually?\\nIs there a fix for this, or am I doing something wrong? Thanks!'},\n",
       " {'text_field': 'Happy to help,\\nThere is an open issue in this area: https://github.com/elastic/elasticsearch/issues/47803'},\n",
       " {'text_field': 'Hi Kibana Grandmasters,\\nI recently upgraded to Elasticsearch and Kibana 7.4.0 on Docker, because I want to use the new SQL query-like feature.  I watched the training video by Arthur Gimpel, plus read through the online Kibana documentation on how to use Dev Tools to run SQL-like queries.  But I’m hitting a syntax error right off the bat.\\nI have only one index in ES at the moment, called “myindex2019.10.09”.  I picked that name when I set up Logstash and Elasticsearch, and I didn’t put a lot of thought into which characters to use or not to use.  But now that “myindex2019.10.09” is populated with data and I want to SQL query it, I get this on Dev Tools:\\nMy query is:\\nGET _sql?format=txt\\n{\\n  \"query\": \"SELECT * FROM myindex2019.10.09\"\\n}\\n\\nDev Tools complains:\\n{\\n  \"error\": {\\n    \"root_cause\": [\\n      {\\n        \"type\": \"parsing_exception\",\\n        \"reason\": \"line 1:27: mismatched input \\'.10\\' expecting {&lt;EOF&gt;, \\',\\', \\'ANALYZE\\', \\'ANALYZED\\', \\'AS\\', \\'CATALOGS\\', \\'COLUMNS\\', \\'CURRENT_DATE\\', \\'CURRENT_TIME\\', \\'CURRENT_TIMESTAMP\\', \\'DAY\\', \\'DEBUG\\', \\'EXECUTABLE\\', \\'EXPLAIN\\', \\'FIRST\\', \\'FORMAT\\', \\'FULL\\', \\'FUNCTIONS\\', \\'GRAPHVIZ\\', \\'GROUP\\', \\'HAVING\\', \\'HOUR\\', \\'INNER\\', \\'INTERVAL\\', \\'JOIN\\', \\'LAST\\', \\'LEFT\\', \\'LIMIT\\', \\'MAPPED\\', \\'MINUTE\\', \\'MONTH\\', \\'NATURAL\\', \\'OPTIMIZED\\', \\'ORDER\\', \\'PARSED\\', \\'PHYSICAL\\', \\'PLAN\\', \\'RIGHT\\', \\'RLIKE\\', \\'QUERY\\', \\'SCHEMAS\\', \\'SECOND\\', \\'SHOW\\', \\'SYS\\', \\'TABLES\\', \\'TEXT\\', \\'TYPE\\', \\'TYPES\\', \\'VERIFY\\', \\'WHERE\\', \\'YEAR\\', \\'{LIMIT\\', IDENTIFIER, DIGIT_IDENTIFIER, QUOTED_IDENTIFIER, BACKQUOTED_IDENTIFIER}\"\\n      }\\n    ],\\n    \"type\": \"parsing_exception\",\\n    \"reason\": \"line 1:27: mismatched input \\'.10\\' expecting {&lt;EOF&gt;, \\',\\', \\'ANALYZE\\', \\'ANALYZED\\', \\'AS\\', \\'CATALOGS\\', \\'COLUMNS\\', \\'CURRENT_DATE\\', \\'CURRENT_TIME\\', \\'CURRENT_TIMESTAMP\\', \\'DAY\\', \\'DEBUG\\', \\'EXECUTABLE\\', \\'EXPLAIN\\', \\'FIRST\\', \\'FORMAT\\', \\'FULL\\', \\'FUNCTIONS\\', \\'GRAPHVIZ\\', \\'GROUP\\', \\'HAVING\\', \\'HOUR\\', \\'INNER\\', \\'INTERVAL\\', \\'JOIN\\', \\'LAST\\', \\'LEFT\\', \\'LIMIT\\', \\'MAPPED\\', \\'MINUTE\\', \\'MONTH\\', \\'NATURAL\\', \\'OPTIMIZED\\', \\'ORDER\\', \\'PARSED\\', \\'PHYSICAL\\', \\'PLAN\\', \\'RIGHT\\', \\'RLIKE\\', \\'QUERY\\', \\'SCHEMAS\\', \\'SECOND\\', \\'SHOW\\', \\'SYS\\', \\'TABLES\\', \\'TEXT\\', \\'TYPE\\', \\'TYPES\\', \\'VERIFY\\', \\'WHERE\\', \\'YEAR\\', \\'{LIMIT\\', IDENTIFIER, DIGIT_IDENTIFIER, QUOTED_IDENTIFIER, BACKQUOTED_IDENTIFIER}\",\\n    \"caused_by\": {\\n      \"type\": \"input_mismatch_exception\",\\n      \"reason\": null\\n    }\\n  },\\n  \"status\": 400\\n}\\n\\nSo clearly it doesn’t like that my index name uses the “.” character.  I’ve tried enclosing the index name in single quotes, and that doesn’t help either.\\nMy question to the forum is:  Is my index name incompatible with the SQL parser because I used the “.” character?  Is there a workaround?  Or am I doing something else wrong?  Thanks!'},\n",
       " {'text_field': '@redapplesonly you need to escape the index name because . (dot) has a special meaning: \"query\": \"SELECT * FROM \\\\\"myindex2019.10.09\\\\\"\".'},\n",
       " {'text_field': 'Hello,\\nI\\'m trying configure ELK with SSO (keycloak)..\\n\\nI\\'m using this architecture but i got this error {\"statusCode\":403,\"error\":\"Forbidden\",\"message\":\"[security_exception] action [cluster:admin/xpack/security/user/authenticate] is unauthorized for user [nginx] run as [user1@mail.pt]\"}\\nnginx have privilege run as user1 configured in a role! Can you help me?\\nThanks'},\n",
       " {'text_field': 'Hello,\\nThe error is the SSO, he send the email in forwarded username... I fix this, using a script in LUA\\nThanks to everyone'},\n",
       " {'text_field': 'Hey,\\nI have following log lines:\\n111.222.3.11 - - [15/Oct/2019:08:40:49 +0200] \"asaa /ssss/asf sss/saaa\" 200 12833 \"-\" \"sada(asd; fffff sff ff f asdasd) ssssss/6sadasf (ddddd, sssssss) ddd/sdasds/2222\"\\n222.333.3.11 - - [15/Oct/2019:08:41:01 +0200] \"asd /asf.asf?afwa asf/2.0\" 499 0 \"-\" \"asf/5.0 (asf NT asf.asf; asf;asf x64; asf:asf.asf) asf/asf asfsaf/asf.0\"\\n333.444.3.11 - - [15/Oct/2019:08:43:37 +0200] \"ss /ssssddd/sssssss/agile/ssaaaaaaaaaaaaaddddddddddddddddddddddddffffffffffffffffffffffffffffffffffffffffffffffaaaaaaaaaaaaaaaaaaaaaaaasssssssssssssssssssssssssgggggggggggggggggggg 200 23333 \"222222222222222222222222222222\" \"333asd/asf.af (asf; asf asf afs asf asf) asfasf/s (asf, asf asfasf) asfew/sssss\"\\n555.555.111.5 - - [15/Oct/2019:08:44:45 +0200] \"1111 /32132/222 HTTP/1.1\" 403 152 \"-\" \"Jajsaif (huasfd asdaefk)\"\\n\\nSo every line is different and the same part is only IP in the beginning and date. What I need to do is to parse error code you can see in every line (499, 403, 200...). The problem is that the error part is always in different place in the line.\\nHow can I filter it so I can see it for example in field Error: 409?\\nThank you very much for help!'},\n",
       " {'text_field': 'You could try\\n    grok { match =&gt; { \"message\" =&gt; [ \"%{HTTPD_COMBINEDLOG}\", \"%{HTTPD_COMMONLOG}\" ] } }'},\n",
       " {'text_field': 'I need continuously collapse data by hour (by custom field). And aggregated index must have equal source index mapping.\\nAt first I wanted to use rollup, but it does not support scrolling and much more.'},\n",
       " {'text_field': 'This should be do-able via scripted metric:\\n      \"scripted_metric\": {\\n          \"init_script\": \"state.b = new String()\",\\n          \"map_script\": \"state.b = doc[\\'field_b\\']\",\\n          \"combine_script\": \"return state.b\",\\n          \"reduce_script\": \"return states.get(0).get(0)\"\\n        }\\n      }\\n\\nI should take the 1st value for field_b, I admit this isn\\'t super user friendly.'},\n",
       " {'text_field': \"I'm currently in the process of moving an Elasticsearch cluster from VM/SAN to physical/local disk.\\nCurrent cluster is 6 VMs, each with 2 data nodes and either a master or client node (total 3 master, 3 client, 12 data). Index shards/replicas are 6/1.\\nNew cluster is 4 physicals, each with 7 data nodes, 1 master and 1 client (total 4 master, 4 client, 28 data), same 6/1 shard/replica.\\nBoth clusters will be 7.1.1\\nWhen I look around for migration methods, I see things like logstash (did it this way last time, since I wanted to do some processing on the way over), or snapshots, etc.\\nMy question is, can I not just add all the new nodes into the cluster, let it re-organize/stabilize, switch the load balancer to point to the new client nodes, then start removing the old nodes? I never see this offered as a migration solution, so I keep thinking there must be a reason why. \"},\n",
       " {'text_field': \"\\n\\n\\n SpeedDaemon:\\n\\nMy question is, can I not just add all the new nodes into the cluster, let it re-organize/stabilize, switch the load balancer to point to the new client nodes, then start removing the old nodes?\\n\\n\\nYes, this should work just fine. I recommend fully evacuating each node using a shard allocation filter before shutting it down. It mostly works to just shut them down gradually and wait for green but there are some corner cases that you can avoid with a proper evacuation first. Shut down the old masters one at a time and make sure each one has properly gone from the cluster before shutting down the next one.\\n\\n\\n\\n SpeedDaemon:\\n\\nI never see this offered as a migration solution, so I keep thinking there must be a reason why.\\n\\n\\nI don't know that we document any specific migration techniques do we?\"},\n",
       " {'text_field': 'Working on compiling Filebeat from source to run on arm architecture.  This worked for 7.3 without any issues.  A new message came up with 7.4\\nhere is a snip of my script that i am using.\\ngo get github.com/elastic/beats\\ncd beats\\ngit fetch\\ngit checkout f940c36\\nGOOS=linux GOARCH=arm go get\\nmake\\n\\nNext I receive this message\\ngithub.com/elastic/beats/filebeat/input/kafka\\ninput/kafka/input.go:197:10: undefined: strings.ReplaceAll\\n\\nI have done a bit of searching but I cant figure out what this message means and why I am receiving it.'},\n",
       " {'text_field': \"Hello,  thanks for reaching out about filebeat.  Which version of Go are you using?  I believe I've seen this error with older versions of Go. We tested version 7.4 with Go 1.12.9.  From the top level of the beats directory you can do the following to check the version of Go used for a particular branch.\\n$ cat .go-version\\n1.12.9\\n\"},\n",
       " {'text_field': \"Hi,\\ni'm starting getting into elasticsearch cluster, and i have read many time the recomendation to have 3 master eligible nodes to have high availability.\\nI've also read that if you have dedicated master nodes should not be used for indexing or requests. However if a node if data and master it can, for instance in a 3 node configuration where each node is master/data node you could send requests to any of them.\\nSo first question: whats the difference on being master only node and master/data node? if i should not send requests to a master only node, i thing a master/data node has more work to do than a master only node, so why should i send request to the first and not the later?\\nMy other question is if the following configuration is a valid variation from the 3 node configuration:\\nnode1 Master/data node\\nnode2 Master/data node\\nnode3 Master/data node (voting only)\\nin this configuration node3 will never act as master, will only participate in voting, i still can loose any of the 3 nodes as soon as there are 2 running, and i could direct requests to node3 as it will never be a master. Is that OK?\\nthanks\"},\n",
       " {'text_field': \"\\n\\n\\n mtudisco:\\n\\nSo first question: whats the difference on being master only node and master/data node? if i should not send requests to a master only node, i thing a master/data node has more work to do than a master only node, so why should i send request to the first and not the later?\\n\\n\\nTypically you don't have both master-only and mixed master/data nodes in a cluster. Mixed nodes are used in smaller, lightly-loaded clusters where there's no risk of overloading anything so it doesn't really matter where you send your requests. Dedicated masters are appropriate where the cluster is more heavily loaded and there it is important to keep as much load as possible off the masters.\\n\\n\\n\\n mtudisco:\\n\\nMy other question is if the following configuration is a valid variation from the 3 node configuration:\\n\\n\\nYes it is.\"},\n",
       " {'text_field': 'I\\'am using Logstash 7.3 installed locally on server with:\\nsudo apt-get update &amp;&amp; sudo apt-get install logstash\\nThe goal is use filebeat to send logs to Elastic from Logstash but when I run Logstash, they starts on 127.0.0.1 and its not accessible by external machine..\\nWhen I run:\\n$ /usr/share/logstash/bin/logstash -e \"\" --log.level=debug\\nI see:\\n[DEBUG] 2019-10-15 18:23:14.272 [LogStash::Runner] runner - http.host: \"127.0.0.1\" [DEBUG] 2019-10-15 18:23:14.273 [LogStash::Runner] runner - http.port: 9600..9700 [DEBUG] 2019-10-15 18:23:14.273 [LogStash::Runner] runner - http.environment: \"production\"\\nAnd when I run netstat:\\n$ netstat -na |grep 9600\\nI get:\\ntcp 0 0 127.0.0.1:9600 0.0.0.0:* LISTEN\\nOn my /etc/logstash/logstash.yml I have:\\nhttp.host: \"10.198.9.233\"\\nhttp.port: 9600-9700\\nI already tried to put any address on http.host like the real machine IP, 0.0.0.0, with no space indentation, 1 space, 2 spaces.. with tab but its not working.. I\\'am thinking witch Logstash is not applying my configs on /etc/logstash/logstash.yml\\nAnybody can help me? Thx'},\n",
       " {'text_field': 'From the debug, the following line;\\n[DEBUG] 2019-10-16 16:17:22.208 [LogStash::Runner] runner - path.settings: \"/usr/share/logstash/config\"\\n\\nIt\\'s looking in that directory for the logstash.yml file. You mentioned earlier that this file is located in /etc/logstash/logstash.yml\\nThis would indicate that the configuration is not being picked up. You can specify the location of logstash.yml with the command line option --path.settings\\nGive that a go with the debug and see if it then picks up the settings file. When it\\'s picking up the correct configuration file, expect to see something like this in the debug log;\\n[2019-10-17T10:23:34,395][DEBUG][logstash.runner          ] *http.host: \"172.16.40.72\" (default: \"127.0.0.1\")'},\n",
       " {'text_field': \"so, I have this huge files. I tried using read mode file input, but nothing happened, I tried piping the content (like I've learned from @dadoonet over here https://www.elastic.co/pt/blog/enriching-your-postal-addresses-with-the-elastic-stack-part-1), but since the files are so huge I'm running out of memory even before the pipe can start logstash.\\nAny ideas?\"},\n",
       " {'text_field': 'Interesting .... couple thoughts / questions\\nCan I assume this is on windows?\\nSo perhaps to test just make a very simple read / write conf.\\nIf so you can use forward slashes with windows path.\\nAlso you can set the sincedb_path to NUL\\nYou might want to clean out your sincedb ..\\n/logstash-6.8.0/data/plugins/inputs/file/.sincedb*\\nAnd perhaps let\\'s just use default tail mode to test.\\nThen just run it and it should start streaming lines.\\nfile {\\n        path =&gt; \"C:/Users/sbrown/elastic/logstash-6.8.0/NOTICE.TXT\"\\n        start_position =&gt; \"beginning\"\\n        sincedb_path =&gt; \"NUL\"\\n}\\n\\noutput {  \\n  stdout {codec =&gt; rubydebug}\\n}\\n\\nI am also curious if the file is compressed or has an unusual new line delimiter.\\nGet that to work then go back and start setting the other settings / config'},\n",
       " {'text_field': 'systemctl status  elasticsearch\\nâ elasticsearch.service - Elasticsearch\\nLoaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled; vendor preset: disabled)\\nActive: failed (Result: exit-code) since Wed 2019-10-16 10:10:43 IST; 12min ago\\nDocs: http://www.elastic.co\\nProcess: 6504 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -p ${PID_DIR}/elasticsearch.pid --quiet (code=exited, status=1/FAILURE)\\nMain PID: 6504 (code=exited, status=1/FAILURE)\\nOct 16 10:10:25 localhost.localdomain systemd[1]: Starting Elasticsearch...\\nOct 16 10:10:27 localhost.localdomain elasticsearch[6504]: OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\\nOct 16 10:10:42 localhost.localdomain systemd[1]: elasticsearch.service: main process exited, code=exited, status=1/FAILURE\\nOct 16 10:10:43 localhost.localdomain systemd[1]: Failed to start Elasticsearch.\\nOct 16 10:10:43 localhost.localdomain systemd[1]: Unit elasticsearch.service entered failed state.\\nOct 16 10:10:43 localhost.localdomain systemd[1]: elasticsearch.service failed.\\nHavent change the config file but the indices for elasticsearch is filled completly and there is no server space.\\nFilesystem                                Size  Used Avail Use% Mounted on\\n/dev/mapper/rhel-root            14G  9.0G  4.8G  66% /\\ndevtmpfs                              16G     0   16G   0% /dev\\ntmpfs                                 16G     0   16G   0% /dev/shm\\ntmpfs                                 16G  1.7G   14G  11% /run\\ntmpfs                                  16G     0   16G   0% /sys/fs/cgroup\\n/dev/sda1                              997M  162M  836M  17% /boot\\n/dev/mapper/vgkibana-lvelastic   50G   33M   50G   1% /var/lib/elasticsearch\\ntmpfs                                   3.2G     0  3.2G   0% /run/user/0'},\n",
       " {'text_field': '\\n\\n\\n anusree_arun:\\n\\nWe have deleted some indices in /var/lib/elasticsearch/nodes/0/* .After that elastic search cant be started\\n\\n\\nUnfortunately this will have left this node in a broken state. There are no user-serviceable parts inside the data path and you should never make any changes to it yourself.\\nThe only sensible path forwards is to wipe this node. This will allow it to start, and then Elasticsearch will recover the replicas from the other nodes in the cluster.'},\n",
       " {'text_field': 'Hello,\\nI want to use multisearch request to search for multiple geo_point.\\nMy problem is that I want to pass an array as parameter in \"term\" node but it doesn\\'t works.\\nI use a template :\\nPOST /_scripts/my_template_3\\n{\\n  \"script\": {\\n    \"lang\": \"mustache\",\\n    \"source\": \"{\\\\\"size\\\\\": 0,\\\\\"query\\\\\": {\\\\\"bool\\\\\": {\\\\\"must\\\\\": {\\\\\"terms\\\\\": {{#toJson}}dates{{/toJson}}}}},\\\\\"aggs\\\\\": {\\\\\"bydate\\\\\": {\\\\\"terms\\\\\": {\\\\\"field\\\\\": \\\\\"datefield\\\\\"},\\\\\"aggs\\\\\": {\\\\\"byindex\\\\\": {\\\\\"terms\\\\\": {\\\\\"field\\\\\": \\\\\"_index\\\\\"},\\\\\"aggs\\\\\": {\\\\\"top\\\\\": {\\\\\"top_hits\\\\\": {\\\\\"sort\\\\\": [{\\\\\"_geo_distance\\\\\": {\\\\\"geo\\\\\": {\\\\\"lat\\\\\": 42.1,\\\\\"lon\\\\\": -158.375},\\\\\"order\\\\\": \\\\\"asc\\\\\",\\\\\"unit\\\\\": \\\\\"m\\\\\",\\\\\"distance_type\\\\\": \\\\\"arc\\\\\"}}],\\\\\"size\\\\\": 1}}}}}}}}\"\\n  }\\n}\\nAnd here my call :\\n{ \"index\": \"name\" }\\n{\"id\":\"my_template_3\", \"params\": {{ \"dates\":{\"datefield\": [\"2019-07-27T12:00:00\", \"2019-07-27T18:00:00Z\", \"2019-07-27T19:00:00Z\" ]}}}}\\nThank you.'},\n",
       " {'text_field': 'Ok, I have resolve my problem.\\nThat was a syntax error'},\n",
       " {'text_field': 'I post this, with xpack enabled and trial license\\ncurl -X POST \"https://127.0.0.1:9200/_xpack/security/role/demo_admin\" -u elastic:password -k -H \"Content-Type: application/json\" --data \\'{\"indices\": [{\"names\": [ \"*\" ],\"privileges\":[ \"manage\" ]}]}\\'\\nand get this back\\n{\"error\":{\"root_cause\":[{\"type\":\"unavailable_shards_exception\",\"reason\":\"[.security-7][0] primary shard is not active Timeout: [1m], request: [BulkShardRequest [[.security-7][0]] containing [index {[.security][_doc][role-demo_admin], source[{\"cluster\":,\"indices\":[{\\n\"names\":[\"\"],\"privileges\":[\"manage\"],\"allow_restricted_indices\":false}],\"applications\":[],\"run_as\":[],\"metadata\":{},\"type\":\"role\"}]}] and a refresh]\"}],\"type\":\"unavailable_shards_exception\",\"reason\":\"[.security-7][0] primary shard is not active Timeout\\n: [1m], request: [BulkShardRequest [[.security-7][0]] containing [index {[.security][_doc][role-demo_admin], source[{\"cluster\":[],\"indices\":[{\"names\":[\"\"],\"privileges\":[\"manage\"],\"allow_restricted_indices\":false}],\"applications\":,\"run_as\":,\"metadat\\na\":{},\"type\":\"role\"}]}] and a refresh]\"},\"status\":503}\\nWhat am I missing?\\n(Sorry about the  - not sure yet how to avoid these being parsed incorrectly by the forum UI.)'},\n",
       " {'text_field': 'What does your node configuration look like? Do you have any non-default settings, e.g. node.data: false?'},\n",
       " {'text_field': 'Hello,\\nI use OSS version of Elasticsearch 6.8.0 + Kibana and I\\'m not able to turn on monitoring features. This page - https://www.elastic.co/what-is/open-x-pack - says that \"Starting with version 6.3, all of the free X-Pack features (monitoring, Search Profiler, Grok Debugger, zoom levels in Elastic Maps Service, dedicated APM UIs, and more) ship with the default distributions of Elasticsearch, Kibana, Beats, and Logstash.\"\\nSo I would expect that since XPack is opensource and I\\'m using opensource version of Elasticsearch I\\'m able to use monitoring in Kibana.\\nWhen I add \"xpack.monitoring.collection.enabled: true\" to \"elasticsearch.yml\" it fails to start complaining that it does not know that setting. So how is it with OSS version and monitoring?'},\n",
       " {'text_field': 'Monitoring requires the default distribution and is not part of OSS.'},\n",
       " {'text_field': 'Hi All,\\nI have a Elasticsearch node setup in kubernetes cluster with following configuration. Another ES node to store the monitoring data. But how to pass the http exporter config to yaml file ?\\nBoth Elasticsearch Version: 6.8.0\\n\"containers\": [\\n  {\\n\"name\": \"elasticsearch\",\\n\"image\": \"docker.elastic.co/elasticsearch/elasticsearch:6.8.0\",\\n\"ports\": [\\n  {\\n    \"name\": \"http\",\\n    \"containerPort\": 9200,\\n    \"protocol\": \"TCP\"\\n  },\\n  {\\n    \"name\": \"transport\",\\n    \"containerPort\": 9300,\\n    \"protocol\": \"TCP\"\\n  }\\n],\\n\"env\": [\\n  {\\n    \"name\": \"node.name\",\\n    \"valueFrom\": {\\n      \"fieldRef\": {\\n        \"apiVersion\": \"v1\",\\n        \"fieldPath\": \"metadata.name\"\\n      }\\n    }\\n  },\\n  {\\n    \"name\": \"discovery.zen.minimum_master_nodes\",\\n    \"value\": \"1\"\\n  },\\n  {\\n    \"name\": \"discovery.zen.ping.unicast.hosts\",\\n    \"value\": \"elasticsearch-master-headless\"\\n  },\\n  {\\n    \"name\": \"cluster.name\",\\n    \"value\": \"elasticsearch\"\\n  },\\n  {\\n    \"name\": \"network.host\",\\n    \"value\": \"0.0.0.0\"\\n  },\\n  {\\n    \"name\": \"ES_JAVA_OPTS\",\\n    \"value\": \"-Xmx1g -Xms1g\"\\n  },\\n  {\\n    \"name\": \"node.data\",\\n    \"value\": \"true\"\\n  },\\n  {\\n    \"name\": \"node.ingest\",\\n    \"value\": \"true\"\\n  },\\n  {\\n    \"name\": \"node.master\",\\n    \"value\": \"true\"\\n  }\\n],\\n\"resources\": {\\n  \"limits\": {\\n    \"cpu\": \"1500m\",\\n    \"memory\": \"3Gi\"\\n  },\\n  \"requests\": {\\n    \"cpu\": \"1\",\\n    \"memory\": \"3Gi\"\\n  }\\n},\\n\"volumeMounts\": [\\n  {\\n    \"name\": \"elasticsearch-master\",\\n    \"mountPath\": \"/usr/share/elasticsearch/data\"\\n  },\\n  {\\n    \"name\": \"default-token-6gzvx\",\\n    \"readOnly\": true,\\n    \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\"\\n  }\\n],\\n\"readinessProbe\": {\\n  \"exec\": {\\n    \"command\": [\\n      \"sh\",\\n      \"-c\",\\n      \"#!/usr/bin/env bash -e\\\\n# If the node is starting up wait for the cluster to be ready (request params: \\'wait_for_status=green&amp;timeout=1s\\' )\\\\n# Once it has started only check that the node itself is responding\\\\nSTART_FILE=/tmp/.es_start_file\\\\n\\\\nhttp () {\\\\n    local path=\\\\\"${1}\\\\\"\\\\n    if [ -n \\\\\"${ELASTIC_USERNAME}\\\\\" ] &amp;&amp; [ -n \\\\\"${ELASTIC_PASSWORD}\\\\\" ]; then\\\\n      BASIC_AUTH=\\\\\"-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}\\\\\"\\\\n    else\\\\n      BASIC_AUTH=\\'\\'\\\\n    fi\\\\n    curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path}\\\\n}\\\\n\\\\nif [ -f \\\\\"${START_FILE}\\\\\" ]; then\\\\n    echo \\'Elasticsearch is already running, lets check the node is healthy\\'\\\\n    http \\\\\"/\\\\\"\\\\nelse\\\\n    echo \\'Waiting for elasticsearch cluster to become cluster to be ready (request params: \\\\\"wait_for_status=green&amp;timeout=1s\\\\\" )\\'\\\\n    if http \\\\\"/_cluster/health?wait_for_status=green&amp;timeout=1s\\\\\" ; then\\\\n        touch ${START_FILE}\\\\n        exit 0\\\\n    else\\\\n        echo \\'Cluster is not yet ready (request params: \\\\\"wait_for_status=green&amp;timeout=1s\\\\\" )\\'\\\\n        exit 1\\\\n    fi\\\\nfi\\\\n\"\\n    ]\\n  },\\n  \"initialDelaySeconds\": 10,\\n  \"timeoutSeconds\": 5,\\n  \"periodSeconds\": 10,\\n  \"successThreshold\": 3,\\n  \"failureThreshold\": 3\\n},\\n\"terminationMessagePath\": \"/dev/termination-log\",\\n\"terminationMessagePolicy\": \"File\",\\n\"imagePullPolicy\": \"IfNotPresent\"\\n  }\\n]\\n\\nAll the elasticsearch.yml configs are passed as env variable.\\nEg.\\ndiscovery.zen.minimum_master_nodes: 1\\nHow to pass the value for remote monitoring  as per the doc @ Collecting Monitoring Data\\nxpack.monitoring.exporters:\\n  id1:\\n    type: http\\n    host: [\"http://es-mon-1:9200\", \"http://es-mon2:9200\"]'},\n",
       " {'text_field': 'Added the below config and the remote monitoring started working.\\n      {\\n        \"name\": \"xpack.monitoring.collection.enabled\",\\n        \"value\": \"true\"\\n      },\\n      {\\n        \"name\": \"xpack.monitoring.exporters.my_remote.type\",\\n        \"value\": \"http\"\\n      },\\n      {\\n        \"name\": \"xpack.monitoring.exporters.my_remote.host\",\\n        \"value\": \"http://&lt;remote_es_ip&gt;:9200\"\\n      }'},\n",
       " {'text_field': 'I have created a Role with the help of the following documentation in Kibana.\\nLinks:\\nhttps://www.elastic.co/guide/en/kibana/7.3/role-management-api-put.html\\nhttps://www.elastic.co/guide/en/kibana/7.3/kibana-privileges.html\\nHowever, I am unable to assign this role to a user created with OpenID based realm. Is there any way to do this?\\nAPI used to create the developer role:\\nURL:\\nPUT {{kb_ip_port}}/api/security/role/developer_role\\n{\\n\\t\"metadata\": {\\n\\t\\t\"version\": 1.0\\n\\t},\\n    \"elasticsearch\": {\\n        \"cluster\": [\\n            \"monitor\",\\n            \"manage_index_templates\",\\n            \"manage_pipeline\",\\n            \"manage_ingest_pipelines\",\\n            \"transport_client\",\\n            \"manage_ml\",\\n            \"monitor_ml\",\\n            \"manage_data_frame_transforms\",\\n            \"monitor_data_frame_transforms\",\\n            \"manage_watcher\",\\n            \"monitor_watcher\",\\n            \"manage_ccr\",\\n            \"manage_ilm\",\\n            \"read_ilm\",\\n            \"manage_rollup\",\\n            \"monitor_rollup\",\\n            \"manage_saml\",\\n            \"manage_token\",\\n            \"create_snapshot\",\\n            \"manage_oidc\",\\n            \"read_ccr\",\\n            \"manage\"\\n        ],\\n        \"indices\": [\\n            {\\n                \"names\": [\\n                    \"*\"\\n                ],\\n                \"privileges\": [\\n                    \"all\"\\n                ]\\n            }\\n        ]\\n    },\\n    \"kibana\": [\\n        {\\n            \"base\": [\\n                \"all\"\\n            ],\\n            \"feature\": {},\\n            \"spaces\": [\\n                \"*\"\\n            ]\\n        }\\n    ]\\n}'},\n",
       " {'text_field': 'Hi there,\\nYou can assign roles to the users that log in with OpenID Connect with role mapping based on various OpenID Connect claims'},\n",
       " {'text_field': \"I have a three node ES cluster running 5.6.5. Last week I restarted the servers in the cluster to make changes to their disk configuration, I did not follow any process to do this but after 12 hours the cluster and indexes were back into a green state. Yesterday I restarted one server, following the rolling upgrade process but without upgrading the server. Indexing (logstash) is still stopped and no documents are being added to the system.\\nI ended up in a state with sync issues, which appears to be identical to this post, I are also running a similarly antiquated version, a situation I are working to address.\\nI attempted to reroute the bad index, then to remove and re-add replicas, which resulted in a wealth of failures and us being in the following state:\\ncorp_b 0 r STARTED    125766000 131.5gb 162.211.235.20 pcorplog3\\ncorp_b 0 p STARTED    125766000 131.5gb 162.211.235.11 pcorplog2\\ncorp_b 1 p STARTED    125746770 128.6gb 162.211.235.11 pcorplog2\\ncorp_b 1 r UNASSIGNED\\ncorp_b 2 p STARTED    125765953 130.9gb 162.211.235.20 pcorplog3\\ncorp_b 2 r STARTED    125765953 130.9gb 162.211.235.10 pcorplog1\\ncorp_b 3 r STARTED    125765360 132.1gb 162.211.235.11 pcorplog2\\ncorp_b 3 p STARTED    125765360 132.1gb 162.211.235.10 pcorplog1\\ncorp_b 4 r STARTED    125752747 133.2gb 162.211.235.11 pcorplog2\\ncorp_b 4 p STARTED    125752747 133.2gb 162.211.235.10 pcorplog1\\ncorp_b 5 p STARTED    125771560   128gb 162.211.235.20 pcorplog3\\ncorp_b 5 r STARTED    125771560   128gb 162.211.235.11 pcorplog2\\n\\nThis report, from _cat/shards, indicates that the primary shard for this index is located on pcorplog2 and the replica is unassigned. I see the same information when querying _cat/shards on all three cluster members.\\nHowever, when I look at _cluster/allocation/explain?pretty I seem to get a different view as to the primary index of this shard; I receive failure reports from the three nodes as follows:\\npcorplog1: IllegalStateException[try to recover [corp_b][1] from primary shard with sync id but number of docs differ: 125852748 (pcorplog3, primary) vs 125852744(pcorplog1)\\npcorplog2: IllegalStateException[try to recover [corp_b][1] from primary shard with sync id but number of docs differ: 125852748 (pcorplog3, primary) vs 125852744(pcorplog1)\\npcorplog3: IllegalStateException[try to recover [corp_b][1] from primary shard with sync id but number of docs differ: 125852748 (pcorplog3, primary) vs 125852744(pcorplog1)\\n\\nI'm very eager to get this issue solved, thanks in advance for any assistance you can offer, full allocation explain follows in a second post.\"},\n",
       " {'text_field': 'Ok, interesting. I suspect the ongoing rebalance attempts may have prevented the out-of-sync copy from being cleaned up properly, because the cleanup only happens if the shard is fully assigned and none of the copies are relocating. I would suggest disabling rebalancing and then setting the number of replicas back down to 0 to avoid the relocation.'},\n",
       " {'text_field': 'so i have a field named subjects, it is an array. so I want to get all the values of subjects field in a particular index. should be about 300-400. Can someone suggest how to achieve this.\\nI am doing this:\\nGET /index/_search { \"size\": \"0\", \"aggs\":{ \"by_subjects\":{ \"terms\": { \"field\": \"subjects.en.keyword\" } } } }\\nbut this is returning me only 10 items in a list.\\nalso its giving me this in result if it helps:\\n\"aggregations\" : { \"by_subjects\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 381,'},\n",
       " {'text_field': ''},\n",
       " {'text_field': \"Hello every one!\\nI would like to know if is possible to specify a pipeline on the beat when I only use the cloud.id and cloud.auth configs. I know that with output.elasticsearch I have this option, but for some reason on a costumer (probably firewall) I can only communicate with cloud tags! I'll solve this problem but I was wondering if it's possible to do this when using cloud config!\\nThanks!! \"},\n",
       " {'text_field': 'Yes it is. Your Filebeat configuration would contain something like this:\\ncloud.id: YOUR_CLOUD_ID\\ncloud.auth: YOUR_CLOUD_USER:YOUR_CLOUD_PASSWORD\\n\\noutput.elasticsearch\\n  pipeline: YOUR_PIPELINE_ID\\n'},\n",
       " {'text_field': 'Hi\\nI have configured an elasticsearch cluster with 4 nodes (3 are master/data nodes, and 1 is a coordinating node).\\nIn the coordinating node i have logstash and kibana. But i\\'m cannot make logstash to read files from input plugin, i have copied the configuration from other server where it works, and as it didnt work in the new server i have simplified it but still not working. In logstash logs there no error message.\\nlogstash configuration is\\n1010_input_file.conf:\\ninput{\\n  file {\\n    path =&gt; \"/opt/audit_logs_modsec/XX/20171231/20171231-2356/*\"\\n    sincedb_path =&gt; \"/dev/null\"\\n    mode =&gt; read\\n    ignore_older =&gt; 0\\n    start_position =&gt; \"beginning\"\\n    type =&gt; \"mod_security\"\\n\\n    codec =&gt; multiline {\\n      charset =&gt; \"US-ASCII\"\\n      pattern =&gt; \"^--[a-fA-F0-9]{8}-Z--$\"\\n      negate =&gt; true\\n      what =&gt; previous\\n    }\\n  }\\n}\\n\\n3000_output.conf:\\noutput {\\n  stdout {\\n        codec =&gt; rubydebug\\n  }\\n        elasticsearch {\\n          hosts =&gt; [\"https://192.168.90.103:9200\"]\\n          cacert =&gt; \\'/etc/logstash/config/certs/ca.crt\\'\\n          ssl =&gt; true\\n          user =&gt; \"elastic\"\\n          password =&gt; \"XXXX\"\\n          index =&gt; \"modsecurity_%{+YYYY.MM.dd}\"\\n        }\\n}\\n\\nAny idea what might be wrong?\\nin this configuration i have elk 7.4, in my previous server i had 7.3.\\nI have used XX to hide certain information, if anyone intereseted i can email de logstash log.\\nthanks'},\n",
       " {'text_field': 'the problem was the line\\nignore_older =&gt; 0\\n\\ni removed it and files where procesed.'},\n",
       " {'text_field': 'I\\'m trying to construct a simple query to match all logs lines that start with \"Error: \", but when I try to search for this string, all lines that include the word \\'error\\' (not case sensitive) anywhere in the string are returned. It doesn\\'t seem to do anything with the colon in the string. I\\'d love it be case sensitive and take the colon into account with the query. If I can somehow indicate that the text should start with this string, that would be even better.\\nAccording to the KQL documentation, if I\\'m reading it right (https://www.elastic.co/guide/en/kibana/7.1/kuery-query.html), I should be able to use this query to accomplish what I\\'m looking for:\\nlog_text: \"Error: \"\\nBut it doesn\\'t return what I described. I tried a number of variations of this as well, hoping someone can guide me in the right direction here, I feel like I\\'m missing something minor. Thanks in advance.'},\n",
       " {'text_field': 'Since text fields are analyzed, whatever you search will also be analyzed and special characters will be stripped.\\nIn other words, even if you had other special characters in your query against log_text, you\\'d still get matches (since they\\'re filtered out).\\nWhat you really want is to search against the keyword version of the field. I think you\\'ll want something along the lines of\\nlog_text.keyword: Error\\\\:*\\n\\nThe * is a wildcard that means \"anything after this\", and the \\\\ escapes the colon so that it actually becomes part of the query.'},\n",
       " {'text_field': 'Hi, I took the Elasticsearch Engineer 1 training this summer (ES 6.x), then completed the Engineer 2 training (ES 7.x)\\nSince I learned this the config settings no longer use :\\ndiscovery.zen.ping.unicast.hosts: [\"server1\", \"server2\", \"server3\"]\\ndiscovery.zen.minimum_master_nodes: 2\\n\\nand instead use\\ndiscovery.seed_hosts: [\"server1\", \"server2\", \"server3\"]    \\ncluster.initial_master_nodes: [\"server1\"]\\n\\nPossible to get a simple example of how servers could join a 3 node cluster in ES 7.x ?\\nIs the following correct?\\nFirst time a \"server1\" joins a 3 node cluster (server1-3), add this to elasticsearch.yml\\ndiscovery.seed_hosts: [\"server1\", \"server2\", \"server3\"]    \\ncluster.initial_master_nodes: [\"server1\"]\\n\\nThen stop server1, delete this line, then restart\\ncluster.initial_master_nodes: [\"server1\"]\\n\\nRepeat for server2 and server 3, ie first time server2 joins:\\ndiscovery.seed_hosts: [\"server1\", \"server2\", \"server3\"]    \\ncluster.initial_master_nodes: [\"server2\"]\\n\\nI have been referring to https://www.elastic.co/guide/en/elasticsearch/reference/7.2/discovery-settings.html however would be grateful for a simple example.'},\n",
       " {'text_field': '\\n\\n\\n AnitaL:\\n\\nwhen using a single node cluster for the first time\\n\\n\\nRight, forming a cluster for the first time is quite different from joining a node to an existing cluster, and is exactly the situation in which you must set cluster.initial_master_nodes. But you must set it to the same value on every node on which it is set. From these docs:\\n\\nWARNING: You must set  cluster.initial_master_nodes  to the same list of nodes on each node on which it is set in order to be sure that only a single cluster forms during bootstrapping and therefore to avoid the risk of data loss.\\n'},\n",
       " {'text_field': 'Howdy Elastic Community,\\nI\\'ve got what I believe is a relatively simple issue that I need assistance resolving. Even though I believe I know the root cause of the issue, I\\'m asking for help because I haven\\'t fully wrapped my head around index mappings and best-practices when it comes to modifying index mappings.\\nThe issue I\\'m trying to resolve is related to attempting to aggregate on a text field which is returning the \" Fielddata is disabled on text fields by default\" error, the full error message is below. As far as I can tell this is affecting my visualizations / dashboards (Other areas may be affected that I\\'m unaware of).\\nThe issue began when my indices rolled over from September to October and from what I can gather the index mapping changed. Prior to October all of my indices were imported via the snapshot / restore API during a migration from ES 5.3 to 6.8 so this October index is the first index created within ES 6.8.\\n\"Fielddata is disabled on text fields by default. Set fielddata=true on [ChannelPartner] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\"\\n\\nFrom the research I\\'ve done, Resolving this issue should be as easy as updating the mapping for the \"ChannelPartner\" field in the Responses-* index from \"text\" to \"keyword\". Based on the index mapping I have the \"ChannelPartner\" field is mapped as both a \"text\", and \"keyword\" field and I should be able to use \"ChannelPartner.keyword\" in my visualizations for aggregation. However, that field does not appear to exist. I\\'ve posted truncated mappings for my September \"Response\" index (unaffected by issue) and my October \"Response\" index below.\\n\\n52%20PM676×746 38.5 KB\\n\\nFor what it\\'s worth, my index pattern within Kibana specifies all of the response indices as so \"responses-*\" and the individual naming pattern is \"responses-YYYY.MM\". Due to the inconsistant index mapping between months could this also be part of the issue?\\nGET /responses-2019.10/_mapping/ [Affected index]\\n{\\n  \"responses-2019.10\" : {\\n    \"mappings\" : {\\n      \"doc\" : {\\n        \"properties\" : {\\n          \"@timestamp\" : {\\n            \"type\" : \"date\"\\n          },\\n          \"@version\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"ApplicationTimeStamp\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"Brand\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"ChannelPartner\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"Creditlimit\" : {\\n            \"type\" : \"float\"\\n          },\\n          \"Financedamount\" : {\\n            \"type\" : \"float\"\\n          },\\n          \"Lastname\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"Lender\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n}\\n\\nGET /responses-2019.09/_mapping\\n\\n{\\n  \"responses-2019.09\" : {\\n    \"mappings\" : {\\n      \"_default_\" : {\\n        \"_all\" : {\\n          \"enabled\" : false\\n        },\\n        \"properties\" : {\\n          \"@timestamp\" : {\\n            \"type\" : \"date\"\\n          },\\n          \"@version\" : {\\n            \"type\" : \"text\"\\n          },\\n          \"ApplicationTimeStamp\" : {\\n            \"type\" : \"text\"\\n          },\\n          \"Brand\" : {\\n            \"type\" : \"keyword\"\\n          },\\n          \"ChannelPartner\" : {\\n            \"type\" : \"keyword\"\\n          }\\n       },\\n      \"APIResponseLog\" : {\\n        \"_all\" : {\\n          \"enabled\" : false\\n        },\\n        \"properties\" : {\\n          \"@timestamp\" : {\\n            \"type\" : \"date\"\\n          },\\n          \"@version\" : {\\n            \"type\" : \"text\"\\n          },\\n          \"ApplicationTimeStamp\" : {\\n            \"type\" : \"text\"\\n          },\\n          \"Brand\" : {\\n            \"type\" : \"keyword\"\\n          },\\n          \"ChannelPartner\" : {\\n            \"type\" : \"keyword\"\\n          }\\n          }\\n}\\n\\nWhat would my best path forward for resolving this issue be? As far as I understand it I believe I have three options to choose from:\\n1 - Assuming the \"ChannelPartner.keyword\" field does exist and It\\'s simply a matter of creating a new index pattern that encompass the updated index mapping while leaving the old mappings behind I could then update my visualizations to utilize the new \"Channelpartner.keyword\" field.\\n2 - I could re-index all of my \"Responses\" indices prior to October using the new updated mapping using the reindex-api. I assume when I create the new indices I\\'ll be re-indexing into I\\'ll want to create an alias for them IE- \"responses-2019.09.updated\" that way my existing index pattern \"responses-*\" will still match indices properly.\\n3 - Enable \"fielddata=true\" within the index mapping for the \"ChannelPartner\" field. Which I really would rather avoid doing.\\nResources Referenced:\\n  \\n    \\n    \\n    Trying to use Aggregate, \"Fielddata is disabled on text fields by default. Set fielddata=true\" Elasticsearch\\n  \\n  \\n    hello sir @dadoonet, thank you for the fast response! \\nmay I know how to run my aggregation on author_id.keyword? is it supposed to look like this? \\n\\'aggs\\' =&gt; [\\n\\t\\t\\t\\t\\'authors\\' =&gt; [\\n\\t\\t\\t\\t\\t\\'terms\\' =&gt; [\\n\\t\\t\\t\\t\\t\\t\\'field\\' =&gt; \\'author_id.keyword\\',\\n\\t\\t\\t\\t\\t\\t\\'size\\' =&gt; 25,\\n\\t\\t\\t\\t\\t\\t\\'order\\' =&gt; [ \\'_term\\' =&gt; \\'asc\\' ]\\n\\t\\t\\t\\t\\t]\\n\\t\\t\\t\\t],\\n\\t\\t\\t]\\n\\nIf it is correct, I\\'ve started retrieving buckets! \\nAgain, thank you very much, appreciate it! \\n  \\n\\n\\n\\n  \\n    \\n    \\n    \"Fielddata is disabled on text fields by default\" error on keyword field Elasticsearch\\n  \\n  \\n    I\\'m getting this error in my log: \\nCaused by: java.lang.IllegalArgumentException: Fielddata is disabled on text fields by default. Set fielddata=true on [component] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\\n        at org.elasticsearch.index.mapper.TextFieldMapper$TextFieldType.fielddataBuilder(TextFieldMapper.java:670) ~[elasticsearch-6.5.0.jar:6.5.0]\\n\\nWhich I don\\'t unders…\\n  \\n\\n\\n\\n  \\n      \\n\\n      Elastic Blog – 7 Sep 16\\n  \\n\\n  \\n    \\n\\nElasticsearch replaces string type with two new types text and keyword.\\n\\n  On using text types for full text search and keyword type for keyword search in Elasticsearch 5.0.\\n\\n\\n  \\n\\n  \\n    \\n    \\n  \\n\\n  \\n\\n'},\n",
       " {'text_field': 'Just wanted to post an update to this thread with my particular solution in-case anyone else finds this thread in the future.\\nLong story short, I did end up re-creating all of my old indices that were originally created using ES 5.3 and imported into my 6.8 cluster using index mappings that did not have multiple document mapping types.\\nOnce I created \"updated\" indices with mappings that conformed to ES 6.8 standards I used the reindex API to copy data from the old indices into the new indices using the code snippet below. The \"script\" portion converts all documents to _type doc.\\nPOST _reindex\\n{\\n  \"source\": {\\n    \"index\": \"OLD-INDEX\"\\n  },\\n  \"dest\": {\\n    \"index\": \"NEW-INDEX\"\\n  },\\n  \"script\": {\\n    \"source\": \"ctx._type = \\'doc\\'\",\\n    \"lang\": \"painless\"\\n  }\\n}\\n\\nOnce all of my original data was re-indexed into updated indices I deleted the corresponding original indices (after taking an index snapshot of course) and then updated the index pattern field list using \"management -&gt; index patterns -&gt; $select_your_index -&gt; refresh field list\".\\nFinally, I updated my visualizations / saved searches using the updated \"parameter.keyword\" fields.'},\n",
       " {'text_field': 'I noticed that copy_to fields work for aggregations but don\\'t get show up in a Match all query or any similar query. Upon further searching I figured setting \"store\": true in the Field mappings should help but I still was not able to see them in searches.\\nThis was the syntax I was trying to use:\\nGET index/_search\\n    {\\n      \"stored_fields\": [\\n        \"CopyField\"\\n      ],\\n      \"query\": {\\n        \"match_all\": {}\\n      }\\n    }\\n\\nAm I missing something here?'},\n",
       " {'text_field': 'Sorry it took time to answer.\\nI tried your example. Here a full script:\\nPUT index\\n{\\n  \"mappings\": {\\n    \"properties\": {\\n      \"ExistingField\": {\\n        \"type\": \"date\",\\n        \"copy_to\": \"CopiedField\"\\n      },\\n      \"CopiedField\": {\\n        \"type\": \"date\",\\n        \"store\": true\\n      }\\n    }\\n  }\\n}\\n\\nPUT index/_doc/1\\n{\\n  \"ExistingField\": \"2019-11-25\"\\n}\\nGET index/_search\\n{\\n  \"stored_fields\": [\\n    \"CopiedField\"]\\n}\\n\\nThis gives:\\n{\\n  \"took\" : 2,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 1,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 1.0,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"index\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"1\",\\n        \"_score\" : 1.0,\\n        \"fields\" : {\\n          \"CopiedField\" : [\\n            \"2019-11-25T00:00:00.000Z\"\\n          ]\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nSo everything looks fine on my side. I tested with 7.4.2.'},\n",
       " {'text_field': 'hi expert,\\nI\\'m still newbie in ES. I have problem with my logstash where there are some failed that no index in my ES after load the config. Below is my log file and show the failed to execute action. Please advice further\\n[2019-10-17T11:17:50,416][WARN ][logstash.config.source.multilocal] Ignoring the \\'pipelines.yml\\' file because modules or command line options are specified\\n[2019-10-17T11:17:50,448][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"7.3.0\"}\\n[2019-10-17T11:17:53,314][INFO ][org.reflections.Reflections] Reflections took 57 ms to scan 1 urls, producing 19 keys and 39 values\\n[2019-10-17T11:17:56,853][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;, :added=&gt;[http://10.10.130.20:9200/]}}\\n[2019-10-17T11:17:57,199][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=&gt;\"http://10.10.130.20:9200/\"}\\n[2019-10-17T11:17:57,344][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=&gt;7}\\n[2019-10-17T11:17:57,352][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the type event field won\\'t be used to determine the document _type {:es_version=&gt;7}\\n[2019-10-17T11:17:57,410][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=&gt;\"LogStash::Outputs::ElasticSearch\", :hosts=&gt;[\"//10.10.130.20:9200\"]}\\n[2019-10-17T11:17:57,534][INFO ][logstash.outputs.elasticsearch] Using default mapping template\\n[2019-10-17T11:17:57,620][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been create for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team.\\n[2019-10-17T11:17:57,628][INFO ][logstash.javapipeline    ] Starting pipeline {:pipeline_id=&gt;\"main\", \"pipeline.workers\"=&gt;4, \"pipeline.batch.size\"=&gt;125, \"pipeline.batch.delay\"=&gt;50, \"pipeline.max_inflight\"=&gt;500, :thread=&gt;\"#&lt;Thread:0x380fb0a4 run&gt;\"}\\n[2019-10-17T11:17:57,643][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=&gt;{\"index_patterns\"=&gt;\"logstash-\", \"version\"=&gt;60001, \"settings\"=&gt;{\"index.refresh_interval\"=&gt;\"5s\", \"number_of_shards\"=&gt;1}, \"mappings\"=&gt;{\"dynamic_templates\"=&gt;[{\"message_field\"=&gt;{\"path_match\"=&gt;\"message\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false}}}, {\"string_fields\"=&gt;{\"match\"=&gt;\"\", \"match_mapping_type\"=&gt;\"string\", \"mapping\"=&gt;{\"type\"=&gt;\"text\", \"norms\"=&gt;false, \"fields\"=&gt;{\"keyword\"=&gt;{\"type\"=&gt;\"keyword\", \"ignore_above\"=&gt;256}}}}}], \"properties\"=&gt;{\"@timestamp\"=&gt;{\"type\"=&gt;\"date\"}, \"@version\"=&gt;{\"type\"=&gt;\"keyword\"}, \"geoip\"=&gt;{\"dynamic\"=&gt;true, \"properties\"=&gt;{\"ip\"=&gt;{\"type\"=&gt;\"ip\"}, \"location\"=&gt;{\"type\"=&gt;\"geo_point\"}, \"latitude\"=&gt;{\"type\"=&gt;\"half_float\"}, \"longitude\"=&gt;{\"type\"=&gt;\"half_float\"}}}}}}}\\n[2019-10-17T11:17:59,423][ERROR][logstash.javapipeline    ] Pipeline aborted due to error {:pipeline_id=&gt;\"main\", :exception=&gt;#&lt;TypeError: no implicit conversion of Integer into String&gt;, :backtrace=&gt;[\"uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/date/format.rb:335:in _parse\\'\", \"uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/date.rb:734:inparse\\'\", \"C:/Elastic/logstash-7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/value_tracking.rb:87:in set_value\\'\", \"C:/Elastic/logstash-7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/value_tracking.rb:36:ininitialize\\'\", \"C:/Elastic/logstash-7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/value_tracking.rb:29:in build_last_value_tracker\\'\", \"C:/Elastic/logstash-7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/inputs/jdbc.rb:216:inregister\\'\", \"C:/Elastic/logstash-7.3.0/logstash-core/lib/logstash/java_pipeline.rb:192:in block in register_plugins\\'\", \"org/jruby/RubyArray.java:1792:ineach\\'\", \"C:/Elastic/logstash-7.3.0/logstash-core/lib/logstash/java_pipeline.rb:191:in register_plugins\\'\", \"C:/Elastic/logstash-7.3.0/logstash-core/lib/logstash/java_pipeline.rb:292:instart_inputs\\'\", \"C:/Elastic/logstash-7.3.0/logstash-core/lib/logstash/java_pipeline.rb:248:in start_workers\\'\", \"C:/Elastic/logstash-7.3.0/logstash-core/lib/logstash/java_pipeline.rb:146:inrun\\'\", \"C:/Elastic/logstash-7.3.0/logstash-core/lib/logstash/java_pipeline.rb:105:in `block in start\\'\"], :thread=&gt;\"#&lt;Thread:0x380fb0a4 run&gt;\"}\\n[2019-10-17T11:17:59,449][ERROR][logstash.agent           ] Failed to execute action {:id=&gt;:main, :action_type=&gt;LogStash::ConvergeResult::FailedAction, :message=&gt;\"Could not execute action: PipelineAction::Create, action_result: false\", :backtrace=&gt;nil}\\n[2019-10-17T11:17:59,796][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n[2019-10-17T11:18:04,670][INFO ][logstash.runner          ] Logstash shut down.'},\n",
       " {'text_field': '\\n\\n\\n dean92:\\n\\n{:pipeline_id=&gt;\"main\", :exception=&gt;#&lt;TypeError: no implicit conversion of Integer into String&gt;, :backtrace=&gt;[\"uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/date/format.rb:335:in _parse\\'\", \"uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/date.rb:734:in parse\\'\", \"C:/Elastic/logstash-7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/value_tracking.rb:87:in set_value\\'\", \"C:/Elastic/logstash-7.3.0/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/plugin_mixins/jdbc/value_tracking.rb:36\\n\\n\\nThe jdbc input is failing to parse the sql_last_value that it has persisted. This could happen if you update the query so that it tracks a sequence rather than a timestamp (or vice versa). The default location for the file it persists the value in is \"#{ENV[\\'HOME\\']}/.logstash_jdbc_last_run\", try removing that file.'},\n",
       " {'text_field': 'Hello Team,\\nI need to send data of s3 input data to multiple pipelines based on some regex in the log path.\\ncan anyone here me here.\\n@andrewkroh'},\n",
       " {'text_field': 'You can use output.elasticsearch.indices to route data to pipelines conditionally.\\nhttps://www.elastic.co/guide/en/beats/filebeat/current/elasticsearch-output.html#indices-option-es'},\n",
       " {'text_field': 'Can short links be generated only by  superuser?'},\n",
       " {'text_field': 'Have you also given the user the kibana_user role?'},\n",
       " {'text_field': 'es version: 6.5.1 docker\\ncluster machine: 3* 16c64G\\nkibana monitor show:\\n\\nimage.png2958×472 86.2 KB\\n\\nstatus:\\nI have a index, which has 196mill docs and the total size is 377GB, mapping data:\\n{\\n  \"mapping\": {\\n    \"t\": {\\n      \"properties\": {\\n....\\n        \"briefIntroduction\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        },\\n        \"business\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        },\\n        \"businessId\": {\\n          \"type\": \"long\"\\n        },\\n        \"businessScope\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        },\\n        \"capitalUnit\": {\\n          \"type\": \"keyword\"\\n        },\\n        \"cityCode\": {\\n          \"type\": \"keyword\"\\n        },\\n        \"clueSource\": {\\n          \"type\": \"long\"\\n        },\\n        \"clue_relation\": {\\n          \"type\": \"join\",\\n          \"eager_global_ordinals\": true,\\n          \"relations\": {\\n            \"company\": [\\n              \"product\",\\n              \"to_clue\",\\n              \"statistics\",\\n              \"browse\"\\n            ]\\n          }\\n        },\\n        \"companyId\": {\\n          \"type\": \"long\"\\n        },\\n        \"companyName\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\"\\n            }\\n          },\\n          \"analyzer\": \"index_ansj\"\\n        },\\n        \"companyNature\": {\\n          \"type\": \"long\"\\n        },\\n        \"companyOrgType\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        },\\n        \"companyScale\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        },\\n        \"companyScore\": {\\n          \"type\": \"long\"\\n        },\\n        \"competingCount\": {\\n          \"type\": \"long\"\\n        },\\n        \"createTime\": {\\n          \"type\": \"date\"\\n        },\\n        \"dataType\": {\\n          \"type\": \"long\"\\n        },\\n        \"detailUrl\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        },\\n        \"enterpriseId\": {\\n          \"type\": \"long\"\\n        },\\n        \"establishTime\": {\\n          \"type\": \"date\"\\n        },\\n        \"fromTime\": {\\n          \"type\": \"date\"\\n        },\\n        \"fromUserId\": {\\n          \"type\": \"long\"\\n        },\\n        \"fromUserName\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        },\\n        \"id\": {\\n          \"type\": \"long\"\\n        },\\n        \"industryCode\": {\\n          \"type\": \"keyword\"\\n        },\\n        \"intro\": {\\n          \"type\": \"text\",\\n          \"fields\": {\\n            \"keyword\": {\\n              \"type\": \"keyword\",\\n              \"ignore_above\": 256\\n            }\\n          }\\n        }\\n....\\n      }\\n    }\\n  }\\n}\\n\\nproblem:\\nAfter insert about 30 docs, I find that the doc cannot be searched in 10 second, so i invoke the \\'clue-test/_refresh \\' api in manually, I found that it take 10+ seconds to response, sometimes it takes 20+ second, in meaning time I only see the log \" overhead, spent [490ms] collecting in the last [1s]\", I don\\'t know what happened in meaning time.'},\n",
       " {'text_field': 'The docs have some info on this: https://www.elastic.co/guide/en/elasticsearch/reference/master/parent-join.html#_global_ordinals'},\n",
       " {'text_field': \"I have some event logs in elasticsearch. Each log has timestamp, action and userId. I want to find the user whose logs have the time sequential condition. Here is some sample data.\\n+---------------------+---------+---------+\\n| timestamp           | action  | userId  |\\n+---------------------+---------+---------+\\n| 2019-10-14 08:34:03 |    A    | userA   |\\n+---------------------+---------+---------+\\n| 2019-10-14 11:39:26 |    A    | userB   |\\n+---------------------+---------+---------+\\n| 2019-10-14 11:41:23 |    A    | userA   |\\n+---------------------+---------+---------+\\n| 2019-10-14 12:10:56 |    B    | userB   |\\n+---------------------+---------+---------+\\n| 2019-10-14 12:51:42 |    B    | userA   |\\n+---------------------+---------+---------+\\n| 2019-10-14 15:23:48 |    C    | userA   |\\n+---------------------+---------+---------+\\n| 2019-10-14 16:05:19 |    B    | userB   |\\n+---------------------+---------+---------+\\n| 2019-10-14 19:44:01 |    A    | userC   |\\n+---------------------+---------+---------+\\n| 2019-10-15 03:18:15 |    D    | userA   |\\n+---------------------+---------+---------+\\n\\nFor example, condition is action: A -&gt; action: B -&gt; action:C then I should get userA. Is elasticsearch can do this ? If elasticsearch can't do this, is there any tool(bigquery or something else) or database can do this? Or it just needs multi query then writing some code to process by myself, or even this question needs machine learning ?\"},\n",
       " {'text_field': 'You can answer this type of question with aggregations, or the new transforms feature in Elasticsearch (if the data is large and continuous).\\nFor example,\\nPUT spans\\n{\\n  \"mappings\": {\\n    \"properties\": {\\n      \"timestamp\":    { \"type\": \"date\" },  \\n      \"action\":  { \"type\": \"keyword\"  }, \\n      \"userId\":   { \"type\": \"keyword\"  }     \\n    }\\n  }\\n}\\n\\nPOST _bulk\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T08:34:03\",\"action\":\"A\",\"userId\":\"userA\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T11:39:26\",\"action\":\"A\",\"userId\":\"userB\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T11:41:23\",\"action\":\"A\",\"userId\":\"userA\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T12:10:56\",\"action\":\"B\",\"userId\":\"userB\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T12:51:42\",\"action\":\"B\",\"userId\":\"userA\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T15:23:48\",\"action\":\"C\",\"userId\":\"userA\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T16:05:19\",\"action\":\"B\",\"userId\":\"userB\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-14T19:44:01\",\"action\":\"A\",\"userId\":\"userC\"}\\n{\"index\":{\"_index\":\"spans\"}}\\n{\"timestamp\":\"2019-10-15T03:18:15\",\"action\":\"D\",\"userId\":\"userA\"}\\n\\nPUT _data_frame/transforms/transaction\\n{\\n  \"source\": {\\n    \"index\": [\\n      \"spans\"\\n    ]\\n  },\\n  \"dest\": {\\n    \"index\": \"transactions\"\\n  },\\n  \"pivot\": {\\n    \"group_by\": {\\n      \"date\": {\\n        \"terms\": {\\n          \"field\": \"userId\"\\n        }\\n      }\\n    },\\n    \"aggregations\": {\\n      \"min_timestamp\": {\\n        \"min\": {\\n          \"field\": \"timestamp\"\\n        }\\n      },\\n      \"max_timestamp\": {\\n        \"max\": {\\n          \"field\": \"timestamp\"\\n        }\\n      },\\n      \"duration\": {\\n        \"bucket_script\": {\\n          \"buckets_path\": {\\n            \"max\": \"max_timestamp\",\\n            \"min\": \"min_timestamp\"\\n          },\\n          \"script\": \"params.max-params.min\"\\n        }\\n      },\\n      \"trace\": {\\n        \"scripted_metric\": {\\n          \"init_script\": \"state.spans = []\",\\n          \"map_script\": \"\"\"\\n            Map span = [\\n              \\'timestamp\\':doc[\\'timestamp\\'].value, \\n              \\'action\\':doc[\\'action\\'].value\\n            ]; \\n            state.spans.add(span)\\n          \"\"\",\\n          \"combine_script\": \"return state.spans\",\\n          \"reduce_script\": \"\"\"\\n            def ret = []; \\n            for (s in states) { \\n              for (span in s) { \\n                ret.add(span); \\n              }\\n            }\\n            ret.sort((HashMap o1, HashMap o2)-&gt;o1[\\'timestamp\\'].toInstant().toEpochMilli().compareTo(o2[\\'timestamp\\'].toInstant().toEpochMilli())); \\n            return ret;\\n            \"\"\"\\n        }\\n      },\\n      \"signature\": {\\n        \"scripted_metric\": {\\n          \"init_script\": \"state.spans = []\",\\n          \"map_script\": \"\"\"\\n            Map span = [\\n              \\'timestamp\\':doc[\\'timestamp\\'].value, \\n              \\'action\\':doc[\\'action\\'].value\\n            ]; \\n            state.spans.add(span)\\n          \"\"\",\\n          \"combine_script\": \"return state.spans\",\\n          \"reduce_script\": \"\"\"\\n            def ret = []; \\n            for (s in states) { \\n              for (span in s) { \\n                ret.add(span); \\n              }\\n            }\\n            ret.sort((HashMap o1, HashMap o2)-&gt;o1[\\'timestamp\\'].toInstant().toEpochMilli().compareTo(o2[\\'timestamp\\'].toInstant().toEpochMilli()));\\n            def signature = \"\";\\n            for (span in ret) {\\n              signature += span[\\'action\\']\\n            }\\n            return signature;\\n            \"\"\"\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nPOST _data_frame/transforms/transaction/_start\\n\\nGET transactions/_search\\n{\\n  \"query\": {\\n    \"wildcard\": {\\n      \"signature.keyword\": \"*ABCD*\"\\n    }\\n  }\\n}\\n\\nThe transform, transforms the raw data to an entity centric index around user:\\n{\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 3,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 1.0,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"transactions\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"dVXi9btFe7o12AMiSauzNKgAAAAAAAAA\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"date\" : \"userA\",\\n          \"duration\" : 6.7452E7,\\n          \"trace\" : [\\n            {\\n              \"action\" : \"A\",\\n              \"timestamp\" : \"2019-10-14T08:34:03.000Z\"\\n            },\\n            {\\n              \"action\" : \"A\",\\n              \"timestamp\" : \"2019-10-14T11:41:23.000Z\"\\n            },\\n            {\\n              \"action\" : \"B\",\\n              \"timestamp\" : \"2019-10-14T12:51:42.000Z\"\\n            },\\n            {\\n              \"action\" : \"C\",\\n              \"timestamp\" : \"2019-10-14T15:23:48.000Z\"\\n            },\\n            {\\n              \"action\" : \"D\",\\n              \"timestamp\" : \"2019-10-15T03:18:15.000Z\"\\n            }\\n          ],\\n          \"signature\" : \"AABCD\",\\n          \"min_timestamp\" : \"2019-10-14T08:34:03.000Z\",\\n          \"max_timestamp\" : \"2019-10-15T03:18:15.000Z\"\\n        }\\n      },\\n      ...\\n}\\n\\nA really useful feature is that this transforms can be continuous (i.e. the transactions index will be automatically updated when new items are added to spans.'},\n",
       " {'text_field': 'Hello,\\nI have elasticsearch server hosted on virtual machine in azure cloud. I have an azure default directory and users added to it. I want to login to elasticsearch using credentials for the users in active directory. I tried the setup by adding realm in elasticsearch configuration as given below but unable to achieve it. Getting\\n\\nAn error occurred while attempting to establish a connection to server detaropwmail2com.onmicrosoft.com/127.0.0.1:636:  ConnectException(Connection refused), ldapSDKVersion=4.0.8, revision=28812\\n\\nconfig\\nxpack:\\nsecurity:\\nauthc:\\nrealms:\\nactive_directory:\\nmy_ad:\\norder: 0\\ndomain_name: detaropwmail2com.onmicrosoft.com\\nurl: ldap://detaropwmail2com.onmicrosoft.com:636'},\n",
       " {'text_field': \"I can only imagine that the DN of your user in your AD is not cn=sanket07,dc=prospera,dc=onmicrosoft,dc=com.\\nYou need to figure out what the DN of your user is and use that in the configuration. The easiest way to do this is from within your Active Directory instance, so please try this first.\\nIf you can't figure this out from AD, then you could enable extra logging in elasticsearch and specifically set\\norg.elasticsearch.xpack.security.authc.support.mapper\\n\\nand\\norg.elasticsearch.xpack.security.authc.ldap\\n\\nto TRACE and look at the Elasticsearch logs, as these will contain the information retrieved for your user from AD ( and the DN among them )\"},\n",
       " {'text_field': \"Is there any way to get a list of indices that currently have lifecycle errors? I'm trying to setup a watcher job that will send a notification based on whether there are any such indices.\\nThanks.\"},\n",
       " {'text_field': 'Hey,\\nthe indices which have errors should show up in Kibana in the Indexmanagement section\\nhttps://www.elastic.co/guide/en/kibana/current/managing-indices.html\\nYou should also be able to query managed indices via ILM, to get only the ones in error state\\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.4/ilm-explain-lifecycle.html'},\n",
       " {'text_field': 'Hey guys,\\nI was wondering if I can see location of IP address when insert an IP address from text file via logstash with filter for geoip in it.\\nI was trying it with Apache log line sample from  GeoIP in the Elastic Stack tutorial. I created a new index\\nPUT my_index\\n{\\n  \"mappings\": {\\n    \"properties\": {\\n      \"location\": {\\n        \"type\": \"geo_point\"\\n      }\\n    }\\n  }\\n}\\n\\nAnd then changed filter in logstash config file for:\\nfilter {\\n  grok { match =&gt; { \"message\" =&gt; \"%{COMBINEDAPACHELOG}\" } }\\n  geoip { source =&gt; \"clientip\" }\\n}\\n\\nThe output I see in Kibana:\\n               Time \\t       geoip.ip  \\tgeoip.timezone      geoip.location.lat    \\tgeoip.location.lon    \\t geoip.continent_code  \\nOct 17, 2019 @ 11:41:23.231\\t162.156.141.3\\tEurope/London\\t         51.496\\t               -0.122\\t                     EU\\n\\nWhen I try to make a new visualisation with Coordinate Map  I have there geopoint location but no points in map are shown.\\nIn Index Pattern settings I don\\'t have field clientip as geo_point but String.\\nThanks for any help!'},\n",
       " {'text_field': 'The default target for a geoip filter is a field called geoip. The default template for indexes that match logstash-* makes that a geo_point.\\nYou are on the right lines, but you need to make  [geoip][location] a geo_point, not [location]. See the template above for how to do that.'},\n",
       " {'text_field': 'Here are examples for 1 snapshot restore\\ncurl -u ${USERNAME}:${PASSWORD} -X POST \"localhost:9200/_snapshot/s3-backup/curator-20191017074006/_restore?pretty\"\\ncurl -u ${USERNAME}:${PASSWORD} -X POST -H \"Content-Type: application/json\"  \"localhost:9200/_snapshot/s3-backup/curator-20191017074006/_restore?pretty\" -d \\\\\\n\\'{\\n  \"indices\": \"index-*\",\\n  \"ignore_unavailable\": true,\\n  \"include_global_state\": true,\\n  \"include_aliases\": false,\\n  \"partial\": false,\\n  \"rename_pattern\": \"index(.+)\",\\n  \"rename_replacement\": \"restored-index$1\"\\n}\\'\\n\\nQuestion is, how to restore all snapshots curator-* for particular repository?'},\n",
       " {'text_field': '\\n\\n\\n O_K:\\n\\nI was confused, as read somewhere that snapshot creation is incremental.\\n\\n\\nThat\\'s an excellent question! Where incremental means capturing changes to files when considered from a SysAdmin perspective, in Elasticsearch, it means something different.\\nThink of an index as an odd matryoshka doll, with the outermost layer as the index itself. The next layer inside is the shards of an index. There remains another layer deeper than the shards, however: segments. Segments are the building blocks of shards. With this understanding, hopefully the following will be a sufficient explanation.\\nAs new data comes in to Elasticsearch, the data is flushed in blocks into segments. A segment is completely immutable, and cannot be changed. If data is changed via an update request, the document in that segment is marked for deletion, and the newer document (which exists in a different segment) is considered the valid one. Documents marked for deletion are tracked, and then at segment merges, they are expunged. As the count of segments continuously increases as new data is added, segment merges are continuously happening behind the scenes, automatically. This is because too many segments would eat up all of the free memory in the JVM. A merged segment is the combination of the data from one or more other smaller segments. The new segment is just as immutable as the older ones, but it is new. This understanding is critical to understanding how snapshots work.\\nWhen a snapshot is initiated by an API call, Elasticsearch checks to see which indices are chosen, and then looks at the repository to see if any data is on the remote side. What does Elasticsearch look for? Segments. When a snapshot is initialized all existing segments in the selected indices are blocked from being deleted. This state persists until the snapshot is completed. Elasticsearch compares all segments already existing in the repository (if any) with the segments from the selected indices. Only segments not found in the repository are copied over. The repository and snapshot metadata track which segments are required to rebuild the selected indices with \"pointers.\" A subsequent incremental snapshot of the exact same indices as the previous snapshot will only copy new segments, and will also contain pointers to the necessary segments which had been copied in the previous snapshot. This is because it is necessary to have those segments as well, in order to completely recreate the index as it was at the time of the snapshot.\\nThis is how incremental snapshots work: at the segment level. It is important to note the difference between this, and presuming a snapshot works at the data level, like a filesystem backup. As segments merge, the data may not have even changed a bit, but the segments are new. This means that there may very likely be data duplication within a repository, even though there will never be segment duplication.\\nWays to prevent this level of duplication include having different snapshot naming conventions, so that you can keep 3 days of hourly snapshots of even your live data, but take daily snapshots of older, unchanging indices (a logging and metrics use case is presumed for this example). With this, you can continue to have hourly data snapshots, but not keep months worth of duplicated data due to frequent segment merges. In many cases, Curator has been used with complex action files, which will force merge old, unchanging indices to a single segment per shard, and then snapshot this data for longer term storage, as the number of segments which need comparison at snapshot initialization time will be considerably smaller.\\n\\n\\n\\n O_K:\\n\\nOne more question, do you know how to keep repository folder and files in s3 bucket more structured? Currently I have some random names there, it would be nice to keep snapshots in folders withYYYY-MM-DD entry per daily snapshot\\n\\n\\nYou should never directly interact with the files in your repository. The only proper way to interface with a snapshot repository is via the API. Those random names are the ones generated by and maintained in the cluster state. Changing them would break things catastrophically.'},\n",
       " {'text_field': 'Greetings. We are currently trying to ingest logs from a Windows server with a system language other than English. Because of that, the logs retrieved by Winlogbeat are not in English, and we need to get them in line with the logs from our other Windows servers. Changing the system language is not an option.\\nFrom what I could find, it seems that Windows event logs are not stored in a language-dependent format, and are printed in accordance with the system language, so it should be possible to read them in another language, but there is surprisingly little knowledge on that matter.\\nWe tried to change the language of the user Winlogbeat runs as, but that did not help. Does anyone know if there is a workaround that would allow us to forward the logs in English?'},\n",
       " {'text_field': 'Hello,\\nwinlogbeat currently sets the language value to 0 which means to use the system default.  This is not currently configurable.  I would recommend making a new enhancement request here to make this language setting user configurable.\\n\\n  \\n      \\n      GitHub\\n  \\n  \\n    \\n\\nBuild software better, together\\n\\nGitHub is where people build software. More than 40 million people use GitHub to discover, fork, and contribute to over 100 million projects.\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nPlease let us know if you have further questions.'},\n",
       " {'text_field': 'Hi,\\nIs there any performance difference between the inline script with parameters and stored script? And also how is elasticsearch not compiling the script if I use the same inline script with parameters multiple times?\\nThanks'},\n",
       " {'text_field': 'Whether a script is stored or inline does not change if the script needs to be recompiled. The same inline script with different parameters should be cached, just like a stored script would be. The cache key is always the content of the script itself. Params are not part of this because they are runtime input to the script and do not change the compilation.\\nThere should not be any performance difference between the two. The reason for stored scripts is to simplify reusing the same script without needing to store it client side.'},\n",
       " {'text_field': \"I have several Coordinate Maps to try to identify certain things in my data, and I have these together in a dashboard. However, all the maps show the same areas, with all their ranges starting at 0, and this is making it harder to locate useful data, especially if the information is in the lower quarter. The metric aggergator I am on the map is SUM. For each document I am interested in has a field either set to 1, or doesn't exist.\\nIs there a way to get the coordinate map to not show the 0 values, or to at least start the ranges from 1?\\nPreviously, I had success by using a separate index pattern for each map, but this is now resulting in close to 30 separate index patterns, and several cloned index entries (with differently named indexes), but this is quickly becoming untannable. As I go along, I can see this easily approaching 100 separate maps.\"},\n",
       " {'text_field': 'Ah, success! Instead of using Coordinate Map Visualizations, I used the Maps application to create the views, which does include a Filter option.'},\n",
       " {'text_field': 'I have two indexes  t-click and t-impression.  I created a common index as t*.\\nindex t-click\\ntid: \"sdalfk-234k\"\\nevent: \"click\"\\ntid: \"sdalfk-234k\"\\nevent: \"click\"\\nindex t-impression\\ntid: \"sdalfk-234k\"\\nsite: \"google.com\"\\nNow, I have to aggregate the websites of click by mapping tid of click with impression index and get the site field.  I imagine it as where condition in SQL but I\\'m aware its not same. I struck here, do let me know, if you need more information.'},\n",
       " {'text_field': 'To achieve the result, based on the way you have currently indexed your data, a join is required, something like:\\nSELECT count(\"t-click\".event) FROM \"t-click\" , \"t-impression\" WHERE \"t-click\".tid = \"t-impression.tid\" WHERE event = \\'click\\' GROUP BY  \"t-impression\".site\\n\\nUnfortunately joins are not supported by ES-SQL or ES search api.\\nYou\\'ll need to restructure and re-index your data by including the site on every event document.\\nYou may want to take a look at parent-child relationships too: https://www.elastic.co/guide/en/elasticsearch/reference/current/parent-join.html'},\n",
       " {'text_field': 'I need to split a log into multiple fields however the log has the same field name for multiple values and when I use the fiedl_split option of kv it will put all the the fields with the same key in the same field. How can I put the values into separate fields with distinct names.\\nThe desire output should be:\\npin=12345-0 d=123 A_foo=nice@bar.com B_foo=bobo C_foo=12345.\\nHowever what I\\'m getting right now is:\\npin=12345~0 d=123 foo= nice@bar.com, bobo, 1234\\n//pin=12345~0&amp;d=123&amp;foo=nice@bar.com&amp;foo=bobo&amp;foo=12345 //\\n//    filter {   //\\n//      kv {    //\\n//       field_split =&gt; \"&amp;?\" //\\n//     }        //\\n//    }        //'},\n",
       " {'text_field': 'You could do that using a ruby filter\\n    kv { field_split =&gt; \"&amp;\" }\\n    ruby {\\n        code =&gt; \\'\\n            foo = event.get(\"foo\")\\n            foo.each_index { |x|\\n                event.set(\"foo-#{x}\", foo[x])\\n            }\\n        \\'\\n    }'},\n",
       " {'text_field': 'Hi there,\\nDue to some custom requirements, I am attempting to ship my WAF logs into Elasticsearch using lambda.\\nThe function is fairly simple Python, as follows:\\nimport re\\nimport requests\\nfrom requests_aws4auth import AWS4Auth\\n\\nregion = \\'us-east-1\\' # e.g. us-west-1\\nservice = \\'es\\'\\ncredentials = boto3.Session().get_credentials()\\nawsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\\n\\nhost = \\'\\' # the Amazon ES domain, including https://\\nindex = \\'awswaf-\\'\\ntype = \\'waflog\\'\\nurl = host + \\'/\\' + index + \\'/\\' + type\\n\\nheaders = { \"Content-Type\": \"application/json\" }\\n\\ns3 = boto3.client(\\'s3\\')\\n\\n# Lambda execution starts here\\ndef handler(event, context):\\n    for record in event[\\'Records\\']:\\n\\n        # Get the bucket name and key for the new file\\n        bucket = record[\\'s3\\'][\\'bucket\\'][\\'name\\']\\n        key = record[\\'s3\\'][\\'object\\'][\\'key\\']\\n\\t\\t##Troubleshooting: Log results in console\\n        #print(\"Bucket :\", bucket)\\n        #print(\"Key :\", key)\\n\\t\\n        # Get, read, and split the file into lines\\n        obj = s3.get_object(Bucket=bucket, Key=key)\\n        body = obj[\\'Body\\'].read()\\n        lines = body.splitlines()\\n        \\n\\t\\t#Build and send the request\\n        document = lines\\n        r = requests.post(url, auth=awsauth, json=document, headers=headers)\\n        #Log response in console\\n        print(\"Response: \", r)\\n\\nThis is passing a WAF log which is just JSON (can provide an example if it would help) to the Elasticsearch domain, however I am getting a 400 Bad Request response for it.\\nI thought that this could be due to an issue with the indexing of the parameters, but I still received the same error after adding the following index:\\nPUT awswaf-\\n{\\n  \"mappings\" : {\\n        \"properties\" : {\\n          \"action\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"formatVersion\" : {\\n            \"type\" : \"long\"\\n          },\\n          \"httpRequest\" : {\\n            \"properties\" : {\\n              \"args\" : {\\n                \"type\" : \"text\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"keyword\",\\n                    \"ignore_above\" : 256\\n                  }\\n                }\\n              },\\n              \"clientIp\" : {\\n                \"type\" : \"keyword\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"ip\"\\n                  }\\n                }\\n              },\\n              \"country\" : {\\n                \"type\" : \"text\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"keyword\",\\n                    \"ignore_above\" : 256\\n                  }\\n                }\\n              },\\n              \"headers\" : {\\n                \"properties\" : {\\n                  \"name\" : {\\n                    \"type\" : \"text\",\\n                    \"fields\" : {\\n                      \"keyword\" : {\\n                        \"type\" : \"keyword\",\\n                        \"ignore_above\" : 256\\n                      }\\n                    }\\n                  },\\n                  \"value\" : {\\n                    \"type\" : \"text\",\\n                    \"fields\" : {\\n                      \"keyword\" : {\\n                        \"type\" : \"keyword\",\\n                        \"ignore_above\" : 256\\n                      }\\n                    }\\n                  }\\n                }\\n              },\\n              \"httpMethod\" : {\\n                \"type\" : \"text\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"keyword\",\\n                    \"ignore_above\" : 256\\n                  }\\n                }\\n              },\\n              \"httpVersion\" : {\\n                \"type\" : \"text\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"keyword\",\\n                    \"ignore_above\" : 256\\n                  }\\n                }\\n              },\\n              \"uri\" : {\\n                \"type\" : \"text\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"keyword\",\\n                    \"ignore_above\" : 256\\n                  }\\n                }\\n              }\\n            }\\n          },\\n          \"httpSourceId\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"httpSourceName\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"rateBasedRuleList\" : {\\n            \"properties\" : {\\n              \"limitKey\" : {\\n                \"type\" : \"text\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"keyword\",\\n                    \"ignore_above\" : 256\\n                  }\\n                }\\n              },\\n              \"maxRateAllowed\" : {\\n                \"type\" : \"long\"\\n              },\\n              \"rateBasedRuleId\" : {\\n                \"type\" : \"text\",\\n                \"fields\" : {\\n                  \"keyword\" : {\\n                    \"type\" : \"keyword\",\\n                    \"ignore_above\" : 256\\n                  }\\n                }\\n              }\\n            }\\n          },\\n          \"terminatingRuleId\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"terminatingRuleType\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          },\\n          \"timestamp\" : {\\n            \"type\" : \"date\",\\n            \"format\" : \"epoch_millis\"\\n          },\\n          \"webaclId\" : {\\n            \"type\" : \"text\",\\n            \"fields\" : {\\n              \"keyword\" : {\\n                \"type\" : \"keyword\",\\n                \"ignore_above\" : 256\\n              }\\n            }\\n          }\\n        }\\n      }\\n}\\n\\nCould you please help me to understand why the request is being rejected?'},\n",
       " {'text_field': 'Yes, a surrounding pair of quotes would indeed result in the exception you\\'re getting:\\n$ curl \\'http://localhost:9200/awswaf-/_doc\\' -H \\'Content-type: application/json\\' --data-binary $\\'\\\\\\'{\"terminatingRuleId\":\"Default_Action\",\"rateBasedRuleList\":[{\"limitKey\":\"IP\",\"rateBasedRuleId\":\"ea04e5d3-bcc1-499b-ab7a-583ef849d48a\",\"maxRateAllowed\":100}],\"terminatingRuleType\":\"REGULAR\",\"webaclId\":\"1950425e-9278-4c05-8c67-4d500b9659be\",\"action\":\"ALLOW\",\"httpRequest\":{\"requestId\":null,\"httpMethod\":\"GET\",\"args\":\"\",\"httpVersion\":\"HTTP/1.1\",\"country\":\"JP\",\"uri\":\"/\",\"headers\":[{\"value\":\"1.2.3.4\",\"name\":\"Host\"},{\"value\":\"HTTP Banner Detection (https://security.ipip.net)\",\"name\":\"User-Agent\"}],\"clientIp\":\"1.2.3.4\"},\"formatVersion\":1,\"httpSourceName\":\"ALB\",\"nonTerminatingMatchingRules\":[],\"httpSourceId\":\"639917743964-app/XXXXXXXXXX/2d9c1970fc485a41\",\"ruleGroupList\":[],\"timestamp\":1571137876737}\\\\\\'\\'\\n{\"error\":{\"root_cause\":[{\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\"}],\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\",\"caused_by\":{\"type\":\"not_x_content_exception\",\"reason\":\"Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes\"}},\"status\":400}\\n'},\n",
       " {'text_field': 'Hi,\\nHope you can help me with the issue I\\'m having.\\nCurrrent scenario:\\nWe are currently integrating both netflow and sflow in Elastic with logstash version 7.3.2 on a CenOS 7 server. We\\'ve managed to visualize netflow correctly but when trying to ingest sflow we are not seeing anything.\\nFor testing purpose, we are basically executing logstash from command line:\\nOn port 6344, we are receiving sflow from a NEXUS 3000 with the following configuration:\\nfeature sflow\\nsflow sampling-rate 4096\\nsflow max-sampled-size 128 -- 64\\nsflow counter-poll-interval 1\\nsflow max-datagram-size 1400\\nsflow collector-ip X.X.X.X vrf default\\nsflow collector-port 6344\\nsflow agent-ip Y.Y.Y.Y\\nno sflow extended switch\\n...\\nsflow data-source interface ...\\n\\nCommand line:\\n[root@xxxxxxxx ~]# /usr/share/logstash/bin/logstash -e \\'input { udp { port =&gt; 6344 }}\\' --debug \\n...\\n[DEBUG] 2019-10-17 18:06:53.529 [Api Webserver] agent - Trying to start WebServer {:port=&gt;9600}\\n[INFO ] 2019-10-17 18:06:53.591 [[main]&lt;udp] udp - UDP listener started {:address=&gt;\"0.0.0.0:6344\", :receive_buffer_bytes=&gt;\"106496\", :queue_size=&gt;\"2000\"}\\n[DEBUG] 2019-10-17 18:06:53.614 [Api Webserver] service - [api-service] start\\n[INFO ] 2019-10-17 18:06:53.919 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=&gt;9600}\\n[DEBUG] 2019-10-17 18:06:53.307 [[main]&gt;worker1] CompiledPipeline - Compiled output\\n...\\n[DEBUG] 2019-10-17 18:06:55.577 [pool-3-thread-2] jvm - collector name {:name=&gt;\"ParNew\"}\\n[DEBUG] 2019-10-17 18:06:55.580 [pool-3-thread-2] jvm - collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[DEBUG] 2019-10-17 18:06:58.261 [logstash-pipeline-flush] PeriodicFlush - Pushing flush onto pipeline.\\n[DEBUG] 2019-10-17 18:07:00.596 [pool-3-thread-1] jvm - collector name {:name=&gt;\"ParNew\"}\\n[DEBUG] 2019-10-17 18:07:00.597 [pool-3-thread-1] jvm - collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[DEBUG] 2019-10-17 18:07:03.261 [logstash-pipeline-flush] PeriodicFlush - Pushing flush onto pipeline.\\n[DEBUG] 2019-10-17 18:07:05.607 [pool-3-thread-1] jvm - collector name {:name=&gt;\"ParNew\"}\\n[DEBUG] 2019-10-17 18:07:05.608 [pool-3-thread-1] jvm - collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[DEBUG] 2019-10-17 18:07:08.261 [logstash-pipeline-flush] PeriodicFlush - Pushing flush onto pipeline.\\n[DEBUG] 2019-10-17 18:07:10.621 [pool-3-thread-1] jvm - collector name {:name=&gt;\"ParNew\"}\\n[DEBUG] 2019-10-17 18:07:10.622 [pool-3-thread-1] jvm - collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[DEBUG] 2019-10-17 18:07:13.261 [logstash-pipeline-flush] PeriodicFlush - Pushing flush onto pipeline.\\n...\\n\\n(same thing on and on)\\nAs you can see NO data is coming through logstash nor error or warning shown, but if we capture paquets, we can see sflow is coming in through port 6344:\\n[root@xxxxxxxx ~]# tcpdump -vvv -i any port 6344\\ntcpdump: listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\\n13:05:30.791669 IP (tos 0x0, ttl 63, id 8334, offset 0, flags [none], proto UDP (17), length 1128)\\n    10.5.0.74.57829 &gt; elastic-netflow.6344: [udp sum ok] UDP, length 1100\\n13:05:30.803948 IP (tos 0x0, ttl 63, id 8335, offset 0, flags [none], proto UDP (17), length 1376)\\n    10.5.0.74.57829 &gt; elastic-netflow.6344: [udp sum ok] UDP, length 1348\\n13:05:30.815049 IP (tos 0x0, ttl 63, id 8336, offset 0, flags [none], proto UDP (17), length 1316)\\n    10.5.0.74.57829 &gt; elastic-netflow.6344: [udp sum ok] UDP, length 1288\\n...\\n\\nSame test, but this time listening on port 6343 where we are receiving sFlow traffic from the hsflowd agent installed locally:\\n[root@xxxxxxxx ~]# /usr/share/logstash/bin/logstash -e \\'input { udp { port =&gt; 6343 }}\\' --debug \\n...\\n[INFO ] 2019-10-17 18:10:36.509 [Ruby-0-Thread-1: /usr/share/logstash/lib/bootstrap/environment.rb:6] agent - Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]}\\n[INFO ] 2019-10-17 18:10:36.562 [[main]&lt;udp] udp - Starting UDP listener {:address=&gt;\"0.0.0.0:6343\"}\\n[DEBUG] 2019-10-17 18:10:36.662 [Api Webserver] agent - Starting puma\\n[INFO ] 2019-10-17 18:10:36.701 [[main]&lt;udp] udp - UDP listener started {:address=&gt;\"0.0.0.0:6343\", :receive_buffer_bytes=&gt;\"106496\", :queue_size=&gt;\"2000\"}\\n[DEBUG] 2019-10-17 18:10:36.725 [Api Webserver] agent - Trying to start WebServer {:port=&gt;9600}\\n[DEBUG] 2019-10-17 18:10:36.831 [Api Webserver] service - [api-service] start\\n[INFO ] 2019-10-17 18:10:37.244 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=&gt;9600}\\n[DEBUG] 2019-10-17 18:10:36.318 [[main]&gt;worker1] CompiledPipeline - Compiled output\\n...\\n[DEBUG] 2019-10-17 18:10:38.826 [pool-3-thread-2] jvm - collector name {:name=&gt;\"ParNew\"}\\n[DEBUG] 2019-10-17 18:10:38.829 [pool-3-thread-2] jvm - collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[DEBUG] 2019-10-17 18:10:41.376 [logstash-pipeline-flush] PeriodicFlush - Pushing flush onto pipeline.\\n[WARN ] 2019-10-17 18:10:43.737 [&lt;udp.1] plain - Received an event that has a different character encoding than you configured. {:text=&gt;\"\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0005\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0001\\\\\\\\n\\\\\\\\u0001P:\\\\\\\\u0000\\\\\\\\u0001\\\\\\\\x86\\\\\\\\xA0\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u00021\\\\\\\\u00013\\\\\\\\xAB.\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u..\\n[DEBUG] 2019-10-17 18:10:43.858 [pool-3-thread-2] jvm - collector name {:name=&gt;\"ParNew\"}\\n[DEBUG] 2019-10-17 18:10:43.859 [pool-3-thread-2] jvm - collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated\\n{\\n      \"@version\" =&gt; \"1\",\\n    \"@timestamp\" =&gt; 2019-10-17T16:10:43.761Z,\\n          \"host\" =&gt; \"10.1.85.21\",\\n       \"message\" =&gt; \"\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0005\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0001\\\\\\\\n\\\\\\\\u0001P:\\\\\\\\u0000\\\\\\\\u0001\\\\\\\\x86\\\\\\\\xA0\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u00021\\\\\\\\u00013\\\\\\\\xAB.\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0000\\\\\\\\u0...\\n}\\n[DEBUG] 2019-10-17 18:10:46.376 [logstash-pipeline-flush] PeriodicFlush - Pushing flush onto pipeline.\\n\\nThis time we are able to see packets received on port 6343.\\n[root@xxxxxxxx ~]# tcpdump -vvv -i any -s 0 port 6343\\n...\\n17:43:44.105817 IP (tos 0x0, ttl 64, id 38404, offset 0, flags [DF], proto UDP (17), length 772)\\n    elastic-netflow.56817 &gt; elastic-netflow.sflow: [bad udp cksum 0xc12d -&gt; 0x0c08!] sFlowv5, IPv4 agent elastic-netflow, agent-id 100000, seqnum 516, uptime 18543802, samples 1, length 744\\n        counter sample (2), length 708, seqnum 516, type 2, idx 1, records 10\\n            enterprise 0, Unknown (2001) length 36\\n            enterprise 0, Unknown (2010) length 28\\n\\t\\t\\t...\\n\\nAny idea? Why are we not able to see the received sflow packets from the NEXUS 3000 device?\\nThanks in advance!!!'},\n",
       " {'text_field': \"I found out what was happening!\\nIt looked like a network issue... I've added a static route for the sflow device and now I see the messages coming through logstash and also in Kibana!\"},\n",
       " {'text_field': 'I\\'ve run into a weird issue with nested sorts. When specifying a sort as such:\\n\"sort\": [\\n    {\\n      \"variations.price.regular\": {\\n        \"mode\": \"min\",\\n        \"order\": \"asc\",\\n        \"nested\": {\\n          \"path\": \"variations.price\"\\n        }\\n      }\\n    }\\n  ]\\n\\nIt works as expected and sort values like \"sort\" : [ 500 ] are returned. However when specifying a nested sort with a filter as such:\\n\"sort\": [\\n    {\\n      \"variations.price.regular\": {\\n        \"mode\": \"min\",\\n        \"order\": \"asc\",\\n        \"nested\": {\\n          \"path\": \"variations.price\",\\n          \"filter\": {\\n            \"term\": {\\n              \"variations.inventory.shop_id\": 1\\n            }\\n          }\\n        }\\n      }\\n    }\\n  ]\\n\\nIt does not work as expected and sort values like \"sort\": [ 9223372036854775807 ] are returned.\\nBelow is a full reproduction of this issue as a Kibana script:\\nDELETE /debug\\n\\nPUT /debug\\n\\nPUT /debug/_mapping/_doc\\n{\\n  \"properties\": {\\n    \"name\": {\\n      \"type\": \"text\"\\n    },\\n    \"variations\": {\\n      \"type\": \"nested\",\\n      \"properties\": {\\n        \"name\": {\\n          \"type\": \"text\"\\n        },\\n        \"inventory\": {\\n          \"type\": \"nested\",\\n          \"properties\": {\\n            \"quantity\": {\\n              \"type\": \"integer\"\\n            },\\n            \"shop_id\": {\\n              \"type\": \"keyword\"\\n            }\\n          }\\n        },\\n        \"price\": {\\n          \"type\": \"nested\",\\n          \"properties\": {\\n            \"regular\": {\\n              \"type\": \"integer\"\\n            },\\n            \"shop_id\": {\\n              \"type\": \"keyword\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nPUT /debug/_doc/1\\n{\\n  \"name\": \"Product One\",\\n  \"variations\": [\\n    {\\n      \"name\": \"Product One - Variation One\",\\n      \"inventory\": [\\n        {\\n          \"quantity\": 100,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"quantity\": 0,\\n          \"shop_id\": 2\\n        }\\n      ],\\n      \"price\": [\\n        {\\n          \"regular\": 1000,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"regular\": 1000,\\n          \"shop_id\": 2\\n        }\\n      ]\\n    },\\n    {\\n      \"name\": \"Product One - Variation Two\",\\n      \"inventory\": [\\n        {\\n          \"quantity\": 0,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"quantity\": 100,\\n          \"shop_id\": 2\\n        }\\n      ],\\n      \"price\": [\\n        {\\n          \"regular\": 2000,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"regular\": 2000,\\n          \"shop_id\": 2\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\\nPUT /debug/_doc/2\\n{\\n  \"name\": \"Product Two\",\\n  \"variations\": [\\n    {\\n      \"name\": \"Product Two - Variation One\",\\n      \"inventory\": [\\n        {\\n          \"quantity\": 0,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"quantity\": 100,\\n          \"shop_id\": 2\\n        }\\n      ],\\n      \"price\": [\\n        {\\n          \"regular\": 3000,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"regular\": 3000,\\n          \"shop_id\": 2\\n        }\\n      ]\\n    },\\n    {\\n      \"name\": \"Product Two - Variation Two\",\\n      \"inventory\": [\\n        {\\n          \"quantity\": 0,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"quantity\": 100,\\n          \"shop_id\": 2\\n        }\\n      ],\\n      \"price\": [\\n        {\\n          \"regular\": 4000,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"regular\": 4000,\\n          \"shop_id\": 2\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\\nPUT /debug/_doc/3\\n{\\n  \"name\": \"Product Three\",\\n  \"variations\": [\\n    {\\n      \"name\": \"Product Three - Variation One\",\\n      \"inventory\": [\\n        {\\n          \"quantity\": 100,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"quantity\": 100,\\n          \"shop_id\": 2\\n        }\\n      ],\\n      \"price\": [\\n        {\\n          \"regular\": 500,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"regular\": 500,\\n          \"shop_id\": 2\\n        }\\n      ]\\n    },\\n    {\\n      \"name\": \"Product Three - Variation Two\",\\n      \"inventory\": [\\n        {\\n          \"quantity\": 0,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"quantity\": 0,\\n          \"shop_id\": 2\\n        }\\n      ],\\n      \"price\": [\\n        {\\n          \"regular\": 6000,\\n          \"shop_id\": 1\\n        },\\n        {\\n          \"regular\": 6000,\\n          \"shop_id\": 2\\n        }\\n      ]\\n    }\\n  ]\\n}\\n\\nPOST /debug/_search\\n{\\n  \"query\": {\\n    \"nested\": {\\n      \"path\": \"variations\",\\n      \"query\": {\\n        \"bool\": {\\n          \"filter\": [\\n            {\\n              \"nested\": {\\n                \"path\": \"variations.inventory\",\\n                \"query\": {\\n                  \"bool\": {\\n                    \"filter\": [\\n                      {\\n                        \"term\": {\\n                          \"variations.inventory.shop_id\": 1\\n                        }\\n                      },\\n                      {\\n                        \"range\": {\\n                          \"variations.inventory.quantity\": {\\n                            \"gte\": 1\\n                          }\\n                        }\\n                      }\\n                    ]\\n                  }\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  },\\n  \"sort\": [\\n    {\\n      \"variations.price.regular\": {\\n        \"mode\": \"min\",\\n        \"order\": \"asc\",\\n        \"nested\": {\\n          \"path\": \"variations.price\",\\n          \"filter\": {\\n            \"bool\": {\\n              \"filter\": [\\n                {\\n                  \"term\": {\\n                    \"variations.inventory.shop_id\": 1\\n                  }\\n                },\\n                {\\n                  \"range\": {\\n                    \"variations.inventory.quantity\": {\\n                      \"gte\": 1\\n                    }\\n                  }\\n                }\\n              ]\\n            }\\n          }\\n        }\\n      }\\n    }\\n  ]\\n}\\n\\nPOST /debug/_search\\n{\\n  \"query\": {\\n    \"nested\": {\\n      \"path\": \"variations\",\\n      \"query\": {\\n        \"bool\": {\\n          \"filter\": [\\n            {\\n              \"nested\": {\\n                \"path\": \"variations.inventory\",\\n                \"query\": {\\n                  \"bool\": {\\n                    \"filter\": [\\n                      {\\n                        \"term\": {\\n                          \"variations.inventory.shop_id\": 1\\n                        }\\n                      },\\n                      {\\n                        \"range\": {\\n                          \"variations.inventory.quantity\": {\\n                            \"gte\": 1\\n                          }\\n                        }\\n                      }\\n                    ]\\n                  }\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  },\\n  \"sort\": [\\n    {\\n      \"variations.price.regular\": {\\n        \"mode\": \"min\",\\n        \"order\": \"asc\",\\n        \"nested\": {\\n          \"path\": \"variations.price\"\\n        }\\n      }\\n    }\\n  ]\\n}\\n\\nAm I doing something wrong when adding the filter within the nested sort? The issue seems similar to Sort is out of order - using Nested Filter but there were no responses on that thread.\\nAll tests run with Elasticsearch and Kibana 6.8.3 docker containers.'},\n",
       " {'text_field': 'For any future forum wanderers that end up here, we ended up solving this by combining the two nested types into a single nested type. Below is a transformation of the above example similar to what we ended up doing:\\nDELETE /debug\\n\\nPUT /debug\\n\\nPUT /debug/_mapping/_doc\\n{\\n \"properties\": {\\n   \"name\": {\\n     \"type\": \"text\"\\n   },\\n   \"variations\": {\\n     \"type\": \"nested\",\\n     \"properties\": {\\n       \"name\": {\\n         \"type\": \"text\"\\n       },\\n       \"priceventory\": {\\n         \"type\": \"nested\",\\n         \"properties\": {\\n           \"inventory_quantity\": {\\n             \"type\": \"integer\"\\n           },\\n           \"regular_price\": {\\n             \"type\": \"integer\"\\n           },\\n           \"shop_id\": {\\n             \"type\": \"keyword\"\\n           }\\n         }\\n       }\\n     }\\n   }\\n }\\n}\\n\\nPUT /debug/_doc/1\\n{\\n \"name\": \"Product One\",\\n \"variations\": [\\n   {\\n     \"name\": \"Product One - Variation One\",\\n     \"priceventory\": [\\n       {\\n         \"inventory_quantity\": 100,\\n         \"regular_price\": 1000,\\n         \"shop_id\": 1\\n       },\\n       {\\n         \"inventory_quantity\": 0,\\n         \"regular_price\": 1000,\\n         \"shop_id\": 2\\n       }\\n     ]\\n   },\\n   {\\n     \"name\": \"Product One - Variation Two\",\\n     \"priceventory\": [\\n       {\\n         \"inventory_quantity\": 0,\\n         \"regular_price\": 2000,\\n         \"shop_id\": 1\\n       },\\n       {\\n         \"inventory_quantity\": 100,\\n         \"regular_price\": 2000,\\n         \"shop_id\": 2\\n       }\\n     ]\\n   }\\n ]\\n}\\n\\nPUT /debug/_doc/2\\n{\\n \"name\": \"Product Two\",\\n \"variations\": [\\n   {\\n     \"name\": \"Product Two - Variation One\",\\n     \"priceventory\": [\\n       {\\n         \"inventory_quantity\": 0,\\n         \"regular_price\": 3000,\\n         \"shop_id\": 1\\n       },\\n       {\\n         \"inventory_quantity\": 100,\\n         \"regular_price\": 3000,\\n         \"shop_id\": 2\\n       }\\n     ]\\n   },\\n   {\\n     \"name\": \"Product Two - Variation Two\",\\n     \"priceventory\": [\\n       {\\n         \"inventory_quantity\": 0,\\n         \"regular_price\": 4000,\\n         \"shop_id\": 1\\n       },\\n       {\\n         \"inventory_quantity\": 100,\\n         \"regular_price\": 4000,\\n         \"shop_id\": 2\\n       }\\n     ]\\n   }\\n ]\\n}\\n\\nPUT /debug/_doc/3\\n{\\n \"name\": \"Product Three\",\\n \"variations\": [\\n   {\\n     \"name\": \"Product Three - Variation One\",\\n     \"priceventory\": [\\n       {\\n         \"inventory_quantity\": 100,\\n         \"regular_price\": 500,\\n         \"shop_id\": 1\\n       },\\n       {\\n         \"inventory_quantity\": 100,\\n         \"regular_price\": 500,\\n         \"shop_id\": 2\\n       }\\n     ]\\n   },\\n   {\\n     \"name\": \"Product Three - Variation Two\",\\n     \"priceventory\": [\\n       {\\n         \"inventory_quantity\": 0,\\n         \"regular_price\": 6000,\\n         \"shop_id\": 1\\n       },\\n       {\\n         \"inventory_quantity\": 0,\\n         \"regular_price\": 6000,\\n         \"shop_id\": 2\\n       }\\n     ]\\n   }\\n ]\\n}\\n\\nPOST /debug/_search\\n{\\n \"query\": {\\n   \"nested\": {\\n     \"path\": \"variations\",\\n     \"query\": {\\n       \"bool\": {\\n         \"filter\": [\\n           {\\n             \"nested\": {\\n               \"path\": \"variations.priceventory\",\\n               \"query\": {\\n                 \"bool\": {\\n                   \"filter\": [\\n                     {\\n                       \"term\": {\\n                         \"variations.priceventory.shop_id\": 1\\n                       }\\n                     },\\n                     {\\n                       \"range\": {\\n                         \"variations.priceventory.inventory_quantity\": {\\n                           \"gte\": 1\\n                         }\\n                       }\\n                     }\\n                   ]\\n                 }\\n               }\\n             }\\n           }\\n         ]\\n       }\\n     }\\n   }\\n },\\n \"sort\": [\\n   {\\n     \"variations.priceventory.regular_price\": {\\n       \"mode\": \"min\",\\n       \"order\": \"asc\",\\n       \"nested\": {\\n         \"path\": \"variations.priceventory\",\\n         \"filter\": {\\n           \"bool\": {\\n             \"filter\": [\\n               {\\n                 \"term\": {\\n                   \"variations.priceventory.shop_id\": 1\\n                 }\\n               },\\n               {\\n                 \"range\": {\\n                   \"variations.priceventory.inventory_quantity\": {\\n                     \"gte\": 1\\n                   }\\n                 }\\n               }\\n             ]\\n           }\\n         }\\n       }\\n     }\\n   }\\n ]\\n}\\n'},\n",
       " {'text_field': 'Hi guys.\\nI am filtering customer id using the below:\\n(%{NUMBER:customer-id:int}\\nOn my Kibana dashboard, I see it with , and I would like to take the commas out.\\nExample:\\n**#** customer-id 389,542,059\\nExpected:\\n**#** customer-id 389542059\\nHow could I achieve that? Thanks!'},\n",
       " {'text_field': \"\\n\\n\\n sud0:\\n\\nHow could I achieve that?\\n\\n\\nThat's a Kibana question, not a logstash question. You need to adjust the numeric field formatter.\"},\n",
       " {'text_field': 'Hi we have a daily report we publish in which we would like to provide a dynamic link (URL) to kibana with a dynamic search term string query  say \"Error Fetching, Shipping Details\" and in a particular dynamic time frame say \"from:10/18/2019 to 10/17/2019\" on a particular search index \"app-metric-*\"\\nfollowed this answer Generating a url that contains a query\\nthe answer is not an exact match to our requirement as the answer has key: params: style where as mine is a simple string query and time in the answer is now -24h where as we would like it to be particular dates with time say from 10/18/2019:6:00 to 10/17/209:6:0:0\\nand tried to create a url like this,\\nhttp://localhost:5601/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-24h,mode:quick,to:now))&amp;_a=(columns:!(_source),filters:!((\\'$state\\':(store:appState),meta:(alias:!n,disabled:!f,index:app-metric-events-,negate:!f,params:(query:\"Error Fetching Shipping ,Details\",type:phrase),type:phrase,value:\"Error Fetching, Shipping Details\"),query:(match:(query:\"Error Fetching, Shipping Details\",type:phrase)))),index:app-metric-events-,interval:auto,query:(language:lucene,query:\\'\\'),sort:!(timestamp,desc))\\nwhen i paste in the browser it is redirecting to kibana site (No search term is populated, index is not populated, timeframe is not populated) but on the site we are getting \"url parse exception\" model  but it\\'s not specific to which part failed, we are fairly new to kibana appreciate the community help in resolving the issue please.\\nThanks'},\n",
       " {'text_field': \"Hi &amp; Welcome to the Kibana community! \\nSo here's a similar query I did in Discover\\nhttp://localhost:5601/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:'2019-10-16T06:00',to:'2019-10-18T06:00'))&amp;_a=(columns:!(_source),index:'logstash-*',interval:auto,query:(language:lucene,query:'Error%20Fetching,%20Shipping%20Details'),sort:!(!('@timestamp',desc)))\\nyou could adapt it for your use case, replace from and to ( (formatted like in the example)), index, and in query replace/encode spaces with %20\\nThen it should work\"},\n",
       " {'text_field': 'can anyone can suggest how to move the processed files from one bucket to other.\\nscenario:\\nBucket 1\\n2019/10/\\nwill S3 plugin move the processed files in the same structure ?'},\n",
       " {'text_field': 'I think it keeps the structure, but you would have to test it to be sure.'},\n",
       " {'text_field': 'Hi there,\\nBeen setting up HTTPS security on a cluster (version 6.8.0) and I have this in my eleasticsearch.yml\\nxpack.license.self_generated.type: basic\\nBut I see this when I check the logs\\n\\n[2019-10-18T12:36:14,091][WARN ][o.e.l.LicenseService     ] [ESTEST] License [will expire] on [Friday, November 08, 2019].\\nIf you have a new license, please update it. Otherwise, please reach out to\\nyour support contact.\\n\\nAny one have a clue as to why? I have only added features under the basic license\\nMy elasticsearch.yml\\n\\ncluster.name: \"test-cluster\"\\nnode.name: \"esdata-0\"\\npath.logs: /var/log/elasticsearch\\npath.data: /var/lib/elasticsearch\\ndiscovery.zen.ping.unicast.hosts: [\"esdata-0:9300\",\"esdata-1:9300\",\"esdata-2:9300\",\"esdata-3:9300\"]\\ndiscovery.zen.minimum_master_nodes: 3\\nnode.master: true\\nnode.data: true\\nnetwork.host: [site, local]\\nnode.max_local_storage_nodes: 1\\nnode.attr.fault_domain: 0\\nnode.attr.update_domain: 0\\ncluster.routing.allocation.awareness.attributes: fault_domain,update_domain\\nxpack.license.self_generated.type: basic\\nxpack.monitoring.collection.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.key: /etc/elasticsearch/Private.key\\nxpack.security.transport.ssl.certificate: /etc/elasticsearch/certificate\\nxpack.security.transport.ssl.certificate_authorities: [ \"/etc/elasticsearch/ServerCA.crt\" ]\\nxpack.security.http.ssl.enabled: true\\nxpack.security.http.ssl.key:  /etc/elasticsearch/Private.key\\nxpack.security.http.ssl.certificate: /etc/elasticsearch/certificate\\nxpack.security.http.ssl.certificate_authorities: [ \"/etc/elasticsearch/ServerCA.crt\" ]\\nbootstrap.memory_lock: true\\n\\nHelp is appriciated\\nBest regards\\nThomas'},\n",
       " {'text_field': 'Is it possible that trial was activated after the inital setup? Could you check the logs for Oct 9 to see if you there is a line that looks like\\n[2019-10-18T20:05:43,519][INFO ][o.e.l.LicenseService     ] [XXXX] license [XXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX] mode [trial] - valid\\n\\nIn any case if you are not using any non-basic features you can go back to basic at any time.'},\n",
       " {'text_field': \"I have setup a cluster on the docker, and after completed taking the snapshot successfully, I am trying to simulate the scenario to restore the snapshot.\\nWhat are the steps to do so? I can't find the instructions from the manul, please help to point me to the right section.\\nThis is what I tried but failed:\\nTake the snapshot on the host's directory which was mounted to all containers. and I could see the directory and files on the host's directory.\\nRestart the cluster and create the repository with the same name as before. The reason to create the repository is that it is possible to restore it to a new cluster where repository does not exist.\\nIt could not find the snapshot.\\nWhy is that?\\nThanks,\"},\n",
       " {'text_field': 'I found the root cause. It was due to the incorrect information provided in the repository of the new cluster. It has to be exactly the same as the old one and it will recognize it automatically.'},\n",
       " {'text_field': 'Hello all,\\nI have problem with terms aggregation; the problem is when am trying to make an aggregation out of multiple lists of emails, elasticsearch is splitting the response in buckets.key by the \"@\" sign in the email address. the request is:\\nGET test/_search\\n{\\n\\t\"_source\": \"emails\",\\n    \"query\": {\\n        \"term\": {\\n            \"companyId\": {\\n                \"value\": 31953\\n            }\\n        }\\n    },\\n\\t\"aggs\": {\\n    \"distinct_emails\": {\\n      \"terms\": {\\n        \"field\": \"emails\"\\n      }\\n    }\\n  }\\n}\\n\\nthe full response is:\\n  {\\n    \"took\": 6,\\n    \"timed_out\": false,\\n    \"_shards\": {\\n        \"total\": 1,\\n        \"successful\": 1,\\n        \"skipped\": 0,\\n        \"failed\": 0\\n    },\\n    \"hits\": {\\n        \"total\": {\\n            \"value\": 18,\\n            \"relation\": \"eq\"\\n        },\\n        \"max_score\": 1,\\n        \"hits\": [\\n            {\\n                \"_index\": \"test\",\\n                \"_type\": \"items\",\\n                \"_id\": \"8290c5f279dabb08a21ed11f3515a94e2408650d\",\\n                \"_score\": 1,\\n                \"_source\": {\\n                    \"emails\": [\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\"\\n                    ]\\n                }\\n            },\\n            {\\n                \"_index\": \"test\",\\n                \"_type\": \"items\",\\n                \"_id\": \"64e5974d5db9e1379fd52393521e256b71b364f9\",\\n                \"_score\": 1,\\n                \"_source\": {\\n                    \"emails\": [\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyotherdomain.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\"\\n                    ]\\n                }\\n            },\\n            {\\n                \"_index\": \"test\",\\n                \"_type\": \"items\",\\n                \"_id\": \"67754094f58ceca963ab0933c7ecb7f7cded6077\",\\n                \"_score\": 1,\\n                \"_source\": {\\n                    \"emails\": [\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\"\\n                    ]\\n                }\\n            },\\n            {\\n                \"_index\": \"test\",\\n                \"_type\": \"items\",\\n                \"_id\": \"f71617eb607b3e77f03794e41ca239322d53b709\",\\n                \"_score\": 1,\\n                \"_source\": {\\n                    \"emails\": [\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\",\\n                        \"info@companyname.com\"\\n                    ]\\n                }\\n            }\\n        ]\\n    },\\n    \"aggregations\": {\\n        \"distinct_emails\": {\\n            \"doc_count_error_upper_bound\": 0,\\n            \"sum_other_doc_count\": 0,\\n            \"buckets\": [\\n                {\\n                    \"key\": \"companyname.com\",\\n                    \"doc_count\": 13\\n                },\\n                {\\n                    \"key\": \"info\",\\n                    \"doc_count\": 13\\n                },\\n                {\\n                    \"key\": \"companyotherdomain.com\",\\n                    \"doc_count\": 1\\n                }\\n            ]\\n        }\\n    }\\n}\\n\\nthe buckets part in the response is:\\n\"buckets\": [\\n                {\\n                    \"key\": \"companyname.com\",\\n                    \"doc_count\": 13\\n                },\\n                {\\n                    \"key\": \"info\",\\n                    \"doc_count\": 13\\n                },\\n                {\\n                    \"key\": \"companyotherdomain.com\",\\n                    \"doc_count\": 1\\n                }\\n            ]\\n\\nI\\'m expecting the below:\\n\"buckets\": [\\n                {\\n                    \"key\": \"info@companyname.com\",\\n                    \"doc_count\": 13\\n                },\\n                {\\n                    \"key\": \"info@companyotherdomain.com\",\\n                    \"doc_count\": 1\\n                }\\n            ]'},\n",
       " {'text_field': 'You can make the aggregations on \"emails.keyword\".\\nSomething like:\\n\"aggs\": { \"distinct_emails\": { \"terms\": { \"field\": \"emails.keyword\" } }'},\n",
       " {'text_field': 'I have created a plugin created for Kibana 7.3.3 and want to access the elasticsearch query that I need in order to get the same set of data that is shown in my dashboard, including the time range.\\nI found in the documentation the VIS object, mentioning the queryFilter located at vis.API.queryFilter but it just returns \"{}\". Is there a different object I need to access, or do I have to fully reconstruct the ES query from the URL?'},\n",
       " {'text_field': 'I finally managed to find it. It took a lot of digging, so to save some time for anyone needing it in the future, I post here an answer.\\nI used a React.Component to create my visualization. From the propsyou can access the filters used in your dashboard:\\n\\ntime range: props.vis.API.timeFilter.getTime()  - Gives an object which has from and to\\n\\nfilters: props.vis.API.queryFilter.getFilters()  - Gives an array of all currently applied filters\\n'},\n",
       " {'text_field': 'Is there a way to use something like digest access authentication (or base authentication, I know, it is an horrible way, but I said \"like\") to login to kibana bypassing the login form?\\nI\\'ve a web application with authentication. My db is synchronized with kibana users (user and password are the same). I\\'ve a direct link to kibana but when I click on it it\\'s necessary to login another time'},\n",
       " {'text_field': \"Hi and welcome to the Kibana community! \\nWouldn't token authentication a good solution for your use case?\\nhttps://www.elastic.co/guide/en/kibana/current/kibana-authentication.html#token-authentication\\nBest,\\nMatthias\"},\n",
       " {'text_field': \"I can't seem to find any answers on this. looking at the docker images for:\\ndocker.elastic.co/beats/metricbeat:7.4.0\\nand\\ndocker.elastic.co/beats/metricbeat-oss:7.4.0\\nThe metricbeat-oss does not include the aws module. was this by design? Is there a way to add it? I'm using the oss version since I'm sending metrics to aws elasticsearch.\\ncheers!\\nJames\"},\n",
       " {'text_field': 'According to the documentation the Metricbeat AWS module is part of x-pack and therefore requires the default distribution.'},\n",
       " {'text_field': 'Hi Elasticsearch Masters,\\nI’m running ES v 7.4.0 as a Docker container.  My host box is a Ubuntu box, 16.04.4; my version of Docker is 17.09.0-ce, build afdb6d4.\\nI want to use the /usr/share/elasticsearch/bin/elasticsearch-env script to run SQL-like queries against ES.  I know there are some minor EnvVar issues with the script (documented here) but I was able to manually correct them and get the script running.\\nBut I didn’t get far:\\n[me@1234567890ab bin]# ./elasticsearch-sql-cli https://elastic@127.0.0.1:9200\\nWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\\npassword:\\nERROR: Cannot communicate with the server https://elastic@127.0.0.1:9200. This version of CLI only works with Elasticsearch version v7.4.0 [22e1767283e61a198cb4db791ea66e3f11ab9910]\\n[me@1234567890ab bin]#\\n\\nSo in the above, the script complains that it can only talking to an ES instance of ver 7.4.0.  But I am within my docker container running ES v7.4.0; there shouldn’t be a version issue.  Is this easily correctable?\\nAnother issue: that password challenge.  I spun up my Docker container, then did a minimum amount of configuration to get my ES service up and running.  I don’t recall setting a password anywhere.\\nWhat would the password be by default?  Where can I set it?  The online documentation (here) makes a reference to “If security is enabled on your cluster…”  Does that refer to this?  Would this be turned on by default?\\nThanks!'},\n",
       " {'text_field': '@redapplesonly if you are not sure about security configuration, there are high chances the command you used to start the CLI is incorrect due to the link to the ES node. Have you tried, by any chance, simply calling elasticsearch-sql-cli without a specific port, number and protocol?\\nRegarding security, yes, that would be the docs link. A first good step would be a look at https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html#general-security-settings.'},\n",
       " {'text_field': 'Hi,\\nSay we have 3 frozen indices and 2 closed indices in a cluster. When the cluster restarts, will the frozen indices be unfrozen and the closed indices reopened?\\nThanks'},\n",
       " {'text_field': 'No, closedness and frozenness persist across restarts.'},\n",
       " {'text_field': 'Unfortunately, the docs for NEST is completely useless in providing answers to so many issues. Everything is available for Elasticsearch and can easily be found. But if you are looking for the same thing in NEST - Forget it!\\nIs there anyway to Upsert a document and exclude various fields from being updated should they exist.\\nMany thanks'},\n",
       " {'text_field': '\\n\\n\\n han1:\\n\\nP.S. The links you have sent me seem to also be just for non NEST syntax or am I missing something.\\n\\n\\nEach of the Kibana Console examples has a dropdown menu at the bottom of the code block that allows switching to another language:\\n\\nimage.png837×1066 55.1 KB\\n\\nswitching language will switch all code blocks to that language, and the preference will be stored locally so that the page will default to this language in future.\\n\\n\\n\\n han1:\\n\\nIt is not that it is completely useless. Its just that whenever we need to find something that is a little bit more than the basic stuff, we have to use the elastic search syntax and try to convert to NEST.\\n\\n\\nThe client tries to map as closely to the underlying Elasticsearch API JSON structure as possible. For example, using the fluent lambda expression syntax:\\nvar client = new ElasticClient();\\n\\nvar searchResponse = client.Search&lt;Document&gt;(s =&gt; s\\n\\t.Query(q =&gt; q\\n\\t\\t.Match(m =&gt; m\\n\\t\\t\\t.Field(f =&gt; f.Path)\\n\\t\\t\\t.Query(\"value\")\\n\\t\\t)\\n\\t)\\n\\t.Sort(so =&gt; so\\n\\t\\t.Field(\"_score\", SortOrder.Descending)\\n\\t)\\n);\\n\\nmaps to\\n{\\n  \"query\": {\\n    \"match\": {\\n      \"path\": {\\n        \"query\": \"value\"\\n      }\\n    }\\n  },\\n  \"sort\": [{\\n    \"_score\": {\\n      \"order\": \"desc\"\\n    }\\n  }]\\n}\\n\\nYou can always use the low level client API and send JSON too, returning a high level response when using the low level client exposed on the high level client:\\nvar lowLevelResponse = client.LowLevel.Search&lt;SearchResponse&lt;Document&gt;&gt;(\"documents\", @\"\\n{\\n  \"\"query\"\": {\\n\\t\"\"match\"\": {\\n\\t  \"\"path\"\": {\\n\\t    \"\"query\"\": \"\"value\"\"\\n\\t  }\\n\\t}\\n  },\\n  \"\"sort\"\": [{\\n    \"\"_score\"\": {\\n      \"\"order\"\": \"\"desc\"\"\\n    }\\n  }]\\n}\");\\n'},\n",
       " {'text_field': 'To achieve my goal - map geo location data from nginx logs to a map - I added a index template to ES with command like this:\\nPUT _template/nginx-default\\n{\\n\"template\": \"nginx-*\",\\n\"order\": 0,\\n\"index_patterns\" : [\\n  \"nginx*\"\\n],\\n\"settings\": {\\n\"index.mapping.ignore_malformed\": true\\n},\\n  \"mappings\" : {\\n      \"properties\" : {\\n        \"@timestamp\" : {\\n          \"type\" : \"date\"\\n        },\\n\\n...\\n\"geoip\" : {\\n\"location\" : {\\n\"type\": \"geo_point\"\\n},\\n},\\n...\\n}\\n}\\nAfter creating this template, I issued a complete reload of all data (removed indices from ES and deleted registry file in filebeat - I dont have that much data).\\nThe location field contains two properties, lat and lon, submitted as numbers:\\ngeoip.location.lat: number\\ngeoip.location.lon: number\\n\\nExample:\\n|#geoip.location.lat|37.751|\\n|---|---|\\n|#geoip.location.lon|-97.822|\\n\\nBut when I try to create a map, I still get the feedback, that there is no geo_ip field:\\n\\nThe index pattern nginx* does not contain any of the following compatible field types: geo_point\\n\\nWhat am I missing here?\\nThanks!'},\n",
       " {'text_field': 'Was the index created after you added the index template? The template only applies when the index is first created as you can not change mappings in existing indices.'},\n",
       " {'text_field': 'Hello,\\nI have a nested json field in my logs called \"instanceId\" :\\n\"responseElements\": { \"instancesSet\": { \"items\": [ { \"instanceId\": \"i-02669ced00e8a3701\", \"currentState\":{ \"code\": 32, \"name\": \"shutting-down\" }, \"previousState\": { \"code\": 16, \"name\": \"running\" } }, { \"instanceId\": \"i-0050374c0e0a6306a\", \"currentState\":{ \"code\": 32, \"name\": \"shutting-down\" }, \"previousState\": { \"code\": 16,\"name\": \"running\" } } ] } }\\nAs you can see there are two values for instanceId in this log (some logs will have many more). What I want to do is to extract all of the values of instanceId and put them into a new array field called \"resp_instance_id\".\\nThis ruby code does not work as it doesn\\'t create an array field:\\nruby {\\n       code =&gt; \\'\\n         response_item_size = event.get(\"[responseElements][instancesSet][items]\").length\\n          response_item_size.times do |index|\\n           event.set(\"[resp_instance_id][#{index}]\", event.get(\"[responseElements][instancesSet] \\n[items][#{index}][instanceId]\"))\\n         end\\n        \\'\\n     }\\n\\nThis results in this strange looking field:\\n\\nScreen Shot 2019-10-18 at 4.38.49 PM.png1010×146 45.6 KB\\n\\nWhich in turn produces multiple fields in our platform like this (there are more than just two in this case):\\n\\nScreen Shot 2019-10-18 at 1.59.22 PM.png492×1194 54.5 KB\\n\\nWhat I want is an array field with the  instead, like this:\\n\"resp_instance_id\" =&gt; [\\n[0] i-02669ced00e8a3701\\n[1] i-0050374c0e0a6306a\\n]\\nI don\\'t wan\\'t to hardcode this by referencing the elements of instanceId manually, like this:\\n[responseElements][instancesSet][items][0][instanceId]\\n[responseElements][instancesSet][items][1][instanceId]\\nBecause instanceId will have an unknown number of values (not just two as in this example).\\nI have tried using a loop in ruby to create a ruby array witch contains all of the elements of \"instanceId\" and was successful in creating that array. However, I was unable to do an add_field using that array of values. It seems that add_field can not be included in a loop in the ruby code, but rather must come after the closing \\' of the ruby code block. I do know if you use \"add_field\" to create the same field multiple times with different values that the resulting field will be an array field. But, without being able to use \"add_field\" inside of the ruby loop, this does not seem possible.'},\n",
       " {'text_field': 'You can do it using\\n        code =&gt; \\'\\n            response_items = event.get(\"[responseElements][instancesSet][items]\")\\n            a = []\\n            response_items.each_index do |x|\\n                a &lt;&lt; response_items[x][\"instanceId\"]\\n            end\\n            event.set(\"resp_instance_id\", a)\\n        \\'\\n    }'},\n",
       " {'text_field': \"Hi folks,\\nES APM on Ruby v3.0.0.  Wondering if it's possible to set the ELASTIC_APM_TRANSACTION_SAMPLE_RATE parameter to a lower value than 0.1 (say 0.05)?  Reason is, our services generate a LOT of transactions (looks like around 100GB of index data per day) and while I've setup Elastic Cloud to handle this no problem, we're being killed by AWS on data transfer cross-zone costs.\\nI'm already at 0.1 for the key heavy volume services so trying to see how I can keep some APM value but reduce the volume of data sent for transactions and spans.\\nCheers\\nDave\"},\n",
       " {'text_field': \"Hi again Dave! Putting Elastic APM to the test – I love it \\nSetting it to 0.05 will work just as expected. The check looks like this, so anything that converts into a float will work:\\nrand &lt;= config.transaction_sample_rate\\n\\nYou might be able to decrease the individual event size a bit too:\\nIf you don't care (or don't care enough) about seeing source code in kibana, there's perhaps a lot of bytes to be saved by skipping those with the options source_lines_error_app_frames and source_lines_span_app_frames. They are both 5 lines by default, meaning the line in question and the 2 immediately before and after. Set them to 0 to disable source code lines entirely.\\nYou could also disable capture_headers or capture_env but that's probably not many bytes after gzip.\\nAnd of course if there's entire libraries' spans that you don't care about, you can disable their auto-instrumentation with disabled_instrumentations.\\nThese are all slightly worsening the overall APM experience but if it's between slightly worse and nothing at all they could be worth it.\\nLet me know how it goes!\"},\n",
       " {'text_field': \"Is this a known issue?\\nI was running 7.3.0 and restored ran fine.\\nBut now upgraded to 7.3.1, I'm getting unassigned shards.\"},\n",
       " {'text_field': 'Hmm.  I think the problem is not all the nodes have s3 plugins.'},\n",
       " {'text_field': 'Hi,\\nUsing ES 7.0.0. I\\'ve been looking for docs/posts about this, but most info is very old or not clear, and after playing around with it I don\\'t understand how it should work.\\nI have different indexes that have a common base. There\\'s an index for \"cars\" and an index for \"computers\". Cars have a \"number_of_wheels\", and computers have \"number_of_cpus\", but both have a \"price\" etc.\\nSo I create the common template with order 1:\\nPUT _template/objects\\n{\\n  \"order\" : 1,\\n  \"index_patterns\" : [\\n    \"objects-*\"\\n  ],\\n  \"mappings\" : {\\n    \"dynamic\" : false,\\n    \"properties\" : {\\n      \"price\" : {\\n        \"type\" : \"long\"\\n      }\\n    }\\n  }\\n}\\n\\nAnd the other 2 templates with order 2, which results in 3 templates:\\nGET /_template/objects*\\n{\\n  \"objects-cars\" : {\\n    \"order\" : 2,\\n    \"index_patterns\" : [\\n      \"objects-cars-*\"\\n    ],\\n    \"settings\" : { },\\n    \"mappings\" : {\\n      \"dynamic\" : false,\\n      \"properties\" : {\\n        \"number_of_wheels\" : {\\n          \"type\" : \"long\"\\n        }\\n      }\\n    },\\n    \"aliases\" : { }\\n  },\\n  \"objects-computers\" : {\\n    \"order\" : 2,\\n    \"index_patterns\" : [\\n      \"objects-computers-*\"\\n    ],\\n    \"settings\" : { },\\n    \"mappings\" : {\\n      \"dynamic\" : false,\\n      \"properties\" : {\\n        \"number_of_cpus\" : {\\n          \"type\" : \"long\"\\n        }\\n      }\\n    },\\n    \"aliases\" : { }\\n  },\\n  \"objects\" : {\\n    \"order\" : 1,\\n    \"index_patterns\" : [\\n      \"objects-*\"\\n    ],\\n    \"settings\" : { },\\n    \"mappings\" : {\\n      \"dynamic\" : false,\\n      \"properties\" : {\\n        \"price\" : {\\n          \"type\" : \"long\"\\n        }\\n      }\\n    },\\n    \"aliases\" : { }\\n  }\\n}\\n\\nI create a document:\\nPOST /objects-computers-foo/_doc\\n{\\n  \"price\": 15,\\n  \"number_of_cpus\": 4\\n}\\n\\nWhich succeeds:\\n{\\n  \"_index\" : \"objects-computers-foo\",\\n  \"_type\" : \"_doc\",\\n  \"_id\" : \"Zc-p420B4UGO-P7XGC0J\",\\n  \"_version\" : 1,\\n  \"result\" : \"created\",\\n  \"_shards\" : {\\n    \"total\" : 2,\\n    \"successful\" : 1,\\n    \"failed\" : 0\\n  },\\n  \"_seq_no\" : 0,\\n  \"_primary_term\" : 1\\n}\\n\\nSeems OK:\\nGET /objects-computers-foo/_search\\n{\\n    \"query\": {\\n        \"match_all\": {}\\n    }\\n}\\n\\nsays\\n{\\n  \"took\" : 0,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 1,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 1.0,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"objects-computers-foo\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"Zc-p420B4UGO-P7XGC0J\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"price\" : 15,\\n          \"number_of_cpus\" : 4\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nBut I don\\'t expect that I can create a computer with wheels:\\nPOST /objects-computers-foo/_doc\\n{\\n  \"price\": 20,\\n  \"number_of_wheels\": 4\\n}\\n\\nHowever, that seem to result in this:\\n{\\n  \"_index\" : \"objects-computers-foo\",\\n  \"_type\" : \"_doc\",\\n  \"_id\" : \"h8-r420B4UGO-P7XLzfc\",\\n  \"_version\" : 1,\\n  \"result\" : \"created\",\\n  \"_shards\" : {\\n    \"total\" : 2,\\n    \"successful\" : 2,\\n    \"failed\" : 0\\n  },\\n  \"_seq_no\" : 1,\\n  \"_primary_term\" : 1\\n}\\n\\nThe documents are now:\\n{\\n  \"took\" : 0,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 2,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 1.0,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"objects-computers-foo\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"Zc-p420B4UGO-P7XGC0J\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"price\" : 15,\\n          \"number_of_cpus\" : 4\\n        }\\n      },\\n      {\\n        \"_index\" : \"objects-computers-foo\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"h8-r420B4UGO-P7XLzfc\",\\n        \"_score\" : 1.0,\\n        \"_source\" : {\\n          \"price\" : 20,\\n          \"number_of_wheels\" : 4\\n        }\\n      }\\n    ]\\n  }\\n}\\n\\nLooking at the index, this is what I see:\\nGET /objects-computers-foo\\n\\n{\\n  \"objects-computers-foo\" : {\\n    \"aliases\" : { },\\n    \"mappings\" : {\\n      \"dynamic\" : \"false\",\\n      \"properties\" : {\\n        \"number_of_cpus\" : {\\n          \"type\" : \"long\"\\n        },\\n        \"price\" : {\\n          \"type\" : \"long\"\\n        }\\n      }\\n    },\\n    \"settings\" : {\\n      \"index\" : {\\n        \"lifecycle\" : {\\n          \"name\" : \"delete_after_365_days\"\\n        },\\n        \"refresh_interval\" : \"5s\",\\n        \"number_of_shards\" : \"1\",\\n        \"provided_name\" : \"objects-computers-foo\",\\n        \"creation_date\" : \"1571482572647\",\\n        \"priority\" : \"100\",\\n        \"number_of_replicas\" : \"1\",\\n        \"uuid\" : \"Y-Dt5nosS-i6WohMQeGHnw\",\\n        \"version\" : {\\n          \"created\" : \"7000099\"\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nPlease ignore the ILM of 365 days, that because I have another order 0 template that applies default ILM\\'s of 365 days, but I think it\\'s unrelated.\\nThe index does seem to have a static mapping with only 2 properties: number_of_cpus and price. So why is the number_of_wheels allowed?\\nAm I not understanding the concept of template inheritance (or dynamic/statis mapping), is this a known issue, is this a bug, or none of the above?'},\n",
       " {'text_field': \"Looks like it's documented correctly after all, I just made wrong assumptions: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html\\nThe key is in the last note:\\n\\nSetting  dynamic  to  false  doesn’t alter the contents of the  _source  field at all. The  _source  will still contain the whole JSON document that you indexed. However, any unknown fields will not be added to the mapping and will not be searchable.\\n\\nI'm still not too sure about how Kibana handles this, since I did see those fields pop-up where I did not expect them to. Will look into that and report back (in case anyone stumbles on the same question).\"},\n",
       " {'text_field': 'Did an upgrade yesterday from 6.8 to 7.1.1 to 7.4\\nToday I find one node out of 27 data nodes failing like below and wonder who and why are sending so large message(s):\\n[2019-10-19T13:53:43,380][DEBUG][o.e.a.a.c.n.i.TransportNodesInfoAction] [d1r2n1] failed to execute on node [FDZezqUCRYqLNHfGV6eECw]\\norg.elasticsearch.transport.RemoteTransportException: [es-mst3][&lt;redacted&gt;:9300][cluster:monitor/nodes/info[n]]\\nCaused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [&lt;transport_request&gt;] would be [4061260842/3.7gb], which is larger than the limit of [4047097036/3.7gb], real usage: [4061245568/3.7gb], new bytes reserved: [15274/14.9kb], usages [request=0/0b, fielddata=0/0b, in_flight_requests=15274/14.9kb, accounting=0/0b]\\n        at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:342) ~[elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128) ~[elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:170) [elasticsearch-7.4.0.jar:7.4.0]\\n        at org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:118) [elasticsearch-7.4.0.jar:7.4.0]\\n\\nEdit: seems it might be backlogging logstash instances. Backlogging due to index template issues after index rollover last night, just fixed the templates and bouncing the logstash instances...'},\n",
       " {'text_field': 'fixing all template issues and bouncing all backlogged logstash instances and a full elastic cluster restart seems to have settled the issue for now '},\n",
       " {'text_field': 'Hello. Is it possible to add a  °C along with data in a Metric visualization? if so, how? Thanks!'},\n",
       " {'text_field': 'Hello. I was planning to put the label right next to the value. my solution is to use the visual builder.'},\n",
       " {'text_field': 'elasticsearch 7.2 plainless script bug\\n`POST _scripts/calculate-score\\n{\\n  \"script\": {\\n   \"lang\": \"painless\",\\n    \"source\": \"Math.log(_score * 2) + params.my_modifier\"\\n  }\\n}\\n\\nGET _scripts/calculate-score\\n\\n\\nGET _search\\n{\\n  \"query\": {\\n    \"script\": {\\n      \"script\": {\\n        \"id\": \"calculate-score\",\\n        \"params\": {\\n          \"my_modifier\": 2\\n        }\\n      }\\n    }\\n }\\n}`\\n\\nwhen execute  _search, the error report below:\\n    `  \"error\": {\\n\"root_cause\": [\\n  {\\n    \"type\": \"script_exception\",\\n    \"reason\": \"compile error\",\\n    \"script_stack\": [\\n      \"Math.log(_score * 2) + params.my_m ...\",\\n      \"         ^---- HERE\"\\n    ],\\n    \"script\": \"Math.log(_score * 2) + params.my_modifier\",\\n    \"lang\": \"painless\"\\n  },'},\n",
       " {'text_field': \"Thanks for the report. I've fixed the docs in https://github.com/elastic/elasticsearch/pull/47691\"},\n",
       " {'text_field': 'Hi.\\nMy logstash is sending logs to /var/log/syslog, which is causing the disk to fill up pretty quick.\\nHow can  I disable it?\\nMy logstash.yml:\\nlog.level: info\\npath.logs: /var/log/logstash\\n\\nsyslog:\\nroot@elk:/etc/logstash# tail -n 10 /var/log/syslog\\nOct  7 14:28:39 elk logstash[7097]:     \"environment\" =&gt; \"production\",\\nOct  7 14:28:39 elk logstash[7097]:           \"class\" =&gt; \"TransactionDocumentPersisterImpl.message\",\\nOct  7 14:28:39 elk logstash[7097]:         \"message\" =&gt; \"Transaction coo2 b2f51909 Message cooLocalNewOwnerDetails 1897334591 | timestamp: 2019-10-07T14:28:38.082+13:00\",\\nOct  7 14:28:39 elk logstash[7097]:            \"host\" =&gt; \"appserver.datacentre.example.co.nz\",\\nOct  7 14:28:39 elk logstash[7097]:        \"@version\" =&gt; \"1\",\\nOct  7 14:28:39 elk logstash[7097]:       \"log-level\" =&gt; \"INFO\",\\nOct  7 14:28:39 elk logstash[7097]:      \"@timestamp\" =&gt; 2019-10-07T01:28:38.083Z,\\nOct  7 14:28:39 elk logstash[7097]:            \"path\" =&gt; \"/mnt/example/app/industry/industry.log\",\\nOct  7 14:28:39 elk logstash[7097]:            \"type\" =&gt; \"web_industry_log\"\\nOct  7 14:28:39 elk logstash[7097]: }'},\n",
       " {'text_field': 'The solution proposed here solved my problem.'},\n",
       " {'text_field': 'Hi team,\\nI want to configure Elasticsearch for SAML authentication when I follow below official link: https://www.elastic.co/guide/en/elastic-stack-overview/6.7/saml-guide-authentication.html#saml-guide-authentication, but met \"not an SSL/TLS record\" error when configure the first step(Enable SSL/TLS for HTTP), then Elasticsearch cluster doesn\\'t work. I try my best to fix it. But I can\\'t solve it. I saw that this is an bug before version 7.4 (https://github.com/elastic/elasticsearch/pull/45852).\\nI want to ask you, so currently we can\\'t enable SAML before version 7.4 release, right?\\nbecause I enable it (xpack.security.http.ssl.enabled: true), the ES cluster doesn\\'t work.\\nAnd if you have other solution to fix this issue for older version, please help us too.\\nThanks a lot!\\n===============================================================================\\nElasticsearch Version: 6.7.0-1\\nMaster Node Config:\\ncluster.name: master-node\\nnode.name: master-node\\nnode.master: true\\nnode.data: false\\nnetwork.host: 0.0.0.0\\nhttp.port: 9200\\ndiscovery.zen.ping.unicast.hosts: [\"192.168.0.178\", \"192.168.0.179\", \"192.168.0.180\"]\\nxpack.security.enabled: true\\nxpack.monitoring.collection.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.transport.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.enabled: true\\nxpack.security.http.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.authc.token.enabled: true\\n\\nData Node config:\\ncluster.name: master-node\\nnode.name: data-node1\\nnode.master: false\\nnode.data: true\\nnetwork.host: 0.0.0.0\\nhttps.port: 9200\\ndiscovery.zen.ping.unicast.hosts: [\"192.168.0.178\", \"192.168.0.179\", \"192.168.0.180\"]\\nxpack.security.enabled: true\\nxpack.monitoring.collection.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.transport.ssl.verification_mode: certificate\\nxpack.security.transport.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.transport.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.enabled: true\\nxpack.security.http.ssl.keystore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.http.ssl.truststore.path: /etc/elasticsearch/certs/elastic-certificates.p12\\nxpack.security.authc.token.enabled: true\\n\\nAnd if I disable xpack.security.http.ssl.enabled, the ES cluster works fine.\\nERROE message:\\n[2019-10-07T14:18:27,811][WARN ][o.e.h.n.Netty4HttpServerTransport] [master-node] caught exception while handling client http traffic, closing connection [id: 0xd2747b4c, L:0.0.0.0/0.0.0.0:9200 ! R:/192.168.0.178:58880]\\nio.netty.handler.codec.DecoderException: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 48454144202f20485454502f312e310d0a417574686f72697a6174696f6e3a204261736963205a57786863335270597a70466247467a64476c6a4d54497a0d0a486f73743a203139322e3136382e302e3137383a393230300d0a436f6e74656e742d4c656e6774683a20300d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a0d0a\\nat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:472) ~[netty-codec-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) ~[netty-codec-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:656) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:556) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:510) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:470) [netty-transport-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:909) [netty-common-4.1.32.Final.jar:4.1.32.Final]\\nat java.lang.Thread.run(Thread.java:748) [?:1.8.0_222]\\nCaused by: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record: 48454144202f20485454502f312e310d0a417574686f72697a6174696f6e3a204261736963205a57786863335270597a70466247467a64476c6a4d54497a0d0a486f73743a203139322e3136382e302e3137383a393230300d0a436f6e74656e742d4c656e6774683a20300d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a0d0a\\nat io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1182) ~[netty-handler-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1247) ~[netty-handler-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502) ~[netty-codec-4.1.32.Final.jar:4.1.32.Final]\\nat io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441) ~[netty-codec-4.1.32.Final.jar:4.1.32.Final]\\n... 15 more'},\n",
       " {'text_field': 'If you enable SSL on the HTTP interface then you need to use https to access the cluster.\\n\\n\\n\\n rambo:\\n\\n[2019-10-07T14:18:27,811][WARN ][o.e.h.n.Netty4HttpServerTransport] [master-node] caught exception while handling client http traffic, closing connection [id: 0xd2747b4c, L:0.0.0.0/0.0.0.0:9200 ! R:/192.168.0.178:58880]\\nio.netty.handler.codec.DecoderException: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record:\\n\\n\\nThis means that something is connecting to your cluster using http:// instead of https://\\nThis will not work because your cluster is only accessible over SSL (https).'},\n",
       " {'text_field': 'Hi, I have a question about child-parent join in Elasticsearch, I have a real model like Category and Product.\\n\"categoryRelations\": {\\n\"type\": \"join\",\\n\"eager_global_ordinals\": true,\\n\"relations\": {\\n\"category\": [\\n\"productContent\",\\n\"category\"\\n]\\n}\\n}\\nand I design mapping in my index like that. My purpose here is category have child category, and in each category has its own product. Can i design like that or any suggestion for doing that? when I tried to put some data with relation category and parent it show exception:  \"DocValuesField \"categoryRelations#category\" appears more than once in this document (only one value is allowed per field)\".\\nThanks so much'},\n",
       " {'text_field': 'Hey,\\nindeed, you cannot self join. If each product would contain the category it is in as an array (aka a multivalue), would that work as well? You might want to take a look at the path hierarchy tokenizer\\n--Alex'},\n",
       " {'text_field': 'Hi all,\\nI\\'m testing an Azure Cloud deployement (Western Europe) and try to use the Kibana centralized Pipelines feature.\\nI have a local Logstash instance that I want to register on my Cloud deployment.\\nLocal Logstash instance version is 6.8.3 running on centos.\\nThe local logstash.yml is set as follows\\npath.data: /var/lib/logstash\\ncloud.id: deploy_01:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\ncloud.auth: elastic:xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\npath.logs: /var/log/logstash\\nxpack.management.enabled: true\\nxpack.management.pipeline.id: [\"pipeline_01\"]\\nxpack.management.elasticsearch.username: elastic\\nxpack.management.elasticsearch.password: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\nxpack.management.elasticsearch.hosts: [\"https://xxxxxxxxxxxxxxxxxxxxxxx.westeurope.azure.elastic-cloud.com:9243/\"]\\nxpack.management.logstash.poll_interval: 5s\\n\\nI have created a pipeline on the Kibana Pipelines UI with ID = \"pipeline_01\" and a basic configuration\\ninput {\\n  beats {\\n    port =&gt; 5044\\n  }\\n}\\nfilter {\\n}\\noutput {\\n  stdout { codec =&gt; rubydebug }\\n}\\n\\nWhen I run Logstash I keep on having the following message\\n[2019-10-07T09:27:51,527][ERROR][logstash.config.sourceloader] No configuration found in the configured sources.\\n\\nFull log is\\n[2019-10-07T09:27:33,753][INFO ][logstash.configmanagement.bootstrapcheck] Using Elasticsearch as config store {:pipeline_id=&gt;[\"pipeline_01\"], :poll_interval=&gt;\"5000000000ns\"}\\n[2019-10-07T09:27:35,229][INFO ][logstash.configmanagement.elasticsearchsource] Configuration Management License OK\\n[2019-10-07T09:27:35,474][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"6.8.3\"}\\n[2019-10-07T09:27:36,034][INFO ][logstash.configmanagement.elasticsearchsource] Elasticsearch pool URLs updated {:changes=&gt;{:removed=&gt;[], :added=&gt;[https://elastic:xxxxxx@xxxxxxxxxxxxxxxxxxxxxxxxxxxx.westeurope.azure.elastic-cloud.com:9243/]}}\\n[2019-10-07T09:27:36,133][WARN ][logstash.configmanagement.elasticsearchsource] Restored connection to ES instance {:url=&gt;\"https://elastic:xxxxxx@xxxxxxxxxxxxxxxxxxxxxxxxxxxxx.westeurope.azure.elastic-cloud.com:9243/\"}\\n[2019-10-07T09:27:36,240][INFO ][logstash.configmanagement.elasticsearchsource] ES Output version determined {:es_version=&gt;7}\\n[2019-10-07T09:27:36,240][WARN ][logstash.configmanagement.elasticsearchsource] Detected a 6.x and above cluster: the `type` event field won\\'t be used to determine the document _type {:es_version=&gt;7}\\n[2019-10-07T09:27:36,441][ERROR][logstash.config.sourceloader] No configuration found in the configured sources.\\n[2019-10-07T09:27:36,632][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n[2019-10-07T09:27:41,535][ERROR][logstash.config.sourceloader] No configuration found in the configured sources.\\n[2019-10-07T09:27:46,529][ERROR][logstash.config.sourceloader] No configuration found in the configured sources.\\n[2019-10-07T09:27:51,527][ERROR][logstash.config.sourceloader] No configuration found in the configured sources.\\n\\nI\\'m not sure where the issue is... in the local logstash.yml ? or I\\'m just not properly using the centralized Pipeline all together ?\\nAny help would be appreciated !\\nMany thanks'},\n",
       " {'text_field': 'Issue is solved after installing logstash 7.4.0 !'},\n",
       " {'text_field': 'Hi elatic team,\\nI\\'m trying to use APM with such releases:\\nKibana version: 7.2.0\\nElasticsearch version: 7.2.0\\nAPM Server version: 7.2.0\\nAPM Agent language and version: .Net APM Agent\\nAnd have some problems, I couldn\\'t push logs to APM server and firstly want to verify APM server status.\\nI configured APM as was mentioned in documentation and instructions in Kibana page, so I install APM server with RPM instructions and edit next fields only -&gt;\\noutput.elasticsearch:\\nhosts: [\"localhost:9200\"]\\nusername: myusername\\npassword: mypassword\\nI used localhost, cause APM server is located inside the same pod as Elasticsearch. Pod exist in Azure Kubernetes Service cluster.\\nThen I check APM-server via button -&gt; \\nimage.png1474×311 19.1 KB\\n\\nAnd got successful message.\\nAccording to official documentation (https://www.elastic.co/guide/en/apm/server/7.4/running-with-systemd.html) we can verify status via  \\'systemctl status apm-server\\' command. After I ran it, got this message -&gt;\\n[root@azure-elasticsearch-prod-0 elasticsearch]# systemctl status apm-server\\nFailed to get D-Bus connection: Operation not permitted\\nIt looks like my APM server is not configured properly.\\nAny thoughts?\\nThanks'},\n",
       " {'text_field': \"Hi @gil, after several times I configured APM server and now can push data to elasticsearch.\\nI figured out that environment variables in deployment.yaml file didn't work for me (container logs in Azure were rather informative) and apm-server.yml file on host is existed in readonly mode. I couldn't edit it either.\\nSo I looked at Narendiran D post (https://medium.com/logistimo-engineering-blog/how-did-i-use-apm-in-kubernetes-ecosystem-8f22d52beb03) and configured ConfigMap object (all environment variables were migrated to ConfigMap).\\nAfter these manipulations everything works fine.\"},\n",
       " {'text_field': 'We recently deploy a new cluster v7.4 and receiving very strange deprecation warnings.\\n[2019-10-07T07:08:33,504][WARN ][o.e.d.t.TransportInfo    ] [node-01.example.com] transport.publish_address was printed as [ip:port] instead of [hostname/ip:port]. This format is deprecated and will change to [hostname/ip:port] in a future version. Use -Des.transport.cname_in_publish_address=true to enforce non-deprecated formatting.\\n[2019-10-07T07:08:33,504][WARN ][o.e.d.h.HttpInfo         ] [node-01.example.com] es.http.cname_in_publish_address system property is deprecated and no longer affects http.publish_address formatting. Remove this property to get rid of this deprecation warning.\\nI started to review which address was actually published:\\ncurl \"https://node-02.example.com.com:9200/_nodes\" | jq \".nodes.http.publish_address\"\\n\"node-07.example.com/10.13.0.160:9200\"\\n\"node-06.example.com/10.13.0.151:9200\"\\n\"node-08.example.com/10.13.0.143:9200\"\\n\"node-01.example.com/10.13.0.51:9200\"\\n\"node-05.example.com/10.13.0.141:9200\"\\n\"node-03.example.com/10.13.0.53:9200\"\\n\"node-04.example.com/10.13.0.55:9200\"\\n\"node-02.example.com/10.13.0.52:9200\"\\nAs for me, looks good.\\nThe second one was:\\n/etc/elasticsearch/jvm.options\\nThere are no extra options such as mentioned in warning.\\nThe actual command line is:\\n/usr/share/elasticsearch/jdk/bin/java -Xms256m -Xmx256m -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+DisableExplicitGC -XX:+AlwaysPreTouch -server -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -Djdk.io.permissionsUseCanonicalPath=true -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Dlog4j.skipJansi=true -XX:+HeapDumpOnOutOfMemoryError -Dio.netty.allocator.type=unpooled -XX:MaxDirectMemorySize=134217728 -Des.path.home=/usr/share/elasticsearch -Des.path.conf=/etc/elasticsearch -Des.distribution.flavor=default -Des.distribution.type=rpm -Des.bundled_jdk=true -cp /usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch -p /var/run/elasticsearch/elasticsearch.pid --quiet\\nPlease help me get rid of this warnings.\\nIt\\'s important for us, because we are going to migrate to ESv7 and there are a lot of systems that are using deprecated features, this warning produce a lot of noise.'},\n",
       " {'text_field': \"I don't have the code in front of me right now, but if I remember correctly you can set es.http.cname_in_publish_address to false to suppress this warning until 7.4.1 comes out.\"},\n",
       " {'text_field': 'Hello Kibana gurus,\\nBoy, I need your help.  I have built a network sFlow collector, which collects a lot of network data.  That data is successfully flowing into Elasticsearch &amp; Kibana.  Let’s say for a toy example, I can see the following data in Kibana:\\nSender        Receiver      Protocol  Sample Rate  Total Length\\n10.10.10.10   20.20.20.20   TCP       64           1500\\n10.10.10.10   30.30.30.30   UDP       64           1500\\n10.10.10.10   20.20.20.20   TCP       64           1500\\n20.20.20.20   10.10.10.10   TCP       64           1500\\n10.10.10.10   20.20.20.20   TCP       64           1500\\n10.10.10.10   20.20.20.20   TCP       64           100\\n10.10.10.10   30.30.30.30   UDP       64           1500\\n\\nI can combine that data by merging whenever Sender/Receiver/Protocol/Sample Rate stats are the same and summing “Total Length”:\\nSender        Receiver      Protocol  Sample Rate  Total Length\\n10.10.10.10   20.20.20.20   TCP       64           4600\\n10.10.10.10   30.30.30.30   UDP       64           3000\\n20.20.20.20   10.10.10.10   TCP       64           1500\\n\\nWhat’s more, because this is sampled data, I really need to multiply those last columns:\\nSender        Receiver      Protocol  Total Data Sent\\n10.10.10.10   20.20.20.20   TCP       294400\\n10.10.10.10   30.30.30.30   UDP       192000\\n20.20.20.20   10.10.10.10   TCP       96000\\n\\nThis is highly useful.  What I need is a Kibana Visualization that produces the last chart.\\nI’ve been reading through the Kibana documentation and tutorials and just haven’t gotten very far at all.  Obviously, I want a Data Table type.  My buckets would be Sender / Receiver / Protocol / Sample Rate… I assume.  But I’m uncertain how to specify the Metrics here:  These would be… sums?  Unique counts?  I’m not sure.\\nCan anyone point me to a tutorial or offer some advice on how to build this kind of Visualization?  Is this something that can be done in one Visualization, or would I have to potentially build visualizations on top of one another, as I might have to build a MySQL query?\\nMany thanks!'},\n",
       " {'text_field': 'I got this working...  Posting the complete solution here, in case anyone else is following in my footsteps.  Thanks Lukas!\\nSTEP ONE:  Define an Scripted Field\\n\\tManagement --&gt; Index Patterns --&gt; (select your current index) --&gt; \"Scripted Fields\" Tab\\n\\t\"Add scripted field\"\\n\\tFill out the form:\\n\\t\\tLanguage:  painless\\n\\t\\tType:  number\\n\\t\\tFormat:  (default)\\n\\t\\tPopularity:  0\\n\\t\\tScript:\\n\\t\\t\\tdoc[\\'SamplingRate\\'].value * doc[\\'TotalLen\\'].value\\n\\n\\t\\tHow to read the script:\\n\\t\\t\\t\"doc[\\'SamplingRate\\'].value\"  ==  \"Consult the data record/doc, find statistic \"SamplingRate,\" get its value\\n\\n\\tMake your changes and click \"CREATE FIELD\"\\n\\nSTEP TWO:  Assemble the Visualization\\n\\n\\tVisualizations --&gt; \"Create new visualization\" --&gt; Data Table --&gt; (select your current index)\\n\\tCreate Four Buckets:\\n\\t\\tFor IP Addresses:\\n\\t\\t\\t\"Add Buckets\" --&gt; Split Rows --&gt; Aggregation: Terms --&gt; Field: (your field here) --&gt; Metric: Alphabetical --&gt; (Everything else default)\\n\\t\\tFor Numbers:\\n\\t\\t\\t\"Add Buckets\" --&gt; Split Rows --&gt; Aggregation: Terms --&gt; Field: (your field here) --&gt; Metric: Descending --&gt; (Everything else default)\\n\\tNow Add Metrics:\\n\\t\\tMetric --&gt; Sum --&gt; Field: (Your created Scripted Field, above)\\n\\t\\tMetric --&gt; Min --&gt; Field: @timestamp\\n\\t\\tMetric --&gt; Max --&gt; Field: @timestamp\\n\\tClick \"Save,\" upper left'},\n",
       " {'text_field': 'I\\'m trying to input the file which name is same for everytime\\nIt reads well but I\\'d like to output only the last line appended in that time\\nmy pipeline configuration is here\\ninput {\\nfile{\\npath =&gt; \"/home/lisa/elasticstack/data_store/hmi/hmi.csv\"\\nstart_position=&gt;\"beginning\"\\ntype =&gt; \"hello\"\\n}\\n}\\nfilter {\\nif [type] == \"hello\" {\\ncsv{\\nseparator=&gt;\",\"\\ncolumns=&gt;[\"message number\",\"timestamp\",\"state\",\"message text\"]\\n}\\ngrok { match =&gt; [ \"path\", \"/(?[^/]+).csv\" ] }\\n}\\n}\\noutput {\\nstdout { codec =&gt; rubydebug }\\n}\\nthe problem is it reads and ouput whole content of overwritten file..\\nI just want to use only appended last line\\nIs there any way to use overwritten file(only appended but overwritten) in this case?..\\nBest Regards'},\n",
       " {'text_field': 'How are you appending a line to the end of the file? If you use an editor this typically results in a new file being stored with the same name as the old, which means Logstash will reprocess it as the inode changed. Instead try something like this: echo \"1,2019-10-08:05:00:00,CA,test message\" &gt;&gt; /home/lisa/elasticstack/data_store/hmi/hmi.csv'},\n",
       " {'text_field': \"Hey guys,\\nI recently upgraded my 3 node ES cluster to version 7.4. This cluster, which has been running forever (literally years) without any real problems, just started falling apart since then. Nothing besides the ES version has changed: Each node runs Ubuntu 18.04 with Java 11, has 6Gb of RAM (half of that allocated to ES), there is only 10Gb of data in ~70 indices (and ~140 shards). Data intake and usage haven't changed. But since that update, the cluster only stays up for a few hours, after which each node dies one after the other, all with the stacktrace like https://pastebin.com/6Wqqxg6r. Since the trace points to a out-of-memory problem, I tried deleteing indices, playing around with GC settings, allocating both more and less memory to ES and locking memory according to https://www.elastic.co/guide/en/elasticsearch/reference/7.4/setting-system-settings.html#systemd. Nothing changed! I can't get my beloved cluster in a stable state. Intake volume seems to correlate with this and crash the nodes faster, but even just turning Xpack monitoring on is seemingly too much and kills one node after the other.\\nAnother strange observation: While I can still delete indices, forcemerge just does nothing. I tried to merge segments in an attempt to give ES some air, but the command just instantly returns and leaves the deleted docs unttouched. Nothing in the logs about that.\\nAfter a weekend of desperation, I am at my wits end and don't have any inclination on what to try next. Any help would be highly appreciated!\\nBest,\\nFrederik\"},\n",
       " {'text_field': \"Hi @frederikwerner\\nLet's start with the networking/memory issue:\\ncould you share the contents of your jvm.options file please so I can take a look?\\nIt looks like you might just be missing the following line in there (added by default in 7.4 jvm.options but if you reused the file from a previous version it's missing):\\n-Dio.netty.allocator.numDirectArenas=0\\n\\nThanks!\"},\n",
       " {'text_field': \"Hello folks,\\nOnce again I am turning to you.\\nMy SSL/TLS works. However when I add my HTTPS, it fails. I have added my certificate password with the command from the link below. I use the same cert for my TLS &amp; HTTPS.\\nError from journalctl or status doesn't say much except that elasticsearch is failing.\\nAny insight ?\\nTried\\n\\nRemoving password from keystore and re-adding them.\\n\\nConfig\\nxpack.security.enabled: true\\nxpack.security.transport.ssl.enabled: true\\nxpack.security.http.ssl.enabled: true\\n\\nxpack.security.http.ssl.keystore.path: certs/master1.p12\\nxpack.security.http.ssl.truststore.path: certs/master1.p12\\n\\nxpack.security.transport.ssl.keystore.path: certs/master1.p12\\nxpack.security.transport.ssl.truststore.path: certs/master1.p12\\n\\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-tls.html\"},\n",
       " {'text_field': \"So... for some reason I cannot explain, the certificate password got removed. It was an empty string. I had to change all my secure keys to match up.\\nHow did I found out ?\\n\\nExported the certificate on a desktop.\\nTried to install the certificate with the password entered at creation.\\nIt didn't work so I tried an empty string and it was successful.\\n\\nIf that can be any help to someone out there.\"},\n",
       " {'text_field': 'Hello\\nI am getting below exception while running fscrawler on linux. I wish the error was little more descriptive. I am using java 8 and fscrawler-es7-2.7\\nbin/fscrawler test\\nException in thread \"main\" java.lang.NumberFormatException: Infinite or NaN\\nat java.math.BigDecimal.(BigDecimal.java:898)\\nat java.math.BigDecimal.(BigDecimal.java:875)\\nat fr.pilato.elasticsearch.crawler.fs.cli.BootstrapChecks.computePercentage(BootstrapChecks.java:61)\\nat fr.pilato.elasticsearch.crawler.fs.cli.BootstrapChecks.checkJvm(BootstrapChecks.java:57)\\nat fr.pilato.elasticsearch.crawler.fs.cli.BootstrapChecks.check(BootstrapChecks.java:40)\\nat fr.pilato.elasticsearch.crawler.fs.cli.FsCrawlerCli.main(FsCrawlerCli.java:141)'},\n",
       " {'text_field': \"Hey. This bug should have been fixed with\\n\\n  \\n      github.com/dadoonet/fscrawler\\n  \\n  \\n      \\n    \\n  \\n\\n\\n  Fix Percentage computation\\n\\n\\n\\n  by dadoonet\\n  on 09:14AM - 27 Sep 19 UTC\\n\\n\\n\\n  1 commits\\n  changed 2 files\\n  with 9 additions\\n  and 3 deletions.\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nBut sadly this patch has not been uploaded to the snapshot repository. I'm running a manual deploy now and you should hopefully see in some minutes a new version of the 2.7-SNAPSHOT uploaded on Oct the 8th.\\nI hope this will help.\"},\n",
       " {'text_field': '\\nVersion: 7.4.0\\nDeployment: AWS ECS Fargate\\n\\nI\\'m getting this error:\\nEADDRNOTAVAIL: Cannot assign requested address - bind - Cannot assign requested address\\nSince I\\'m deploying to Fargate, I can\\'t in advance know (or even determine) what the host\\'s IP address will be. It appears from a variety of topics, that people have figured this out, unfortunately, I\\'m not seeing a solution in any of them.\\nLogstash.yml\\nhttp.host: \"\"\\nhttp.port: 9600\\nxpack.monitoring.enabled: true\\nxpack.monitoring.elasticsearch.hosts: [\"https://72298998827d49a3921c2b7060be2394.us-west-2.aws.found.io:9243\" ]\\nxpack.monitoring.elasticsearch.username: logstash_application_user\\nxpack.monitoring.elasticsearch.password: ${es_password}\\nxpack.management.enabled: true\\nxpack.management.pipeline.id: [\"rawlogs\"]\\nxpack.management.elasticsearch.hosts: [ \"https://72298998827d49a3921c2b7060be2394.us-west-2.aws.found.io:9243\" ]\\nxpack.management.elasticsearch.username: logstash_admin_user\\nxpack.management.elasticsearch.password: ${logstash_admin}\\nxpack.management.logstash.poll_interval: 5s\\nDockerfile:\\nFROM docker.elastic.co/logstash/logstash:7.4.0\\nRUN rm -f /usr/share/logstash/pipeline/logstash.conf\\nADD pipeline/ /usr/share/logstash/pipeline/\\nADD config/ /usr/share/logstash/config/'},\n",
       " {'text_field': 'It appears that my issue was specifying the Elastic IP assigned to the network load balancer, instead of the docker default of \"0.0.0.0\".\\nThe topic\\'s error is gone, but the application is still being shutdown, without any apparent cause.'},\n",
       " {'text_field': 'I am trying to move a dashboard from the default space to another.  following this blog (https://www.elastic.co/blog/how-to-migrate-to-kibana-spaces)\\n[root@vm21:/etc/kibana ] $ curl -k -u elastic:somepassword https://vm21:5601/app/kibana#/dashboard/0ced9280-e937-11e9-8780-d10befb3ed2b &gt; export.json\\nit created the a file export.json\\nI then tried to import:\\ncurl -k -H \"Content-Type: application/json\" -H \"kbn-xsrf: true\" -u elastic:somepasswprd https://vm21:5601/s/testspace1/app/kibana/import --data-binary @export.json\\ngot a response:\\n{\"statusCode\":404,\"error\":\"Not Found\",\"message\":\"Not Found\"}\\ni also tried this:\\ncurl -k -H \"Content-Type: application/json\" -H \"kbn-xsrf: true\" -u elastic:somepassword https://vm21:5601/s/testspace1/api/kibana/dashboards/import --data-binary @export.json\\ngot a response:\\n{\"statusCode\":400,\"error\":\"Bad Request\",\"message\":\"Invalid request payload JSON format\"}[root@vm21:/etc/kibana ] $\\nI\\'d appreciate any help/tips.\\nthanks,\\nsirjune'},\n",
       " {'text_field': '@elasticforme: i followed the blog i mentioned in the original post\\n@stephenb: you\\'re correct. i missed some in the command line. tired eyes yesterday.\\nthese allowed me to export/import the dashboard to another space.\\n\\nexport Dashboard\\ncurl -k -u elastic:mypassword https://vm21:5601/api/kibana/dashboards/export?dashboard=42ad2f70-e937-11e9-8780-d10befb3ed2b &gt; export.json\\nimport Dashboard\\ncurl -k -H \"Content-Type: application/json\" -H \"kbn-xsrf: true\" -u elastic:mypassword https://vm21:5601/s/testspace1/api/kibana/dashboards/import --data-binary @export.json\\n\\n\\nwhen i login as testuser (who i gave access to testspace1). It loads the dashboard but a box pops out (lower right corner) with \"Unable to update UI setting. Request failed with status code: 403\"'},\n",
       " {'text_field': 'I\\'m currently trying to install ES on an RPi 4B. I\\'ve used this thread as a sort of guide.\\nHere are the steps I\\'ve taken:\\nsudo apt-get install default-jre\\nsudo nano /etc/profile\\nexport JAVA_HOME=$(readlink -f /usr/bin/java | sed \"s:/bin/java::\") export PATH=$JAVA_HOME/bin:$PATH\\nThose two entries were added to the end of the file.\\nsudo reboot\\ncd Downloads\\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.4.0-no-jdk-amd64.deb sudo dpkg -i --force-all --ignore-depends=libc6 elasticsearch-7.4.0-no-jdk-amd64.deb\\nAt this point, ES appears to install, but I get this message during install:\\ncould not find java in JAVA_HOME or bundled at /usr/share/elasticsearch/jdk/bin/java\\nI then try and modify the elasticsearch config file, specifically the JAVA_HOME entry from this thread with the following command:\\nsudo nano /etc/default/elasticsearch\\n# Elasticsearch Java path\\nJAVA_HOME=/usr/lib/jvm/java-11-openjdk-armhf\\nIt appears though that the service still will not start. This is the service status:\\npi@raspberrypi:~/Downloads $ sudo systemctl status elasticsearch.service\\n● elasticsearch.service - Elasticsearch\\nLoaded: loaded (/usr/lib/systemd/system/elasticsearch.service; disabled; vendor preset: enabled)\\nActive: failed (Result: exit-code) since Mon 2019-10-07 15:10:34 PDT; 9s ago\\nDocs: http://www.elastic.co\\nProcess: 5880 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -p ${PID_DIR}/elasticsearch.pid --quiet (code=exited, stat\\nMain PID: 5880 (code=exited, status=1/FAILURE)\\nOct 07 15:10:33 raspberrypi elasticsearch[5880]: at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.jav\\nOct 07 15:10:33 raspberrypi elasticsearch[5880]: at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAw\\nOct 07 15:10:33 raspberrypi elasticsearch[5880]: at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java\\nOct 07 15:10:33 raspberrypi elasticsearch[5880]: at org.elasticsearch.cli.Command.main(Command.java:90)\\nOct 07 15:10:33 raspberrypi elasticsearch[5880]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:1\\nOct 07 15:10:33 raspberrypi elasticsearch[5880]: at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:9\\nOct 07 15:10:33 raspberrypi elasticsearch[5880]: Refer to the log for complete error details.\\nOct 07 15:10:34 raspberrypi systemd[1]: elasticsearch.service: Main process exited, code=exited, status=1/FAILURE\\nOct 07 15:10:34 raspberrypi systemd[1]: elasticsearch.service: Failed with result \\'exit-code\\'.\\nOct 07 15:10:34 raspberrypi systemd[1]: Failed to start Elasticsearch.\\nDoes anyone know how to get this working? Or is there a specific configuration issue that jumps out? Any help i can get on this is much appreciated. Thank you.'},\n",
       " {'text_field': 'Someone will likely tell me that \"omg, dropping a jna file into the lib folder isn\\'t supported!!!\", but this is the only way I have been been able to get elasticsearch 7.4.0 on the Raspberry Pi operational. Anyway, I hope this helps.\\nUse the following commands to install Bellsoft OpenJRE 11:\\n\\nwget https://download.bell-sw.com/java/11.0.5+11/bellsoft-jre11.0.5+11-linux-arm32-vfp-hflt.deb\\nsudo dpkg -i bellsoft-jre11.0.5+11-linux-arm32-vfp-hflt.deb\\n\\nNow we\\'re going to take the JRE that we installed and pre-game by copying it into a directory that the elasticsearch installer will recognize. This makes it so the installer completes all tasks (including the creation of the keystore) successfully. Then we proceed to install elasticsearch:\\n\\nsudo mkdir -p /usr/share/elasticsearch/jdk\\nsudo cp -rf /usr/lib/jvm/bellsoft-java11-arm32-vfp-hflt/* /usr/share/elasticsearch/jdk\\nwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.4.0-no-jdk-amd64.deb\\nsudo dpkg -i --force-all --ignore-depends=libc6 elasticsearch-7.4.0-no-jdk-amd64.deb\\n\\nBecause we installed a deb package by ignoring a dependency, any time you try to update/install software with the aptitude installer it will now insist you that you first fix missing dependencies by removing elasticsearch (bad). To fix this, you need to edit the dpkg status file with \"sudo nano /var/lib/dpkg/status\" and do a search for elasticsearch.\\nThis should bring you to a section that looks like this:\\n\\nPackage: elasticsearch\\nStatus: install ok installed\\nPriority: optional\\nSection: web\\nInstalled-Size: 198434\\nMaintainer: Elasticsearch Team info@elastic.co\\nArchitecture: amd64\\nSource: elasticsearch\\nVersion: 7.4.0\\nDepends: bash (&gt;= 4.1), lsb-base (&gt;= 4), libc6, adduser, coreutils (&gt;= 8.4)\\n\\nWhat you want to do here is remove the libc6 dependency from the \"Depends\" line (last line). Afterwards, save and exit the file by pressing CTRL X, then press Y when prompted, and then Enter.\\nThe next two commands are  where the JNA magic happens:\\n\\nsudo mv /usr/share/elasticsearch/lib/jna-4.5.1.jar /usr/share/elasticsearch/lib/jna-4.5.1.jar.old\\nsudo wget -P /usr/share/elasticsearch/lib http://repo1.maven.org/maven2/net/java/dev/jna/jna/4.5.1/jna-4.5.1.jar\\n\\nThe fact that I had to use a different jna.jar file is interesting, and a little perplexing. Additionally, it seems that something about the way the elasticsearch service starts changed quite a bit from 7.3.2 to 7.4.0. In 7.3.2 JNA didn\\'t seem to matter because the log would show the service starting with JNA options disabled, and although the 7.4.0 log reflects the same JNA options disabled as in 7.3.2, unless the jna-4.5.1.jar file is replaced, the 7.4.0 service never starts. I suspect that JNA is required now, but the logs aren\\'t quite reflecting that. Additionally, this is also causing elasticsearch to take a much longer time to successfully start as a service. Moving on ...\\nNow we will append some options to the end of the elasticsearch.yml file to disable machine learning (necessary) and SecComp:\\n\\necho \\'xpack.ml.enabled: false\\' | sudo tee -a /etc/elasticsearch/elasticsearch.yml\\necho \\'bootstrap.system_call_filter: false\\' | sudo tee -a /etc/elasticsearch/elasticsearch.yml\\n\\nNote that disabling SecComp isn\\'t completely necessary as the service will start, but the log will complain about it with the following:\\n[WARN ][o.e.b.JNANatives ] unable to install syscall filter:\\njava.lang.UnsupportedOperationException: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed\\nThe last two commands are pretty self explanatory:\\n\\nsudo systemctl enable elasticsearch\\nsudo systemctl start elasticsearch\\n'},\n",
       " {'text_field': 'I am currently trying to generate a certificate that will allow us to connect to kibana from a remote browser with the FQDN subdomain.domain.com. I am having trouble finding a way to for the browser to find the CA certificate without installing on the end user pc.\\nThe commands I use are:\\nelasticsearch-certutil ca\\nelasticsearch-certutil cert --ca elastic-stack-ca.p12\\nI then create a certificate in PEM Format:\\nelasticsearch-certutil cert --ca elastic-stack-ca.p12 --dns subdomain.domain.com --ip public ip --name subdomain.domain.com --pem\\nI have also tried using the same CN as the CA certificate but get the same result.\\nThe Root Ca is missing off the chain until you install it on the end user pc.\\nI am deploying the FQDN to a group of users and I need to make it as basic as possible to access Kibana without the need to install certificates.\\nI could be missing some extremely obvious so any advise or guidance would be extremely appreciated.'},\n",
       " {'text_field': '\\n\\n\\n cowensel:\\n\\nI am having trouble finding a way to for the browser to find the CA certificate without installing on the end user pc.\\n\\n\\nThis is how web PKI works, a good resource to gain some further understanding is https://developer.mozilla.org/en-US/docs/Mozilla/Security/x509_Certificates.\\nIn summary, browsers and operating systems come bundled with a list of CA certificates and they trust the certificates that are signed by these CA certificates. Obviously, the CA certificate that you generate with elasticsearch-certutil ca is not one of these CA certificates that browser trust and when you create the subdomain.domain.com certificate and sign it with this CA, the clients that try to connect to it, do not trust it.\\nYour options are to\\na) Ship the CA certificate and have the users install it in their browsers so that they can trust the kibana certificate\\nb) Get a certificate for kibana that is signed by a known and well trusted CA which your users browsers will be trusting already. See for instance, the list of CAs that firefox and chrome trust by default'},\n",
       " {'text_field': 'I have file json(/tmp/es/ecep_elib_table_books.json) with content:\\n\\nimage.png584×616 58.1 KB\\n\\nI config file /_default/6/_settings.json\\n\\nimage.png449×523 38.2 KB\\n\\nI config file _settings.json into my job\\n\\nWhen I run bin/fscrawler --config_dir ./demo_01 demo_01 --restart, I get error\\n\\nimage.png1280×800 213 KB\\n\\nI hope someone can help my fix it \\nThanks.'},\n",
       " {'text_field': 'Sorry, I will learn from experience next time.\\nThe great thing is that I solved the problem above.\\nThank bro.'},\n",
       " {'text_field': \"I have ES and logstash 7.4 installed on a RPi 4B. I understand that Kibana no longer supports 32-bit operating systems, but I found another thread where someone installed 64-bit Kibana on a 32-bit OS.\\nI figured I could attempt this on Raspbian, so here are the commands I used:\\nsudo apt-get install nodejs\\nmkdir /usr/share/kibana &amp;&amp; cd /usr/share/kibana\\nwget https://artifacts.elastic.co/downloads/kibana/kibana-7.4.0-amd64.deb\\nsudo dpkg -i --force-all kibana-7.4.0-amd64.deb\\nFinally, I attempted to start it using my node installation from the directory /usr/share/kibana. Kibana attempts to start, but after several seconds I get a fatal error:\\npi@raspberrypi:/usr/share/kibana $ /usr/bin/node src/cli\\nlog [04:29:36.441] [info][plugins-system] Setting up [4] plugins: [security,translations,inspector,data]\\nlog [04:29:36.501] [info][plugins][security] Setting up plugin\\nlog [04:29:36.509] [warning][config][plugins][security] Generating a random key for xpack.security.encryptionKey. To prevent sessions from being invalidated on restart, please set xpack.security.encryptionKey in kibana.yml\\nlog [04:29:36.511] [warning][config][plugins][security] Session cookies will be transmitted over insecure connections. This is not recommended.\\nlog [04:29:36.843] [info][plugins][translations] Setting up plugin\\nlog [04:29:36.846] [info][data][plugins] Setting up plugin\\nlog [04:29:36.859] [info][plugins-system] Starting [3] plugins: [security,translations,data]\\nlog [04:30:43.568] [fatal][root] Error: /usr/share/kibana/node_modules/@elastic/nodegit/build/Release/nodegit.node: wrong ELF class: ELFCLASS64\\nat Object.Module._extensions..node (internal/modules/cjs/loader.js:718:18)\\nat Module.load (internal/modules/cjs/loader.js:599:32)\\nat tryModuleLoad (internal/modules/cjs/loader.js:538:12)\\nat Function.Module._load (internal/modules/cjs/loader.js:530:3)\\nat Module.require (internal/modules/cjs/loader.js:637:17)\\nat require (internal/modules/cjs/helpers.js:22:18)\\nat Object.&lt;anonymous&gt; (/usr/share/kibana/node_modules/@elastic/nodegit/dist/nodegit.js:12:12)\\nat Module._compile (internal/modules/cjs/loader.js:689:30)\\nat Module._compile (/usr/share/kibana/node_modules/pirates/lib/index.js:99:24)\\nat Module._extensions..js (internal/modules/cjs/loader.js:700:10)\\nat Object.newLoader [as .js] (/usr/share/kibana/node_modules/pirates/lib/index.js:104:7)\\nat Module.load (internal/modules/cjs/loader.js:599:32)\\nat tryModuleLoad (internal/modules/cjs/loader.js:538:12)\\nat Function.Module._load (internal/modules/cjs/loader.js:530:3)\\nat Module.require (internal/modules/cjs/loader.js:637:17)\\nat require (internal/modules/cjs/helpers.js:22:18)\\nlog [04:30:43.582] [info][plugins-system] Stopping all plugins.\\nlog [04:30:43.584] [info][data][plugins] Stopping plugin\\nlog [04:30:43.586] [info][plugins][translations] Stopping plugin\\nlog [04:30:43.587] [info][plugins][security] Stopping plugin\\nFATAL Error: /usr/share/kibana/node_modules/@elastic/nodegit/build/Release/nodegit.node: wrong ELF class: ELFCLASS64\\nI realize that it's not meant to run on a 32-bit OS, but if anyone could help here I would appreciate it. The other Kibana thread I linked seemed to have success using Node to accomplish this, but I'm at a loss on how to continue in my specific circumstance.\"},\n",
       " {'text_field': '@sixian_he Yes, I can tell you. Sorry it\\'s taken so long to reply. I feel like I\\'ve been as helpful as every other joker who hasn\\'t replied here. For what it\\'s worth, for as well designed as this forum is, I feel like team members should find it easier to reply with something, but it seems as though the opposite is true. Moving on ...\\nFirst, download and install kibana:\\n\\ncd ~/ &amp;&amp; wget https://artifacts.elastic.co/downloads/kibana/kibana-7.4.0-amd64.deb\\nsudo dpkg -i --force-all kibana-7.4.0-amd64.deb\\n\\nBuild @elastic/nodegit and ctags on your platform to replace the nodegit amd64 build files located at kibana/node_modules/@elastic/nodegit/build with the one you just compiled.\\nThe following command installs nodejs and npm. Note that only nodejs is required to run our custom kibana install, not npm. However, npm is needed to install packages we will use to replace in kibana:\\n\\nsudo apt-get install nodejs npm\\n\\nThe following commands will install nodegit. Phantomjs is used in conjunction with npm to build nodegit.\\nNodegit prereqs:\\n\\nsudo apt-get install libkrb5-dev\\n\\nNPM install of nodegit. As stated above, nodegit depends on phantomjs, which we will also download:\\n\\ncd ~/ &amp;&amp; git clone https://github.com/nodegit/nodegit.git &amp;&amp; cd nodegit\\nwget https://github.com/fg2it/phantomjs-on-raspberry/releases/download/v2.1.1-wheezy-jessie-armv6/phantomjs\\nexport PATH=$PATH:~/nodegit\\nsudo chmod -R 777 ~/nodegit\\nnpm install\\n\\nThe following commands backup the old nodegit module:\\n\\nsudo mv /usr/share/kibana/node_modules/@elastic/nodegit/build/Release /usr/share/kibana/node_modules/@elastic/nodegit/build/Release.old\\nsudo mv /usr/share/kibana/node_modules/@elastic/nodegit/dist/enums.js /usr/share/kibana/node_modules/@elastic/nodegit/dist/enums.js.old\\n\\nReplace nodegit module with one just built:\\n\\nsudo cp -rf ~/nodegit/build/Release /usr/share/kibana/node_modules/@elastic/nodegit/build\\nsudo cp ~/nodegit/dist/enums.js /usr/share/kibana/node_modules/@elastic/nodegit/dist\\n\\nInstall node-ctags into local nodegit directory:\\n\\ncd ~/nodegit\\nnpm install ctags\\n\\nThe following commands change the folder name and backup the old ctags module:\\n\\nsudo mv /usr/share/kibana/node_modules/@elastic/node-ctags/ctags/build/ctags-node-v64-linux-x64 /usr/share/kibana/node_modules/@elastic/node-ctags/ctags/build/ctags-node-v64-linux-arm\\nsudo mv /usr/share/kibana/node_modules/@elastic/node-ctags/ctags/build/ctags-node-v64-linux-arm/ctags.node /usr/share/kibana/node_modules/@elastic/node-ctags/ctags/build/ctags-node-v64-linux-arm/ctags.node.old\\n\\nReplace node-ctags module with one just built:\\n\\nsudo cp ~/nodegit/node_modules/ctags/build/Release/ctags.node /usr/share/kibana/node_modules/@elastic/node-ctags/ctags/build/ctags-node-v64-linux-arm\\n\\nEdit the kibana start script so that it uses your node installation:\\n\\nsudo nano /usr/share/kibana/bin/kibana\\n\\nLocate the line that starts with NODE=\"${DIR}/node/bin/node\" and edit it to mirror the below example:\\n\\nNODE=\"/usr/bin/node\"\\n\\nSave and exit the file.\\nEnable and start the kibana service. Be patient as kibana takes a while to load:\\n\\nsudo systemctl enable kibana\\nsudo systemctl start kibana\\n'},\n",
       " {'text_field': 'Hello,\\nI have json log like this:\\n{\"thread\":\"http-nio-8943-exec-9\",\"level\":\"INFO\",\"loggerName\":\"cz.direct.common.restapi.interceptor.LoggingFilter\",\"message\":{\"requestNumber\":140,\"responseCode\":200,\"contentType\":\"application/json;charset=UTF-8\",\"headers\":{\"X-Fishtag\":\"tal1d20191008083158852\"},\"payload\":{\"version\":\"1.1.13-SNAPSHOT\",\"buildDateTime\":\"2019-09-30T10:14\",\"runningTime\":685055.0}},\"endOfBatch\":false,\"loggerFqcn\":\"org.apache.commons.logging.LogAdapter$Log4jLog\",\"instant\":{\"epochSecond\":1570516318,\"nanoOfSecond\":859087000},\"contextMap\":{\"fishtag\":\"tal1d20191008083158852\"},\"threadId\":33,\"threadPriority\":5,\"timestamp\":\"2019.10.08 08:31:58.859\"}\\nfilebeat.yml:\\nprocessors:\\n- decode_json_fields:\\nfields: [\"message\"]\\nfilebeat.prospectors:\\n-   fields:\\nenv: dev02\\ntype: restapilog\\nfields_under_root: true\\npaths:\\n- /var/log/app/restapi-talend-/logs/actual/restapi-talend-.log\\ntags:\\n- app-dev-02-insurance-restapi-talend\\n- java-based\\ntype: log\\nlogstash filter:\\nfilter {\\nif[type] == \"restapilog\" {\\ndate {\\nmatch =&gt; [\"timestamp\", \"yyyy.MM.dd HH:mm:ss.SSS\"]\\ntimezone =&gt; \"Europe/Prague\"\\ntarget =&gt; \"@timestamp\"\\n}\\n}\\nand kibana output:\\n\\n1.PNG1397×854 43.3 KB\\n\\nFilebeat version: 6.8.3-1\\nELK: 6.8.2-1'},\n",
       " {'text_field': 'i solved it on my own...'},\n",
       " {'text_field': 'Hello, I have several times an index will get stuck due to mis-configuration or get an error while testing out the ILM API. I should hopefully get this all resolved but what bothers me is once an index gets stuck like this I don\\'t know how to progress it.\\nExample:\\nI had a policy for warm phase\\n\"warm\": {\\n    \"min_age\": \"1d\",\\n    \"actions\": {\\n      \"readonly\" : { },\\n      \"forcemerge\": {\\n        \"max_num_segments\": 1\\n      },\\n      \"allocate\": {\\n        \"include\" : {\\n          \"box_type\": \"hot,warm\"\\n        }\\n      }\\n    }\\n  }\\n\\nIt would get stuck saying \"Waiting for [5] shards to be allocated to nodes matching the given filters\". Issue is I have no box_type of hot or warm. So how could I then fix this misconfiguration and continue the ilm? Is it possible to update the executing policy on an index? Or to assign a new policy to index but change it\\'s lifetime?'},\n",
       " {'text_field': \"Also there's an API to manually move index into a new step https://www.elastic.co/guide/en/elasticsearch/reference/6.8/ilm-move-to-step.html#_path_parameters_16\"},\n",
       " {'text_field': 'Hello,\\nI have a Elasticsearch cluster with following configuration.\\nElasticsearch version : 5.1.1\\nNumber of Nodes : 10\\nA daily snapshot process is running in the Cluster. In which the last snapshot was created few days ago and it\\'s status is still IN_PROGRESS . I tried to delete the pending snapshot via api but it fails.\\nAlso I have performed a cluster rolling restart, created a new repository and tried with taking new snapshot but its giving snapshot in progress exception.\\nI have listed the tasks in cluster and only found the snapshot delete task with \"cancellable\" : false.\\nPlease help me to solve this. Thanks in advance.'},\n",
       " {'text_field': 'There were bugs in older Elasticsearch versions such as yours where snapshots could get stuck. A full cluster restart will solve the problem. So will upgrading to a recent version of ES (e.g. 6.8 or 7.4)'},\n",
       " {'text_field': 'Hi guys,\\nIs it possible to set the default time range through the kibana API?\\nIn other words, is it possible to automatically edit the kibana advanced settings through the kibana API?\\nThanks in advance \\nCheers,\\nBalaji'},\n",
       " {'text_field': 'Hi @astrobalaji,\\nyes, this is possible. You can use the saved object API to do this. The object type is config.'},\n",
       " {'text_field': 'Hi Team,\\nI have below logstash configuration file by trying to connect to SAP HANA database(HANA DB and elk stack are not on same laptop). But there\\'s error when executing logstash -f ../config/logstashJDBC.conf.\\n\\ninput {\\n\\tjdbc{\\n\\t     jdbc_connection_string =&gt; \"jdbc:sap://qa13hdbbe901.lab.od.sap.biz:30115\"\\n\\t     jdbc_driver_library =&gt; \"/usr/sap/hdbclient/\"\\n\\t     jdbc_driver_class =&gt; \"com.sap.db.jdbc.Driver\"\\n\\t     jdbc_user =&gt; \"SF_READONLY\"\\n\\t     jdbc_password =&gt; \"Read0n1ysf123\"\\n\\t     statement =&gt; \"SELECT NOMINEE_ID, NOMINATION_ID, NOMINEE_USER_ID, READINESS, CREATED_DATE, CREATED_BY FROM SFBIZX3_TALSHELLEYH.SM_NOMINEE\"\\n\\t     use_column_value =&gt; true\\n\\n\\t}\\n}\\n\\n\\n\\noutput { \\n         elasticsearch {\\n\\t                     action =&gt; \"index\"\\n\\t                     hosts  =&gt; \"localhost:9200\"\\n\\t                     index  =&gt; \"testjdbc\"\\n                         manage_template =&gt; true\\n                         template =&gt; \"C:/elkstack/elasticsearch-7.0.1-windows-x86_64/mapping/dynamic.json\"\\n                         template_name=&gt; \"dynamic\"\\n                         template_overwrite =&gt; true\\n                         document_id =&gt; \"%{NOMINEE_ID}\"\\n         \\t           }\\n         stdout { codec =&gt; rubydebug }\\n        }\\n\\n\\nError is\\n\\n[2019-10-08T16:28:34,386][DEBUG][logstash.outputs.stdout  ] Closing {:plugin=&gt;\"LogStash::Outputs::Stdout\"}\\n[2019-10-08T16:28:34,392][DEBUG][logstash.outputs.elasticsearch] Closing {:plugin=&gt;\"LogStash::Outputs::ElasticSearch\"}\\n[2019-10-08T16:28:34,411][DEBUG][logstash.outputs.elasticsearch] Stopping sniffer\\n[2019-10-08T16:28:34,417][DEBUG][logstash.outputs.elasticsearch] Stopping resurrectionist\\n[2019-10-08T16:28:34,931][DEBUG][logstash.outputs.elasticsearch] Waiting for in use manticore connections\\n[2019-10-08T16:28:34,946][DEBUG][logstash.outputs.elasticsearch] Closing adapter #&lt;LogStash::Outputs::ElasticSearch::HttpClient::ManticoreAdapter:0x3fd3c880&gt;\\n[2019-10-08T16:28:34,959][ERROR][logstash.javapipeline    ] Pipeline aborted due to error {:pipeline_id=&gt;\"main\", :exception=&gt;#&lt;LogStash::ConfigurationError: Must set :tracking_column if :use_column_value is true.&gt;, :backtrace=&gt;[\"C:/elkstack/logstash-7.0.1/logstash-7.0.1/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.13/lib/logstash/inputs/jdbc.rb:212:in `register\\'\", \"C:/elkstack/logstash-7.0.1/logstash-7.0.1/logstash-core/lib/logstash/java_pipeline.rb:191:in `block in register_plugins\\'\", \"org/jruby/RubyArray.java:1792:in `each\\'\", \"C:/elkstack/logstash-7.0.1/logstash-7.0.1/logstash-core/lib/logstash/java_pipeline.rb:190:in `register_plugins\\'\", \"C:/elkstack/logstash-7.0.1/logstash-7.0.1/logstash-core/lib/logstash/java_pipeline.rb:280:in `start_inputs\\'\", \"C:/elkstack/logstash-7.0.1/logstash-7.0.1/logstash-core/lib/logstash/java_pipeline.rb:244:in `start_workers\\'\", \"C:/elkstack/logstash-7.0.1/logstash-7.0.1/logstash-core/lib/logstash/java_pipeline.rb:145:in `run\\'\", \"C:/elkstack/logstash-7.0.1/logstash-7.0.1/logstash-core/lib/logstash/java_pipeline.rb:104:in `block in start\\'\"], :thread=&gt;\"#&lt;Thread:0x5b156470 run&gt;\"}\\n[2019-10-08T16:28:34,988][ERROR][logstash.agent           ] Failed to execute action {:id=&gt;:main, :action_type=&gt;LogStash::ConvergeResult::FailedAction, :message=&gt;\"Could not execute action: PipelineAction::Create&lt;main&gt;, action_result: false\", :backtrace=&gt;nil}\\n[2019-10-08T16:28:34,998][TRACE][logstash.agent           ] Converge results {:success=&gt;false, :failed_actions=&gt;[\"id: main, action_type: LogStash::PipelineAction::Create, message: Could not execute action: PipelineAction::Create&lt;main&gt;, action_result: false\"], :successful_actions=&gt;[]}\\n[2019-10-08T16:28:35,057][DEBUG][logstash.agent           ] Starting puma\\n[2019-10-08T16:28:35,076][DEBUG][logstash.agent           ] Trying to start WebServer {:port=&gt;9600}\\n[2019-10-08T16:28:35,095][DEBUG][logstash.instrument.periodicpoller.os] Stopping\\n[2019-10-08T16:28:35,141][DEBUG][logstash.instrument.periodicpoller.jvm] Stopping\\n[2019-10-08T16:28:35,146][DEBUG][logstash.instrument.periodicpoller.persistentqueue] Stopping\\n[2019-10-08T16:28:35,149][DEBUG][logstash.instrument.periodicpoller.deadletterqueue] Stopping\\n\\n\\nAnyone can advice?'},\n",
       " {'text_field': 'You could try adding the driver jar file to &lt;logstash install dir&gt;/logstash-core/lib/jars/, but be aware that could get removed during a logstash upgrade.'},\n",
       " {'text_field': \"Hi, This is Jason.\\nI'm in trouble for deleting hard-limited ml-job.\\n\\nimage.png1678×75 7.81 KB\\n\\nI'd deleted them yesterday, however It's still deleting \\nI used dev-tool to delete them again today, but failed like below\\n\\nimage.png730×586 27.8 KB\\n\\nHow can I delete these jobs?\\nThanks &amp; regards\\nJason\"},\n",
       " {'text_field': 'Since your are on v6.8.0 you might also want to consider the following:\\nDELETE _ml/_delete_expired_data\\n\\nThis request normally occurs every night with our nightly maintenance, but there is a bug in our nightly maintenance scheduler. The bug is fixed in 6.8.4 and 7.4.1. But, since this bug exists in your version, your jobs will continually accumulate results and state without bound.'},\n",
       " {'text_field': 'Hi\\nI enabled SSL, but logstash met below error:\\n\\n[2019-10-08T17:21:54,292][ERROR][logstash.licensechecker.licensereader] Unable to retrieve license information from license server {:message=&gt;\"No Available connections\"}\\n[2019-10-08T17:21:55,728][WARN ][logstash.outputs.elasticsearch] Attempted to resurrect connection to dead ES instance, but got an error. {:url=&gt;\"https://elastic:xxxxxx@192.168.0.178:9200/\", :error_type=&gt;LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=&gt;\"Elasticsearch Unreachable: [https://elastic:xxxxxx@192.168.0.178:9200/][Manticore::ClientProtocolException] PKIX path validation failed: java.security.cert.CertPathValidatorException: Path does not chain with any of the trust anchors\"}\\n\\nelasticsearch log:\\n\\n[2019-10-08T17:39:32,351][WARN ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [master-node] http client did not trust this server\\'s certificate, closing connection [id: 0x937b59d2, L:0.0.0.0/0.0.0.0:9200 ! R:/192.168.0.179:45462]\\n\\nI used below command to generate certificate:\\n\\nbin/elasticsearch-certutil cert ca --pem --out certs/certs.zip\\n\\nThen generated below files:\\n\\nca.crt , instance.crt, instance.key\\n\\nelasticsearch.yml:\\n\\nxpack.ssl.key: /etc/elasticsearch/certs/instance.key\\nxpack.ssl.certificate: /etc/elasticsearch/certs/instance.crt\\nxpack.ssl.certificate_authorities: /etc/elasticsearch/certs/ca.crt\\n\\nlogstash.yml:\\n\\nxpack.monitoring.enabled: true\\nxpack.monitoring.elasticsearch.username: elastic\\nxpack.monitoring.elasticsearch.password: *****\\nxpack.monitoring.elasticsearch.hosts: [\"https://192.168.0.178:9200\"]\\nxpack.monitoring.elasticsearch.ssl.certificate_authority: \"/etc/logstash/ca.crt\"\\nxpack.monitoring.elasticsearch.ssl.verification_mode: certificate\\nxpack.monitoring.elasticsearch.sniffing: true\\nhttp.host: \"192.168.0.179\"\\n\\nlog.conf:\\n\\ninput {\\nfile {\\npath =&gt; \"/var/log/messages\"\\n}\\n}\\noutput {\\nelasticsearch {\\nhosts =&gt; [\"https://192.168.0.178:9200\"]\\nindex =&gt; \"system-syslog-%{+YYYY.MM}\"\\nuser =&gt; \"elastic\"\\npassword =&gt; \"*****\"\\nssl =&gt; true\\ncacert =&gt; \"/etc/logstash/ca.crt\"\\n}\\n}\\n\\nlogstash and elasticsearch version : 6.7.0'},\n",
       " {'text_field': 'solution: https://www.elastic.co/blog/how-to-setup-tls-for-elasticsearch-kibana-logstash-filebeat-with-offline-install-in-linux'},\n",
       " {'text_field': \"In TSVB, the time series graph changes when I add metrics and aggregations in the Data Tab, but the Metric and Gauge does not change and remains 'Zero' only.\\n\\nTSVB.png1282×647 30.1 KB\\n\\nKindly Help!\"},\n",
       " {'text_field': \"It is a workaround to change your TSVB interval to a much larger value, if that's what you're asking. The button to switch from the interval to the entire time range is in 7.4\"},\n",
       " {'text_field': 'Does Canvas support interactivity through features like click-to-filter, drill-down, zoom, tooltips etc?'},\n",
       " {'text_field': '@sreed, Canvas has very limited interactivity at the moment. The original idea was that Canvas was designed for presentational purposes as opposed to data exploration, however, this kind of interactivity has been coming up more recently so it is something that we are considering for our future roadmap.'},\n",
       " {'text_field': 'I have the mapping defined for \"timestamp\" field on ES as follows:\\n\"timestamp\": { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss Z\" }\\nand I\\'m trying to index the following document,\\n{ \"account\":\"170/170/78202\", \"timestamp\":\"2019-10-06T10:52:09-04:00\" }\\nBut getting exception as \"failed to parse date field [2019-10-06T10:52:09-04:00] with format [yyyy-MM-dd HH:mm:ss Z]\". In ES version 6.7, it worked well, but in 7.3.2, it doesn\\'t work. Do we need to change in the way we index the date type values or need to adjust the mapping we defined in the index?'},\n",
       " {'text_field': 'The problem was with the colon( : ) in the timezone value. I\\'ve removed that and it worked.\\n\\'{\"account\":\"170/170/78202\",\"timestamp\":\"2019-10-06 10:52:09 -0400\"}\\''},\n",
       " {'text_field': \"Hello,\\n2 of the most interesting event id's in siem are 4688 and 4689, which can be enabled with a gpo and enable us to monitor every command used in your network.\\nInteresting fields are:\\nwinlog.event_data.NewProcessName\\nwinlog.event_data.ParentProcessName\\nwinlog.event_data.ProcessName\\n\\nUnfortunately some of the processes audited are located in %USERPROFILE% folder, which means it looks like this:\\nC:\\\\Users\\\\username\\\\AppData\\\\Local\\\\GitHubDesktop\\\\app-2.2.0\\\\resources\\\\app\\\\git\\\\mingw64\\\\bin\\\\git.exe\\nIt would be nice if the security processor of Winlogbeat could get expanded and the executable itself could be extracted to process.name\\nThat would enable us to do all kinds of siem related interesting analytics on them.\\nIs this already on a to do? Should I make a GitHub issue?\\nGrtz\\nWillem\"},\n",
       " {'text_field': \"@andrewkroh For now I created https://github.com/elastic/beats/issues/14038\\nOnce PR 13530 has been done, I'll see if I can create a PR if I find some spare time\"},\n",
       " {'text_field': 'Hi All,\\nI´m currently running ELK in a dockerized environment, and I use marathon/mesos to manage it. Up until now I was running a single node cluster but it is obviously not fit for production. So, my goal is to set up a two node cluster, with two elasticsearch nodes and a kibana client capable of using them both.\\nright now im setting in kibana the url of elasticsearch like this:\\n\\n                \"env\": {\\n                    \"ELASTICSEARCH_HOSTS\": \"http://99.9.99.99:9200\"\\n                },\\n\\n\\nI need to add a second url for another elastic search node. So far I´ve tried all combinations that I could think of and none of them work, like:\\n\\n\"http://99.9.99.99:9200, http://99.9.99.98:9200\"\\n\"[http://99.9.99.99:9200, http://99.9.99.98:9200]\"\\n\"\"[http://99.9.99.99:9200\", \"http://99.9.99.98:9200\"]\"\\n\\nBut I always get the same error:\\n\\n{\"type\":\"log\",\"@timestamp\":\"2019-10-07T19:37:34Z\",\"tags\":[\"fatal\",\"root\"],\"pid\":1,\"message\":\"{ ValidationError: child \"elasticsearch\" fails because [child \"hosts\" fails because [single value of \"hosts\" fails because [\"hosts\" must be a valid uri with a scheme matching the http|https pattern]]]\\\\n    at Object.exports.process (/usr/share/kibana/node_modules/joi/lib/errors.js:196:19)\\\\n    at internals.Object._validateWithOptions (/usr/share/kibana/node_modules/joi/lib/types/any/index.js:675:31)\\\\n    at module.exports.internals.Any.root.validate (/usr/share/kibana/node_modules/joi/lib/index.js:146:23)\\\\n    at Config._commit (/usr/share/kibana/src/legacy/server/config/config.js:139:35)\\\\n    at Config.set (/usr/share/kibana/src/legacy/server/config/config.js:108:10)\\\\n    at Config.extendSchema (/usr/share/kibana/src/legacy/server/config/config.js:81:10)\\\\n    at extendConfigService (/usr/share/kibana/src/legacy/plugin_discovery/plugin_config/extend_config_service.js:45:10) name: \\'ValidationError\\' }\"}\\n\\nI´m trying to avoid having a custom bind kibana.yml since this complicates deployment, so it would be very usefull to find a way of having this working using an external property.\\nThanks in advance!'},\n",
       " {'text_field': '@Merlin_Nunez I\\'m not familiar with marathon, but my first thought then is maybe you need to escape the string and write something like:\\n\"[\\\\\"http://99.9.99.99:9200\\\\\",\\\\\"http://99.9.99.98:9200\\\\\"]\"\\n\\nor\\n\"[\\\\\\\\\\\\\"http://99.9.99.99:9200\\\\\\\\\\\\\",\\\\\\\\\\\\\"http://99.9.99.98:9200\\\\\\\\\\\\\"]\"\\n\\nThe last one I\\'m suggesting is because in quick search I ran, looks like an user reported in the marathon repo that he was having problems escaping the string and he was only successful that way.\\nCheers'},\n",
       " {'text_field': 'Hello,\\nI started working on Logstash and Elasticsearch and my first task was trying to get XML files indexed by Elasticsearch.\\nFor that I have a\\n\\nFile input plugin that reads whole XML files\\nXML filter plugin which creates properties using XPath\\nand an Elasticsearch output plugin where it all should get added to\\n\\nMy configuration looks like this\\ninput {\\n  file {\\n    path =&gt; \"/absolute/path/to/xmls/**/*.xml\"\\n    start_position =&gt; \"beginning\"\\n    max_open_files =&gt; 10000\\n    mode =&gt; \"read\"\\n    close_older =&gt; \"1 minute\"\\n    codec =&gt; multiline {\\n      pattern =&gt; \"\\\\Z\"\\n      what =&gt; \"previous\"\\n    }\\n  }\\n}\\n\\nfilter {\\n  xml {\\n    source =&gt; \"message\"\\n    store_xml =&gt; false\\n    force_array =&gt; false\\n    xpath =&gt; [\\n      \\'/html/head/meta_identity/identifier/text()\\', \"meta_identity_identifier\",\\n      \\'/html/head/meta_identity/sortkey/text()\\', \"meta_identity_sortkey\",\\n      \\'/html/head/meta_identity/database/text()\\', \"meta_identity_database\",\\n      \\'/html/head/meta_identity/langauge/text()\\', \"meta_identity_language\"\\n    ]\\n  }\\n}\\n\\noutput {\\n  elasticsearch {\\n    index =&gt; \"xml-data\"\\n    hosts =&gt; [\"localhost:9200\"]\\n    sniffing =&gt; false\\n  }\\n\\n  stdout { codec =&gt; rubydebug }\\n}\\n\\nNow to evaluate, let\\'s take an XML file. Here is a snippet of it to visualise:\\n&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n &lt;meta_identity&gt;\\n  &lt;identifier&gt;a00001&lt;/identifier&gt;\\n  &lt;sortkey&gt;000.000&lt;/sortkey&gt;\\n  &lt;database&gt;Guidelines&lt;/database&gt;\\n  &lt;language&gt;en&lt;/language&gt;\\n &lt;/meta_identity&gt;\\n ...\\n&lt;/head&gt;\\n&lt;/html&gt;\\n\\nAs can be seen from my filter, I want to do a very simple thing and extract these four element values into properties for Elasticsearch.\\nBut my problem is that the XPath entries are not being parsed, I get everything else (@version, @timestamp, etc) but none of the properties I defined in either Elasticsearch or stdOut.\\nI tried creating a mutator to see if that might fix my issue:\\nfilter {\\n  xml {\\n    ...\\n  }\\n\\n  mutate {\\n    replace =&gt; [\\n      \"meta_identity_identifier\", \"%{meta_identity_identifier}\",\\n      \"meta_identity_sortkey\", \"%{meta_identity_sortkey}\",\\n      \"meta_identity_database\", \"%{meta_identity_database}\",\\n      \"meta_identity_language\", \"%{meta_identity_language}\"\\n    ]\\n  }\\n}\\n\\nNow I can see the properties, but the value is not what it is supposed to be. The values are shown as %{meta_identity_language} etc.\\nLogstash doesn\\'t give any insight when run in --verbose.\\nWhat am I missing?'},\n",
       " {'text_field': 'Well I finally managed to solve it.\\nIt turns out that some files had an xmlns attribute defined on html which caused XPath to not be able to parse anything.\\nEven if I assigned specifically that it exists in XPath: /html[@xmlns=\"http://domain.tld/path/to\"] it wasn\\'t able to resolve it.\\nFinal solution was to set remove_namespaces to true.'},\n",
       " {'text_field': 'Hi,\\nCan any one please let me know, how to disable/hide the kibana Management tab for some users,i used the predefined role and it works but the kibana_dashboard_only_user shows all the available spaces and i want to show a certain spaces not all of them\\nKibana version 7.2\\nThanks.'},\n",
       " {'text_field': '@haythem I see! Ok so you can do this:\\nhttps://www.elastic.co/guide/en/kibana/current/xpack-dashboard-only-mode.html#advanced-dashboard-mode-configuration\\nJust create a new role with access only to your desired spaces and than declare this role as dashboard only.'},\n",
       " {'text_field': 'Hello Folks,\\nI use SSL/TLS encryption, it works fine. I use keystore to access my password. When I launch winlogbeat from powershell, it fails, see error below.\\nHowever, if I make it run manually from PowerShell, I have no errors. I can see my logs on Kibana.\\nThat is how I call my password.\\noutput.elasticsearch:\\n  protocol: \"https\"\\n  username: \"elastic\"\\n  password: \"${ES_PWD}\"\\n\\n.\\\\winlogbeat.exe test config -c .\\\\winlogbeat.yml -e return Config OK.\\nError\\nWinlogbeat logs\\n2019-10-08T10:38:27.985-0400\\tERROR\\tinstance/beat.go:878\\tExiting: error initializing publisher: missing field accessing \\'output.elasticsearch.password\\' (source:\\'C:\\\\Program Files\\\\winlogbeat\\\\winlogbeat.yml\\')\\nPowerShell\\n&gt; start-service : Service \\'winlogbeat (winlogbeat)\\' cannot be started due to the following error: Cannot open winlogbeat\\n&gt; service on computer \\'.\\'.\\n&gt; At line:1 char:1\\n&gt; + start-service winlogbeat\\n&gt; + ~~~~~~~~~~~~~~~~~~~~~~~~\\n&gt;     + CategoryInfo          : OpenError: (System.ServiceProcess.ServiceController:ServiceController) [Start-Service],\\n&gt;    ServiceCommandException\\n&gt;     + FullyQualifiedErrorId : CouldNotStartService,Microsoft.PowerShell.Commands.StartServiceCommand\\n\\nI have read from a previous post to copy my keystore where my winlogbeat.yml file is, which I did, but it still fails. Any insight ?\\n\\nLinks\\n\\n  \\n    \\n    \\n    Using Keystores - not working (v6.6.0) Beats\\n  \\n  \\n    Hello, \\nI\\'m trying to use a keystore value in my beat configuration and I can\\'t seem to get it working... \\nFor example - I\\'m currently trying it with heartbeat. \\nTHIS WORKS - \\noutput.elasticsearch: \\npassword: \"clear_text_pw\" \\nTHIS DOES NOT WORK - \\noutput.elasticsearch: \\npassword: \"${ES_PWD}\" \\nWhen starting heartbeat the log gives me: \\nExiting: error initializing publisher: missing field accessing \\'output.elasticsearch.password\\' (source:\\'/etc/heartbeat/heartbeat.yml\\') \\nI definitely have the value…\\n  \\n\\n'},\n",
       " {'text_field': 'So I found the answer. It was the most ... easiest thing I haven\\'t tested. I feel quit dumb haha.\\nSolution\\npassword: \"${ES_PWD}\" =&gt; password: \"$ES_PWD\"\\nThat was it. Moving keystore where winlogbeat.yml is changes nothing. Tried it. I really laughed at myself on that.'},\n",
       " {'text_field': 'Kibana version: 7.4.0\\nElasticsearch version: 7.4.0\\nAPM Server version: 7.4.0\\nAPM Agent language and version: Java and 1.10.0\\nTrying to get metrics from JMX for Spring Boot microservice which is deployed in Kubernetes. Enabled Spring actuator where I can see the metrics using \"/actuator\" endpoint.\\nIncluded the following configuration in elasticapm.properties file\\ncapture_jmx_metrics=\"object_name[com.zaxxer.hikari:type=Pool,name=*] attribute[TotalConnections] attribute[ActiveConnections]\"\\nI am expecting Hikari connection pool metrics in Elasticsearch, but not seeing anything. By default APM Javagent reporting other metrics like JVM, GC, transaction performance metrics.\\nAm I missing something, or do I need to configure anything to capture Hikari Pool connections metrics.\\nThank you for any help.'},\n",
       " {'text_field': \"I found another bug which prevented the registration of JMX metrics \\nCould you try again? https://apm-ci.elastic.co/job/apm-agent-java/job/apm-agent-java-mbp/job/PR-879/5/artifact/src/github.com/elastic/apm-agent-java/\\nI have successfully tested it manually now with\\ncapture_jmx_metrics=object_name[com.zaxxer.hikari:type=HikariDataSource,name=*] attribute[ConnectionTimeout]\\n\\nYou should see logs similar to this:\\nDEBUG co.elastic.apm.agent.jmx.JmxMetricTracker - MBean added at runtime: com.zaxxer.hikari:type=HikariDataSource,name=*\\nDEBUG co.elastic.apm.agent.jmx.JmxMetricTracker - Found mbeans for object name com.zaxxer.hikari:type=HikariDataSource,name=*\\nDEBUG co.elastic.apm.agent.jmx.JmxMetricTracker - Found number attribute ConnectionTimeout=30000\\nDEBUG co.elastic.apm.agent.jmx.JmxMetricTracker$JmxMetricRegistration - Registering JMX metric com.zaxxer.hikari:name=dataSource,type=HikariDataSource ConnectionTimeout.null as metric_name: jvm.jmx.ConnectionTimeout labels: name=dataSource, type=HikariDataSource\\n\\nYou'll find the metrics in the index apm-*-metric\\n\\n\\n\\n\\nfield name\\nvalue\\n\\n\\n\\n\\nlabels.name\\ndataSource\\n\\n\\nlabels.type\\nHikariDataSource\\n\\n\\njvm.jmx.ConnectionTimeout\\n30000\\n\\n\\n\\n\"},\n",
       " {'text_field': 'I am upgrading my JS agent from 3.0.0 to 4.5.0 and the custom transactions I had already implemented no longer work. I have an angular app so I am using angular-rum 0.2.0.\\nI have enabled debug mode so I can see it is detecting external.http spans but somehow not including them in the customs transaction I created.\\nCan anyone say what I am doing wrong?\\nHere is an example:\\nTransactionService.startTransaction Transaction {name: \"XXX\", .....}\\nlogging-service.js:56 TransactionService.startSpan POST http://localhost:8080/XXX external.http\\nlogging-service.js:56 TransactionService.addTask task1\\nlogging-service.js:56 TransactionService.removeTask task1\\nlogging-service.js:56 TransactionService.startSpan GET http://localhost:8080/YYY external.http\\nlogging-service.js:56 TransactionService.addTask task1\\nlogging-service.js:56 TransactionService.removeTask task1\\nlogging-service.js:56 TransactionService transaction finished Transaction {name: \"XXX\",...}\\nlogging-service.js:56 Transaction was discarded! Transaction does not include any spans\\nlogging-service.js:56 Could not create a payload from the Transaction Transaction {name: \"XXX\",...}\\nlogging-service.js:56 TransactionService.add Transaction {name: \"XXX\",...}'},\n",
       " {'text_field': \"If you are creating the transaction through custom API, you can use the managed option in the API so that the custom transaction becomes managed one and all automatically instrumented spans would be added to them.\\napm.startTransaction('custom', 'custom', {\\n    managed: true\\n})\\n\\nThanks,\\nVignesh\"},\n",
       " {'text_field': 'After resolving my previous errors the logstash application is starting successfully, connecting with Elasticsearch on Elastic Cloud, validating the licenses successfully, starting the pipelines, and metrics, and successfully running a healthcheck; then a sigterm is issued and the process is killed. I\\'ve set the logs to \"debug\" and have not found any issues.\\nThe ECS Container Healthcheck runs successfully with a 200 response status code, then shortly thereafter, a sigterm is issued:\\n[2019-10-08T22:13:18,844][DEBUG][logstash.agent ] API HTTP Request {:status=&gt;200, :request_method=&gt;\"GET\", :path_info=&gt;\"/\", :query_string=&gt;\"pretty\", :http_version=&gt;\"HTTP/1.1\", :http_accept=&gt;\"/\"}\\n[2019-10-08T22:13:20,141][DEBUG][org.logstash.execution.PeriodicFlush] Pushing flush onto pipeline.\\n[2019-10-08T22:13:20,449][DEBUG][logstash.agent ] Converging pipelines state {:actions_count=&gt;0}\\n[2019-10-08T22:13:20,588][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ParNew\"}\\n[2019-10-08T22:13:20,588][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[2019-10-08T22:13:25,141][DEBUG][org.logstash.execution.PeriodicFlush] Pushing flush onto pipeline.\\n[2019-10-08T22:13:25,455][DEBUG][logstash.agent ] Converging pipelines state {:actions_count=&gt;0}\\n[2019-10-08T22:13:25,593][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ParNew\"}\\n[2019-10-08T22:13:25,593][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[2019-10-08T22:13:30,141][DEBUG][org.logstash.execution.PeriodicFlush] Pushing flush onto pipeline.\\n[2019-10-08T22:13:30,444][DEBUG][logstash.agent ] Converging pipelines state {:actions_count=&gt;0}\\n[2019-10-08T22:13:30,599][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ParNew\"}\\n[2019-10-08T22:13:30,600][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[2019-10-08T22:13:35,141][DEBUG][org.logstash.execution.PeriodicFlush] Pushing flush onto pipeline.\\n[2019-10-08T22:13:35,443][DEBUG][logstash.agent ] Converging pipelines state {:actions_count=&gt;0}\\n[2019-10-08T22:13:35,605][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ParNew\"}\\n[2019-10-08T22:13:35,605][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[2019-10-08T22:13:40,141][DEBUG][org.logstash.execution.PeriodicFlush] Pushing flush onto pipeline.\\n[2019-10-08T22:13:40,444][DEBUG][logstash.agent ] Converging pipelines state {:actions_count=&gt;0}\\n[2019-10-08T22:13:40,610][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ParNew\"}\\n[2019-10-08T22:13:40,610][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=&gt;\"ConcurrentMarkSweep\"}\\n[2019-10-08T22:13:42,030][WARN ][logstash.runner ] SIGTERM received. Shutting down.\\nlogstash.yml\\nnode.name: Fargate-Logstash\\ncloud.id: \\ncloud.auth: logstash_admin_user:${logstash_admin}\\nhttp.host: \"0.0.0.0\"\\nhttp.port: 9600\\nlog.level: debug\\nxpack.monitoring.enabled: true\\nxpack.monitoring.elasticsearch.hosts: [\"Elasticsearch host URL on Elastic Cloud\" ]\\nxpack.monitoring.elasticsearch.username: logstash_application_user\\nxpack.monitoring.elasticsearch.password: ${es_password}\\nxpack.management.enabled: true\\nxpack.management.pipeline.id: [\"rawlogs\", \".monitoring-logstash\"]\\nxpack.management.elasticsearch.hosts: [ \"Elasticsearch host URL on Elastic Cloud\" ]\\nxpack.management.elasticsearch.username: logstash_admin_user\\nxpack.management.elasticsearch.password: ${logstash_admin}'},\n",
       " {'text_field': 'Ugh, I found the problem, it was a misconfiguration in AWS.'},\n",
       " {'text_field': 'I have input file plugin configured in read mode where I read the entire file and use multiline to concat it into a single event.\\ninput {\\n  file {\\n    path =&gt; \"/absolute/path/to/xmls/**/*.xml\"\\n    start_position =&gt; \"beginning\"\\n    max_open_files =&gt; 10000\\n    mode =&gt; \"read\"\\n    close_older =&gt; \"1 minute\"\\n    codec =&gt; multiline {\\n      charset =&gt; \"ISO-8859-1\"\\n      pattern =&gt; \"\\\\Z\"\\n      what =&gt; \"previous\"\\n    }\\n  }\\n}\\n\\nOn initial run it worked perfectly. However I am confused as to what happens when an XML file, with the same filename gets added again with the last modified updated?\\nIn theory, the file should be read again? Currently it doesn\\'t seem to do so.\\nI tried by adding an XML file with a filename that got processed earlier. The XML files last modified has changed to a more recent time but with 10 minutes into waiting, the file has yet to be read by Logstash.\\nThe reason why I want to re-use XML files with the same name is that these have content that occasionally get updated so I want to pull them in again and my elsticsearch output upserts the content if needed (based on document_id and XML internal updated timestamp).\\nCould someone explain to me how the file updates should work? Could it be that I have to disable .sincedb file from being created (/dev/null)?'},\n",
       " {'text_field': 'it is going by inode. hence it is not reading it because logstash thinks it has already read file sitting on same inode.\\ncheck mode detail explanation that I put, as I had exact same problem\\nhttps://discuss.elastic.co/t/elk-kibana-showing-data-that-was-already-deleted-wont-show-new-data/181631/2'},\n",
       " {'text_field': 'For an Elasticsearch cluster with 3 master only nodes and 2 data nodes, do I point elasticsearch.hosts in kibana.yml at the data nodes or the master only nodes (or both)?'},\n",
       " {'text_field': '3 master 2 data is a fine architecture.\\nPoint Kibana at the data nodes\\nIf you want a little more  production / HA you could perhaps take a look at this'},\n",
       " {'text_field': 'I have two Apache webservers (1x uat and 1x prod). I am sending the logs to my ELK setup for both of them.\\nFor the UAT one, it is working fine.\\nFor the PROD one, the logs are now shown in Kibana.\\nI need some help to debug it, and find out where the issue is. I\\'ll post some config files below so you guys have an idea how this is setup.\\n\\nThe LogFormat config on the httpd.conf is the same for both environments.\\nPROD Apache version: Apache/2.2.15 (Unix)\\nUAT Apache version: Apache/2.2.9 (Unix)\\n\\n/etc/logstash/conf.d/logstash.conf:\\ninput {\\n\\n## PROD\\nfile {\\n        type =&gt; \"apache_access_log\"\\n        start_position =&gt; \"beginning\"\\n        path =&gt; \"/mnt/logs/web/access_log\"\\n    }\\n\\n## UAT\\n\\nfile {\\n        type =&gt; \"uat_apache_access_log\"\\n        start_position =&gt; \"beginning\"\\n        path =&gt; \"/mnt/logs/uatweb/access_log\"\\n    }\\n\\n}\\n\\n\\nfilter {\\n    # Remove unwanted carrage returns, global to all filter types\\n    mutate {\\n            gsub =&gt; [ \\'message\\', \"\\\\r\", \\'\\' ]\\n    }\\n\\n    ######################################################\\n\\n# PROD\\n# Apache access filter\\n\\n    if [type] == \"apache_access_log\" {\\n        mutate {\\n            replace =&gt; { \\'host\\' =&gt; \\'webserver.datacentre.example.com\\' }\\n            add_field =&gt; { \\'environment\\' =&gt; \\'production\\'\\n                           \\'service\\' =&gt; \\'apache_access\\'\\n            }\\n        }\\n        grok {\\n            match =&gt; {\\n                \"message\" =&gt; \"%{IPORHOST:clientip}%{SPACE}\\\\[%{HTTPDATE:timestamp}\\\\]%{SPACE}%{NUMBER:port}%{SPACE}%{WORD:method}%{SPACE}%{URIPATHPARAM:request_uri}%{SPACE}%{NOTSPACE}%{SPACE}%{NUMBER:status_code}%{SPACE}%{NOTSPACE:bytes_delivered}%{SPACE}%{NUMBER:duration%}%{SPACE}(?:%{URI:referrer}|.*)%{SPACE}%{QS:agent}%{SPACE}%{GREEDYDATA:general_data}\"\\n            }\\n        }\\n\\n        date {\\n            match =&gt; [\"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\\n            target =&gt; \"@timestamp\"\\n        }\\n    }\\n\\n# UAT\\n# Apache access filter\\n\\n    if [type] == \"uat_apache_access_log\" {\\n        mutate {\\n            replace =&gt; { \\'host\\' =&gt; \\'uatweb.datacentre.example.com\\' }\\n            add_field =&gt; { \\'environment\\' =&gt; \\'uat\\'\\n                           \\'service\\' =&gt; \\'apache_access\\'\\n            }\\n        }\\n        grok {\\n            match =&gt; {\\n                \"message\" =&gt; \"%{IPORHOST:clientip}%{SPACE}\\\\[%{HTTPDATE:timestamp}\\\\]%{SPACE}%{NUMBER:port}%{SPACE}%{WORD:method}%{SPACE}%{URIPATHPARAM:request_uri}%{SPACE}%{NOTSPACE}%{SPACE}%{NUMBER:status_code}%{SPACE}%{NOTSPACE:bytes_delivered}%{SPACE}%{NUMBER:duration%}%{SPACE}(?:%{URI:referrer}|.*)%{SPACE}%{QS:agent}%{SPACE}%{GREEDYDATA:general_data}\"\\n            }\\n        }\\n\\n        date {\\n            match =&gt; [\"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\"]\\n            target =&gt; \"@timestamp\"\\n        }\\n    }\\n\\noutput {\\nelasticsearch {\\n    hosts =&gt; [\"localhost:9200\"]\\n    # Weekly index (for pruning)\\n    index =&gt; \"mw-log-index-%{+YYYY.\\'w\\'ww}\"\\n}\\nstdout { codec =&gt; rubydebug }\\n}\\n\\nOn KIbana, there is NO logs for PROD:\\n\\nScreenshot from 2019-10-09 13-53-04.png916×160 10.9 KB\\n\\n... however for UAT there are:\\n\\nScreenshot from 2019-10-09 13-53-41.png1526×290 20.7 KB\\n\\n/var/log/logstash/logstash-plain.log:\\n[2019-10-09T13:45:04,253][DEBUG][org.logstash.config.ir.CompiledPipeline][main] Compiled filter\\n P[filter-mutate{\"replace\"=&gt;{\"host\"=&gt;\"webserver.datacentre.example.com\"}, \"add_field\"=&gt;{\"environment\"=&gt;\"production\", \"service\"=&gt;\"apache_access\"}}|[str]pipeline:209:9:```\\nmutate {\\n            replace =&gt; { \\'host\\' =&gt; \\'webserver.datacentre.example.com\\' }\\n            add_field =&gt; { \\'environment\\' =&gt; \\'production\\'\\n                           \\'service\\' =&gt; \\'apache_access\\'\\n            }\\n        }\\n```]\\n\\nHow can I troubleshoot this? where to start looking?'},\n",
       " {'text_field': 'The string /web/access_log never occurs in that file, which tells me that logstash never sees the file. Are you sure that the name is right and that logstash has execute access to that directory?'},\n",
       " {'text_field': 'I\\'m using the ODBC driver to connect Tableau to Elasticsearch and ran into this. Through trial and error, the user connected to the driver needs to have the manage role in order to pull in the \"tables\" from ES. I tried by giving only read and monitor privs for the indices, but Tableau couldn\\'t find the names of the indices I needed with only those two.\\nUnder the manage priv, it says it has:\\nAll monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate).\\n\\nI\\'d love it if I could exclude the delete priv and keep the other ones open. Is there any way to do a custom one?\\nThis is definitely a problem for anyone using the Tableau connector to ES and has users with different permission levels because they need to have this priv in order to make the connection to indices happen at all.'},\n",
       " {'text_field': 'No it didn\\'t work, but I just got it to work with \"read\" and \"view_index_metadata\" for the index privs. Thanks for your help!'},\n",
       " {'text_field': 'How to index file json has size large 100mb with fscrawler and elastic.\\nI use fscrawler 2.6 and elastic 6.8.0.'},\n",
       " {'text_field': 'I fixed it.\\nDone'},\n",
       " {'text_field': 'Hey everybody,\\nthe title seems as if the solution is easy... but my problem is a bit more tricky.\\nI have several products which can have several articles and each article has a price list with different price list ids.\\nThis is a snippet of my elastic document:\\n\"article\": [\\n{\\n\"price_list\": [\\n{\\n\"price_list_id\": 1,\\n\"price_net\": 756.26\\n},\\n{\\n\"price_list_id\": 2,\\n\"price_net\": 688.2\\n},\\n{\\n\"price_list_id\": 3,\\n\"price_net\": 688.2\\n},\\n{\\n\"price_list_id\": 4,\\n\"price_net\": 739.45\\n}\\n]\\n}\\n]\\nAnd I need to get all products which have at least one article with the price_list_id 4 AND a price_net range with gte = 413 and lte = 702.\\nIf I try this :\\n\"bool\": {\\n\"must\": [\\n{\\n\"range\": {\\n\"article.price_list.price_net\": {\\n\"gte\": 413,\\n\"lte\": 702\\n}\\n}\\n},\\n{\\n\"term\": {\\n\"article.price_list.price_list_id\": 4\\n}\\n}\\n]\\n}\\nthe above snippet matches. But it should not because the price_net with the price_list_id = 4 is not between 413 and 702\\nHas anyone a solution for my problem?\\nThanks a lot.'},\n",
       " {'text_field': 'If you are searching for multiple fields within an array, then you need to take a look at the nested datatype and the nested query\\nhope this helps!\\n--Alex'},\n",
       " {'text_field': \"HI all,\\nI am using ELK stack of version 7.1.1 with x-pack installed.\\ni am able to create an index pattern but my documents are not appearing in the that index pattern.\\nI have created an index pattern which matches my index\\n\\ncompletion of pattern creation with timestamo filter\\n\\nScreenshot_2019-10-09 logstash- - Kibana.png602×563 26.8 KB\\n\\nCross verified whether it has documents in it or not\\n\\nScreenshot_2019-10-09 Kibana(1).png754×226 16 KB\\n\\nBut on the discover section. i'm not able to see  those logs\\n\\nScreenshot_2019-10-09 Discover - Kibana.png925×619 21.4 KB\\n\\nand im able to see my syslogs in syslog-* but not able to see logstash indices logs in  logstash-*\\ni'm so confused please help solve it.\"},\n",
       " {'text_field': 'Hello,\\nIt looks like your Time Filter is set to just show logs from \"Today\". Expand it to include a wide date/time range and you should have better luck!\\nRegards,\\nAaron'},\n",
       " {'text_field': 'can kibana connect to hive without using elasticsearch ?'},\n",
       " {'text_field': 'oke, thank you bro '},\n",
       " {'text_field': \"Hi!\\nI'm testing Elastic Cloud to see if this is something for us, and I'm first implementing it in our Go application using the Go client (https://github.com/elastic/apm-agent-go).\\nRight now I'm finding out how I'm going to implement how to start transactions and spans, our application is spread across different modules doing different stuff, so if I  want to start a transaction in our top level function for handling http, and want to start spans around in our different modules to track operations I have to pass the transaction around to the functions in other modules to be able to create spans.\\nThis changes just about every function signature and therefore screws up our testing and would require a fair amount of refactoring. is there a way to access the transaction without having to pass it around in function parameters?\"},\n",
       " {'text_field': 'Hey @nicolaigj, welcome to the forum!\\n\\nThis changes just about every function signature and therefore screws up our testing and would require a fair amount of refactoring. is there a way to access the transaction without having to pass it around in function parameters?\\n\\nManual propagation is just the way things need to be done in Go, since it lacks anything like thread-local storage.\\nRather than passing transactions or spans explicitly, I would recommend you use context. These docs need a bit of love, but demonstrate how you can pass context between functions and start spans using it: https://www.elastic.co/guide/en/apm/agent/go/current/instrumenting-source.html#custom-instrumentation-propagation\\nSo rather than\\nfunc doSomething(tx *apm.Transaction) {\\n    span := tx.StartSpan(\"name\", \"type\")\\n    defer span.End()\\n}\\ndo this instead:\\nfunc doSomething(ctx context.Context) {\\n    span, ctx := apm.StartSpan(ctx, \"name\", \"type\")\\n    defer span.End()\\n}\\nPassing context around may have additional benefits, such as being able to propagate other request-scoped data, and request cancellation.\\nIf you specifically want to extract the current transaction from a context object, you can use apm.TransactionFromContext.\\n\\nAnd another question, if I implement passing transaction as function parameters, is there a correct way to fake a transaction in testing that don\\'t send anything to the server?\\n\\nThis might be moot if you pass context around instead, since apm.StartSpan will return a no-op span if the context doesn\\'t contain a transaction. Thus if you pass context.Background() into your functions, no spans will be generated.\\nIf you want to test your instrumentation, you could initialise a Tracer a fake transport. The agent has a testing package, go.elastic.co/apm/apmtest, with various things that you could use, such as apmtest.NewRecordingTracer, which can be used for recording events in memory.'},\n",
       " {'text_field': 'Hi all,\\nI\\'ve been migrating order &amp; order items one-to-many relation from MySQL into ES using Logstash.\\nWith the help of filter aggregation for my joined SQL, the items become a nested property under each order and they look like this:\\n\"order_metadata\": {\\n      \"some_data_foo\": \"123QUX\",\\n      \"some_data_bar\": \"4a8d77550086253a0ac3f1\",\\n      \"some_other_data_baz\": \"Some Words\",\\n      \"some_other_data_foo\": \"9876\"\\n},\\n\"some_order_field\": \"Foo Bar Baz Qux 123\"\\n\"items\": [\\n    {\\n        \"category\": \"bar\",\\n        \"some_json_structure\": \"{\\\\\"size\\\\\":\\\\\"XL\\\\\",\\\\\"color\\\\\":\\\\\"red\\\\\",\\\\\"depth\\\\\":0.2,\\\\\"height\\\\\":0.6,\\\\\"unit\\\\\":\\\\\"M\\\\\"}\",\\n        \"code\": \"XYZ002-T004\",\\n        \"metadata\": \"{\\\\\"location\\\\\":\\\\\"FooBar\\\\\"}\",\\n        \"quantity\": 1,\\n        \"price\": 5.0,\\n        \"name\": \"Sample Item\",\\n        \"id\": 23966914,\\n        \"weight\": 0.2\\n    },\\n    {\\n        \"category\": \"foo\",\\n        \"some_json_structure\": \"[]\",\\n        \"code\": \"XYZ005-T012\",\\n        \"metadata\": \"{\\\\\"location\\\\\":\\\\\"FooBar Baz\\\\\"}\",\\n        \"quantity\": 2,\\n        \"price\": 10.0,\\n        \"name\": \"Sample Item 2\",\\n        \"id\": 25966999,\\n        \"weight\": 0.5\\n    }\\n],\\n\\nOriginally the order_metadata field (which is never a nested structure but is made of key: value lists), comes as stringified JSON and using json { source =&gt; \"order_metadata\" } in filter {} I can easily parse this into an object field.\\nHowever I cannot seem to parse the stringified JSON fields that are found within the array of objects using json { source =&gt; \\'items.property_name\\' }, such as items.some_json_structure and items.metadata seen in the above structure.\\nTo note, items.some_json_structure that comes in is either [] or an object like in the first item, while items.metadata that comes in is either an empty string or an object.\\n\\nHow can I parse the stringified JSON values of the fields of an object in the array of objects?\\nI noticed that doing must match queries that look up an item that has name = Sample AND code = XYZ take a lot more time to query than top level object properties. Is there any advantage that can be gained by somehow making each item a top level property of the order, mapping them as objects?\\n\\nThanks for your time!'},\n",
       " {'text_field': 'You would pass [items][0][some_json_structure] to a json filter. If you have a variable number of them you would have to iterate over them in a ruby filter and parse them there.'},\n",
       " {'text_field': 'I am trying to write a watcher. I\\'ve tested the search expression on the console, and it appears to work. When I use \"Simulate\" within Kibana, it says that the trigger should fire. However, it isn\\'t firing - the UI shows it as not having been triggered.\\nI have seen the same behavior in ES / Kibana 7.1.1 and 7.4.0\\nThe specific watcher is trying to alert if the average idle CPU on our kubernetes cluster has been below a threshold for the last 15 minutes. To try to test the watcher, I\\'ve made the threshold 90% (0.9) - production would be much lower. So this should fire if system.cpu.idle.norm.pct averages to &lt; 0.9 for the last 15 minutes, grouped by host.name\\nWatcher code:\\n{\\n  \"trigger\": {\\n    \"schedule\": {\\n      \"interval\": \"1m\"\\n    }\\n  },\\n  \"input\": {\\n    \"search\": {\\n      \"request\": {\\n        \"search_type\": \"query_then_fetch\",\\n        \"indices\": [\\n          \"metricbeat-*\"\\n        ],\\n        \"rest_total_hits_as_int\": true,\\n        \"body\": {\\n          \"query\": {\\n            \"bool\": {\\n              \"must\": [\\n                {\\n                  \"range\": {\\n                    \"@timestamp\": {\\n                      \"gte\": \"now-15m\"\\n                    }\\n                  }\\n                },\\n                {\\n                  \"term\": {\\n                    \"fields.cluster_name\": \"review\"\\n                  }\\n                }\\n              ]\\n            }\\n          },\\n          \"aggs\": {\\n            \"per_host\": {\\n              \"terms\": {\\n                \"field\": \"host.name\",\\n                \"size\": 30\\n              },\\n              \"aggs\": {\\n                \"avg_cpu_idle\": {\\n                  \"avg\": {\\n                    \"field\": \"system.cpu.idle.norm.pct\"\\n                  }\\n                },\\n                \"cpu_in_use\": {\\n                  \"bucket_script\": {\\n                    \"buckets_path\": {\\n                      \"avg_cpu_idle\": \"avg_cpu_idle\"\\n                    },\\n                    \"script\": \"Math.round( (1 - params.avg_cpu_idle) * 1000) / 10.0\"\\n                  }\\n                },\\n                \"filtered\": {\\n                  \"bucket_selector\": {\\n                    \"buckets_path\": {\\n                      \"idle\": \"avg_cpu_idle\"\\n                    },\\n                    \"script\": \"params.idle &lt; 0.9\"\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"condition\": {\\n    \"array_compare\": {\\n      \"ctx.payload.aggregations.per_host.buckets\": {\\n        \"path\": \"avg_cpu_idle.value\",\\n        \"lte\": {\\n          \"value\": 0.9,\\n          \"quantifier\": \"some\"\\n        }\\n      }\\n    }\\n  },\\n  \"actions\": {\\n    \"send_email\": {\\n      \"email\": {\\n        \"profile\": \"standard\",\\n        \"to\": [\\n          \"my.email@example.com\"\\n        ],\\n        \"subject\": \"Review Apps: High CPU usage\",\\n        \"body\": {\\n          \"text\": \"Environment Review Apps High CPU usage: The following nodes have high CPU over the past 15 minutes: {{#ctx.payload.aggregations.per_host.buckets}}\\\\n\\\\n{{key}}: {{cpu_in_use.value}}%{{/ctx.payload.aggregations.per_host.buckets}}\"\\n        }\\n      }\\n    }\\n  },\\n  \"throttle_period_in_millis\": 21600000\\n}\\n\\nThings that might be related:\\n\\nI am using the standard metricbeat kubernetes setup as on https://www.elastic.co/guide/en/beats/metricbeat/current/running-on-kubernetes.html - this exports data to an index that is used for multiple days, and only rolls over on a data limit - so today\\'s data (2019-10-9) is still going into index metricbeat-7.3.2-2019.09.30-000001. I think ES uses some optimization to skip indexes that don\\'t relate to the correct date - could that be the problem?\\n'},\n",
       " {'text_field': \"Apologies - my mistake entirely.\\nWhen I was simulating the watcher, it was using the logged in user. However I was creating the watcher via the api (using a script) with a different user that didn't have access to the appropriate indexes. Giving the watcher-creation user access to the metricbeat indexes solves the problem.\"},\n",
       " {'text_field': \"Hi!\\nI have four nodes cluster from the two ones are data and master nodes, one dedicated master node and one coordinating node. I collect logs from windows and linux hosts with winlogbeat and filebeat which are sent to logstash at coordinatiing node.\\nRecently I updated cluster to version 7.4 and after this I have the big problem. Linux logs sent from coordinating node let's name it as DB05 still have field agent.version 7.3.2. For the begining I tried to reinstall filebeat on that host, nothing helped. Then I erased completely all files of filebeat and installed again, no result again. I switched off DB05 but logs were uploading to index. So I find out that the source of that logs in index wasn't DB05 despite field agent.hostname was DB05. I switched off all elastic nodes besides one. And there was new logs. I restricted connections to nodes with firewall only between nodes but I still get these fantom logs.\\nThe source for these logs are /var/log/messages and /var/log/secure. I tried to look through different servers but no one server has such a logs.\\nFinally I got stuck because only one node gets logs somewhere but there is no any source except itself. And this node doesn't have this info which is uploaded to index.\\nAny help appreciated....\"},\n",
       " {'text_field': \"I solved the problem.\\nWho or what was sending fake logs I couldn't find out. I just deleted hole index with these logs and that was all.\"},\n",
       " {'text_field': 'I have a requirement in which i need to update the indexed data.  Based on the value from the recent document , i need to update the current document.\\nEx: similar to a counter. Can someone pour in some suggestions please? I am running out of ideas '},\n",
       " {'text_field': 'I created a scripted field which will return a numeric value 1, if Operation is LOGIN and -1 on logout. Created a vertical bar chart visualisation with sum of the scripted field in X-axis and time in yaxis. That resolved my issue. Thanks to all for the time and answers.\\nif(doc[\\'Operation\\'].value==\"LOGIN\"){ return 1; } if (doc[\\'Operation\\'].value==\"LOGOUT\"){ return -1; } else{ return 0; }'},\n",
       " {'text_field': 'I am migrating mysql data to elasticsearch via logstash. The database is too big and have lots of relations. I already imported the simple tables and table with nested data(single level parent/child) as well.\\nProblem : I have user, post and comments table. I want all three table into one type as follow.\\n&gt; {\\n&gt; \"user_id\"   : 200,\\n&gt; \"user_name\" : \"john doe\",\\n&gt; \"posts\" : [\\n&gt;     {\\n&gt;         post_id         : 1,\\n&gt;         post_title      : \"post description\",\\n&gt;         \"comments\"  : [\\n&gt;             {\\n&gt;                 \"comment_id\" : 1\\n&gt;                 \"comment_text\"  : \"good post\"\\n&gt;             },\\n&gt;             {   \"comment_id\" : 2\\n&gt;                 \"comment_text\"  : \"awsome post\"\\n&gt;             }\\n&gt;         ]\\n&gt;     },\\n&gt;     {\\n&gt;         post_id         : 2,\\n&gt;         post_title      : \"post description\",\\n&gt;         \"comments\"  : [\\n&gt;             {\\n&gt;                 \"comment_id\" : 3\\n&gt;                 \"comment_text\"  : \"good post\"\\n&gt;             },\\n&gt;             {   \"comment_id\" : 4\\n&gt;                 \"comment_text\"  : \"awsome post\"\\n&gt;             }\\n&gt;         ]\\n&gt;     }\\n&gt; ]\\n&gt; }\\n\\nI also want it to be 3 level nested object. if a user upload a pic in comments then that media is a nested object under comments.'},\n",
       " {'text_field': 'I have done this by spending two days. here is my config file.\\n\\ninput {\\njdbc{\\njdbc_validate_connection =&gt; true\\njdbc_connection_string =&gt; \"jdbc:mysql://172.17.0.2:3306/dbname\"\\njdbc_user =&gt; \"username\"\\njdbc_password =&gt; \"password\"\\njdbc_driver_library =&gt; \"/home/ilsa/mysql-connector-java-5.1.36-bin.jar\"\\njdbc_driver_class =&gt; \"com.mysql.jdbc.Driver\"\\nstatement =&gt; \"your query that contain joins \"\\n}\\n}\\nfilter {\\naggregate {\\ntask_id =&gt; \"%{user_id}\"\\ncode =&gt; \"\\nmap[\\'user_id\\'] = event.get(\\'user_id\\')\\nmap[\\'email\\'] = event.get(\\'email\\')\\nmap[\\'username\\'] = event.get(\\'username\\')\\nmap[\\'posts\\'] ||= \\nmap[\\'posts\\'] &lt;&lt; {\\n\\'post_id\\' =&gt; event.get(\\'post_id\\'),\\n\\'content\\' =&gt; event.get(\\'content\\'),\\n\\'comments\\' =&gt;  &lt;&lt; {\\n\\'comment_id\\' =&gt; event.get(\\'comment_id\\'),\\n\\'comment_post_id\\' =&gt; event.get(\\'comment_post_id\\'),\\n\\'comment_text\\' =&gt; event.get(\\'comment_text\\')\\n}\\n}\\nevent.cancel()\"\\npush_previous_map_as_event =&gt; true\\ntimeout =&gt; 30\\n}\\n}\\noutput {\\nstdout{ codec =&gt; rubydebug }\\nelasticsearch{\\naction =&gt; \"index\"\\nindex =&gt; \"index_name\"\\ndocument_type =&gt; \"_doc\"\\ndocument_id =&gt; \"%{user_id}\"\\nhosts =&gt; \"localhost:9200\"\\n}\\n}\\n'},\n",
       " {'text_field': \"1st: fairly new to ElasticStack configuration\\nI've looked at the other topics on High CPU, but I do not believe they are helping me\\nI have an Ubuntu box with 8 cores and 8 G of Ram\\nThis machine is running Logstash, Elastic Search and Kibana (and Grafana)\\nLogstash is indexing 8 pipelines, each ingesting a separate CSV file\\nYesterday I changed the writers to the CSVs from updating every 5 min to update every 15 min\\nDid not seem to make any difference.\\nI have sincedb_path set to /dev/null and start_position is set to 'beginning'\\nMy 1 m and 5 m load averages are at 93%\\nTop shows:\\ntop - 13:17:33 up 21:49,  1 user,  load average: 7.36, 7.45, 7.59\\nTasks: 217 total,   1 running, 117 sleeping,   0 stopped,   0 zombie\\n%Cpu(s):  0.3 us,  0.1 sy,  0.5 ni, 99.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\\nKiB Mem :  8144444 total,  1681432 free,  4149572 used,  2313440 buff/cache\\nKiB Swap:  4194300 total,  4192508 free,     1792 used.  3700760 avail Mem\\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\\n3835 logstash  39  19 7287796 1.596g  27932 S   4.7 20.5  59:19.61 java\\n1006 elastic+  20   0 9243596 1.936g 295228 S   2.1 24.9  63:21.69 java\\n6059 elkadmin  20   0   42924   4080   3340 R   0.5  0.1   0:00.04 top\\nand Hot Threads is showing:\\n::: {nj6v-elasticsearch}{OdDLDn1FT8au-x541oLPDg}{k2YYnOUKRt28TqFTrZj3GA}{localhost}{127.0.0.1:9300}{dim}{ml.machine_memory=8339910656, xpack.installed=true, ml.max_open_jobs=20}\\nHot threads at 2019-10-09T13:22:29.007Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:\\nWhich I think means no hot threads? Or I executed the command wrong (GET /_nodes/hot_threads in the Kibana Console)\\nI had started with 4 Cores and when the CPU went high I added 4 more and it just filled up again\\nI did read something about the reload changing from sec to mSec. Is there a change I need to make to a config file somewhere\\nAnyone have an idea what's going on here?\\nThanks!\\nMike\"},\n",
       " {'text_field': 'You do not have high CPU usage. Your CPU is 99.2% idle.\\nYou have high load averages, which is not the same thing. (This provides some interesting history around this.)\\nA process contributes to load when it is running on a CPU, but it can also be counted as part of the load when it is waiting in state TASK_UNINTERRUPTIBLE. That can include waiting on disk IO or mutexes (and many other things).\\nHow confident are you that logstash is causing the high load average? Does it drop when logstash is stopped? If it is logstash then I would start looking at stack traces for each thread in the JVM and try to figure out what it could be waiting on.'},\n",
       " {'text_field': 'The following is not a part of aws elasticsearch managed service.\\nI have ES6.8 cluster on m4.2xlarge (32GB RAM) centos7 machines on aws.\\nGET _cat/nodes?v&amp;s=name\\nip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\\n10.1x.x.x31           64          99  10    1.32    1.25     1.24 di        -      data-1\\n10.1x.x.x6            30          99   6    0.72    0.80     0.84 di        -      data-2\\n10.1x.x.x34           68          99  36    1.12    1.08     1.18 di        -      data-3\\n10.1x.x.x03           49          99  17    1.36    1.40     1.40 di        -      data-4\\n10.1x.x.x33           44          99  49    1.54    1.68     1.67 di        -      data-5\\n10.1x.x.x10           44          99  13    1.26    1.45     1.57 di        -      data-6\\n10.1x.x.x8            32          99   8    1.39    1.17     1.17 di        -      data-7\\n10.1x.x.x7            43          71   2    0.42    0.31     0.26 mi        *      master-3\\n\\nGET _cat/allocation?v&amp;s=node\\nshards disk.indices disk.used disk.avail disk.total disk.percent host        ip          node\\n347       58.5gb    65.5gb    958.4gb   1023.9gb            6 10.1x.x.x31 10.1x.x.x31 data-1\\n240       42.3gb    48.1gb    975.8gb   1023.9gb            4 10.1x.x.x6  10.1x.x.x6  data-2\\n304         55gb    61.6gb    962.2gb   1023.9gb            6 10.1x.x.x34 10.1x.x.x34 data-3\\n382         57gb    64.4gb    959.5gb   1023.9gb            6 10.1x.x.x03 10.1x.x.x03 data-4\\n391       60.1gb    67.1gb    956.8gb   1023.9gb            6 10.1x.x.x33 10.1x.x.x33 data-5\\n391       55.8gb    63.1gb    960.8gb   1023.9gb            6 10.1x.x.x10 10.1x.x.x10 data-6\\n287       49.3gb    59.4gb    964.5gb   1023.9gb            5 10.1x.x.x8  10.1x.x.x8  data-7\\n\\nwe do bulk inserts/bulk updates on it every morning and searching during the day.\\nit\\'s pretty stable. I set it up 4 months ago using official ansible role. So only elasticsearch and datadog agent is installed there on top of default centos7 image.\\nnode.ml: false\\nbootstrap.memory_lock: true\\nthread_pool.write.queue_size: 1000\\nthread_pool.search.queue_size: 10000\\nindices.queries.cache.size: 5%\\nes_heap_size: 20g &lt;-- !!! m4.2xlarge has 32GB RAM\\n\\nNo issues with ES6.8 at all, no timeouts, no downtime since the cluster started.\\nAs a part of migration to ES7 I started a new cluster with the same ansible role, same configs, same hardware.\\neverything is the same.\\nInitial load is usually pretty high but no issues happened as expected.\\nThen the second process that sort of merges some documents using aggregation queries. Got an error. The culprit was the size of buckets is set to 2000000 (different topic why it\\'s set to this number). Though only a small portion returned back due to bucket_selector pipeline agg.\\nExample with nested query which is a bit more complex than aggs without nested we have:\\nGET index_id1/_search\\n{\\n  \"size\": 0,\\n  \"aggs\": {\\n    \"byNested\": {\\n      \"nested\": {\\n        \"path\": \"nestedObjects\"\\n      },\\n      \"aggs\": {\\n        \"sameIds\": {\\n          \"terms\": {\\n            \"script\": {\\n              \"lang\": \"painless\",\\n              \"source\": \"return doc[\\'nestedObjects.id\\'].value\"\\n            },\\n            \"size\": 2000000  &lt;-- too big\\n          },\\n          \"aggs\": {\\n            \"byId\": {\\n              \"reverse_nested\": {}\\n            },\\n            \"byId_bucket_filter\": {\\n              \"bucket_selector\": {\\n                \"buckets_path\": {\\n                  \"totalCount\": \"byId._count\"\\n                },\\n                \"script\": {\\n                  \"source\": \"params.totalCount &gt; 1\"\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nBefore adjusting the client and switching to composite aggregation I wanted to let it finish with the test next day and changed \"search.max_buckets\" to 2000000. The rest is the same as on ES6.8\\nNext day I found out that although the test es7 cluster hadn\\'t been used at all, on 4 nodes elasticsearch service died. checked RAM and looked like this. jvm ate up everything:\\n\\n1.png1148×501 18.9 KB\\n\\nRestarted the service on all nodes with es_heap_size 50% of ram = 16g and continued the process with aggregations. Got this:\\ntype: circuit_breaking_exception Reason: \"[parent] Data too large, data for [&lt;http_request&gt;] would be [16518949320/15.3gb], which is larger than the limit of [16254631936/15.1gb], real usage: [16518948880/15.3gb], new bytes reserved: [440/440b], usages [request=16440/16kb, fielddata=317/317b, in_flight_requests=154603580/147.4mb, accounting=540721543/515.6mb]\" \\n\\nOk, found out that between ES7.4 and 6.8 the difference is that the parent breaker takes real memory usage into account. Set it to indices.breaker.total.use_real_memory: false as on ES6.8 there is no such thing.\\nRan the process again - no more issues.\\nSo the only difference in terms of cluster config is that ES6.8 es_heap_size = 20g (or 63%) vs ES7.4 es_heap_size = 16g ( or 50%).\\nNext 10 hrs nobody touched the cluster though i see this pattern:\\nMemory graph for ES7 for the past 10hrs NOT being used:\\n\\n2.png1560×465 20.9 KB\\n\\nMemory graph for ES6 for the whole week being used:\\n\\n3.png1575×540 21.3 KB\\n\\nI understand the issue with the aggregation and as I mentioned will switch to composite to get buckets - different topic.\\nMy question is why ES7 RAM consumption pattern is so different than on ES6.8 (same configs/envs/hardware)? I see these zig-zags on ES7 but it\\'s growing. After every clean up consumed memory is higher then it was after previous cleanup. Should I expect it to consume the RAM again and die? Maybe there is anything to track what is exactly causing it grow while not used?\\nIs there anything else I could adjust before diving into aggregations update? Maybe GC settings or..?\\nAlso why switching off real memory usage for the parent breaker did the trick?'},\n",
       " {'text_field': \"Thanks for posting those jvm.options @andrx.  The fact that you're missing\\n-Dio.netty.allocator.numDirectArenas=0 likely explains a good part if not all of your memory issues.\\n\\nI see changes for G1GC there (using bundled jdk). In your opinion worth to try it out?\\n\\nYes I think so but I wouldn't spend too much time one it either. Eventually G1GC will become the new standard GC so moving to it has its advantages in terms of long term maintainability. Also, in some cases (particularly when using large heap sizes) G1GC has shown to improve performance though that might not apply here with 16G heap size.\\n\\nI remember other posts mentioning data loss issues.\\n\\nThere were issues with G1GC and some older Java versions but nowadays all should be fine here. We also do have bootstrap checks in place that will prevent ES from starting in case you're running one of these older versions (at this point, those are really old so that's unlikely) and G1GC.\"},\n",
       " {'text_field': 'I have apache logs like that:\\n188.122.20.100 [09/Oct/2019:14:00:02 +0200] \"POST /something/template/ HTTP/1.1\" 200 286 \"url.html\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 13_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 [FBAN/FBIOS;FBDV/iPhone10,4;FBMD/iPhone;FBSN/iOS;FBSV/13.0;FBSS/2;FBID/phone;FBLC/pl_PL;FBOP/5;FBCR/Orange]\" urlagain\\nAnd I created GROK pattern which in grok debugger fits perfect:\\n%{IP:ip}\\\\s\\\\[%{HTTPDATE:timestamp}\\\\]\\\\s\\\\\"(?:%{WORD:metoda}\\\\s%{NOTSPACE:request}\\\\s(?:HTTP/%{NUMBER:httpversion}))\"\\\\s%{NUMBER:response}\\\\s(?:%{NUMBER:bytes})\\\\s(\\\\\"%{GREEDYDATA:URL}\\\\\")?\\\\s\\\\\"(%{GREEDYDATA:system})?\\\\\"\\\\s%{GREEDYDATA:referer}\\nBut in implementation in configuration logstash.conf it did not work in elastic.\\nConf file:\\ninput {\\n  beats {\\n    port =&gt; 5044\\n  }\\n}\\n\\nfilter {\\n\\tgrok {\\n         match =&gt; [ \"message\", \"%{IP:ip}\\\\s\\\\[%{HTTPDATE:timestamp}\\\\]\\\\s\\\\\"(?:%{WORD:metoda}\\\\s%{NOTSPACE:request}\\\\s(?:HTTP/%{NUMBER:httpversion}))\"\\\\s%{NUMBER:response}\\\\s(?:%{NUMBER:bytes})\\\\s(\\\\\"%{GREEDYDATA:URL}\\\\\")?\\\\s\\\\\"(%{GREEDYDATA:system})?\\\\\"\\\\s%{GREEDYDATA:referer}\" \\n         }\\n}\\n\\noutput {\\n    elasticsearch {\\n        hosts =&gt; [\"elasticsearch:9200\"]\\n\\t\\tindex =&gt; \"apache-%{+yyyy.MM.dd}\"\\n    }'},\n",
       " {'text_field': 'You need to escape the double quotes after httpversion and you need to add ] after the pattern.'},\n",
       " {'text_field': \"Hello,\\nWe've got a webpage in which you can search for data in our elastic database. According to the result of that search we edit a viz to show the results.\\nCurrently this is done by creating an index with the search results which is used in a kibana viz.\\nIt works perfectly when there is only one user but when there are multiple users at the same time they edit each other viz because their search result is stored in the same index.\\nI would like each client to have an independent visualization.\\nHow can this be done ?\\nWe thought about creating an index per session but it seems less than ideal.\"},\n",
       " {'text_field': 'I found a solution : I embeded a snaphsot of the viz in the wabpage instead of the object and edited the url of the iframe to add the same query used in the search function of the webpage. This allows each client to have a different visualization without even creating indexes.'},\n",
       " {'text_field': 'Hi,\\nI am trying to use coordinate maps in kibana for my visualization.\\nI  mutate my longitude and latitude fields with rename and convert to float, but when i tried in Kibana, it showing no compactible field geo_point.\\nLogstash:\\ninput {\\nfile\\n{\\npath =&gt; \"G:/Transit/*.csv\"\\nstart_position =&gt; \"beginning\"\\nsincedb_path =&gt; \"F:\\\\ELKstack\\\\logstash\\\\data\\\\plugins\\\\inputs\\\\file\\\\transit.txt\"\\n}\\n}\\nfilter {\\ncsv {\\nseparator =&gt; \",\"\\ncolumns =&gt; [\"Unit#\",\"Unit Name\",\"Add1\",\"City\",\"Pr\",\"Pcode\",\"Country\",\"Tel1\",\"Access\",\"W_GMT\",\"S_GMT\",\"LATITUDE\",\"LONGTITUDE\",\"NAME_FRN\",\"ADDR_1_FRN\",\"Hrs_Mon_Op\",\"Hrs_Mon_Cls\",\"Hrs_Tue_Op\",\"Hrs_Tue_Cls\",\"Hrs_Wed_Op\",\"Hrs_Wed_Cls\",\"Hrs_Thu_Op\",\"Hrs_Thu_Cls\",\"Hrs_Fri_Op\",\"Hrs_Fri_Cls\",\"Hrs_Sat_Op\",\"Hrs_Sat_Cls\",\"Hrs_Sun_Op\",\"Hrs_Sun_Cls\",\"LANG_SERVICES\",\"Close Date\",\"Forced Relo?\",\"FY 2019\",\"F24\"]\\n}\\nmutate { convert =&gt; { \"LONGTITUDE\" =&gt; \"float\"}}\\nmutate { convert =&gt; { \"LATITUDE\" =&gt; \"float\"}}\\nmutate {rename =&gt; [\"LONGTITUDE\", \"Location(lon)\"]}\\nmutate {rename =&gt; [\"LATITUDE\", \"Location(lat)\"]}\\nmutate {rename =&gt; [\"Unit#\", \"TransitId\"]}\\nmutate {rename =&gt; [\"City\", \"City\"]}\\nmutate {rename =&gt; [\"Add1\", \"Address\"]}\\nmutate {rename =&gt; [\"Pr\", \"Province\"]}\\nmutate {rename =&gt; [\"Pcode\", \"Postal Code\"]}\\n#mutate {rename =&gt; [\"my_Region\", \"Region\"]}\\nmutate {remove_field =&gt; [\"Unit Name\",\"Country\",\"Tel1\",\"Access\",\"W_GMT\",\"S_GMT\",\"LATITUDE\",\"LONGTITUDE\",\"NAME_FRN\",\"ADDR_1_FRN\",\"Hrs_Mon_Op\",\"Hrs_Mon_Cls\",\"Hrs_Tue_Op\",\"Hrs_Tue_Cls\",\"Hrs_Wed_Op\",\"Hrs_Wed_Cls\",\"Hrs_Thu_Op\",\"Hrs_Thu_Cls\",\"Hrs_Fri_Op\",\"Hrs_Fri_Cls\",\"Hrs_Sat_Op\",\"Hrs_Sat_Cls\",\"Hrs_Sun_Op\",\"Hrs_Sun_Cls\",\"LANG_SERVICES\",\"Close Date\",\"Forced Relo?\",\"FY 2019\",\"F24\",\"my_Region\"]}\\n}\\noutput {\\nstdout { codec =&gt; rubydebug }\\nelasticsearch {\\naction =&gt; \"index\"\\nhosts =&gt; \"https://127.0.0.1:9200\"\\nindex =&gt; \"branchres-transit\"\\ncacert =&gt; \"/ELKstack/logstash/config/certs/ca/ca.crt\"\\nuser =&gt; \"logstash_writer\"\\npassword =&gt; \"password\"\\nmanage_template =&gt; false\\n}\\n}\\n\\nkibana.jpg1062×802 91.8 KB\\n'},\n",
       " {'text_field': 'Your index template should include something like\\n\"mappings\": {\\n  \"properties\": {\\n    \"location\": {\\n      \"type\": \"geo_point\"\\n    }\\n  }\\n}\\n\\nI suspect it doesn\\'t.'},\n",
       " {'text_field': 'Hi,\\nAfter upgrade from 7.3 to 7.4 APM-Servers are failing with common error\\nFailed to publish events: temporary bulk send failure and Queue is full\\nthis is a very busy system and we have 6 APM servers and 5 ELK servers\\nDuring peak times we have around 1000-1500/s Total Events Rate per APM server\\nserver config is: 8CPU, 40GB Ram, fast SSD/NVMe storage\\ni went tonz of time through this \"common problem\" page:\\nhttps://www.elastic.co/guide/en/apm/server/master/common-problems.html#queue-full\\nplease let me know if i am doing something wrong and/or if i can do something to fix this Queue issue on ELK and/or APM server\\ni was playing with bulk_max_size and workers and queue.mem.events and at the end it always fails\\ni dont have any problem with logstash servers which are also heavily used\\nMany thanks\\nTomislav\\nKibana version: 7.4\\nElasticsearch version: 7.4\\nAPM Server version: 7.4\\nAPM Agent language and version: elasticapm-java/1.10.0\\nFresh install or upgraded from other version? Upgraded from 7.3\\nOct  9 18:16:30 elksrv01 apm-server: 2019-10-09T18:16:30.709+0200#011ERROR#011pipeline/output.go:121#011Failed to publish events: temporary bulk send failure\\nOct  9 18:16:30 elksrv01 apm-server: 2019-10-09T18:16:30.762+0200#011ERROR#011[request]#011middleware/log_middleware.go:74#011queue is full#011{\"request_id\": \"54c3149c-15c8-4737-851c-cddbb1876799\", \"method\": \"POST\", \"URL\": \"/intake/v2/events\", \"content_length\": 2955, \"remote_address\": \"10.10.20.22\", \"user-agent\": \"elasticapm-java/1.10.0\", \"response_code\": 503, \"error\": \"queue is full\"}\\nOct  9 18:16:30 elksrv01 apm-server: 2019-10-09T18:16:30.813+0200#011ERROR#011[request]#011middleware/log_middleware.go:74#011queue is full#011{\"request_id\": \"ed1882eb-0ea1-4b8d-985f-6db1bc3e02e7\", \"method\": \"POST\", \"URL\": \"/intake/v2/events\", \"content_length\": 36128, \"remote_address\": \"10.10.30.23\", \"user-agent\": \"elasticapm-java/1.10.0\", \"response_code\": 503, \"error\": \"queue is full\"}\\nOct  9 18:16:30 elksrv01 apm-server: 2019-10-09T18:16:30.891+0200#011ERROR#011pipeline/output.go:121#011Failed to publish events: temporary bulk send failure\\nOct  9 18:16:30 elksrv01 apm-server: 2019-10-09T18:16:30.922+0200#011ERROR#011pipeline/output.go:121#011Failed to publish events: temporary bulk send failure\\nOct  9 18:16:31 elksrv01 apm-server: 2019-10-09T18:16:31.424+0200#011ERROR#011[request]#011middleware/log_middleware.go:74#011queue is full#011{\"request_id\": \"dc8715ad-d144-43fb-aee3-21f39f4d4872\", \"method\": \"POST\", \"URL\": \"/intake/v2/events\", \"content_length\": -1, \"remote_address\": \"10.10.20.27\", \"user-agent\": \"java-agent/1.6.1\", \"response_code\": 503, \"error\": \"queue is full\"}\\nconfig file:\\napm-server:\\nhost: \"10.10.10.11:8200\"\\nidle_timeout: 60s\\nread_timeout: 45s\\nmax_connections: 0\\noutput.elasticsearch:\\nhosts:\\n- 10.10.10.11:9200\\n- 10.10.10.12:9200\\n- 10.10.10.13:9200\\n- 10.10.10.14:9200\\n- 10.10.10.15:9200\\nbulk_max_size: 10240\\nmax_retries: 12\\nworker: 8\\nusername: \"elastic\"\\npassword: \"XXXXXXXXXXXXXXXXXXXXX\"\\nqueue.mem.events: 81920\\npipeline: \"_none\"\\nlogging.to_syslog: false\\nlogging.level: error\\nlogging.to_files: true\\nlogging.files:\\npath: /var/log/apm-server\\nname: apm-server\\nkeepfiles: 7\\npermissions: 0644\\nxpack.monitoring.enabled: true\\nxpack.monitoring.elasticsearch:\\nOptional protocol and basic auth credentials.\\n#protocol: \"https\"\\nusername: \"apm_system\"\\npassword: \"XXXXXXXXXXXXXXXXXXXX\"'},\n",
       " {'text_field': \"Hi, unfortunately I don't have a date, but it will be released soon.\\nIn case you didn't see it already in the link you posted above, another option is to use a processor to drop that field completely, as my colleague explained here:\\n\\n  \\n    \\n    \\n    APM: 503 Queue is full, server sleeping, nothing helps APM\\n  \\n  \\n    APM does not keep logs at all \\n\\nDo you mean that you have disabled keeping logs? By default the APM Server does write to log files. \\nI assume you might encounter a similar bug to what we have seen in another discuss entry (APM Failed to publish events: temporary bulk send failure / Queue is full 503 error). \\nFrom 7.4 on apm pipelines are enabled by default, and a new field client.ip is indexed. Providing invalid data for fields that are part of the pipelines, can lead to errors and ingestion re…\\n  \\n\\n\\nHope that helps and sorry again.\"},\n",
       " {'text_field': 'Hi, I have a logfile that looks like this:\\n61f3483e-e7f4-4202-b34a-d85108f0fea9 - some log line\\n61f3483e-e7f4-4202-b34a-d85108f0fea9 - some other log line ID=123456\\n61f3483e-e7f4-4202-b34a-d85108f0fea9 - another log line\\n\\nThe first part is an execution ID, then there is the log text and one of the logs has an ID that came from the request. I am trying to get that ID and add it as a value on all the events, including the ones before it.\\nMy filter looks like this (the execution ID is dissected before), it is able to add the value to the events after that one but not the ones before.\\n\\tgrok {\\n\\t\\tmatch =&gt; { \"message\" =&gt; \"ID=(?&lt;id&gt;\\\\d{6})\" }\\n\\t}\\n\\t\\n\\tif \"_grokparsefailure\" in [tags] {\\n\\t\\taggregate {\\n\\t\\t\\ttask_id =&gt; \"%{execid}\"\\n\\t\\t\\tcode =&gt; \"event.set(\\'id\\', map[\\'id\\'])\"\\n\\t\\t\\ttimeout =&gt; 5\\n\\t\\t\\tpush_map_as_event_on_timeout =&gt; true \\n\\t\\t}\\n\\t} else {\\n\\t\\taggregate {\\n\\t\\t\\ttask_id =&gt; \"%{execid}\"\\n\\t\\t\\tcode =&gt; \"map[\\'id\\'] = event.get(\\'id\\')\"\\n\\t\\t}\\n\\t}\\n\\nIf I add a sleep before the second aggregate it kinda works but logstash eventually stalls and stops working correctly due to the high number of events.\\nIs there a way to make aggregate wait until it gets the event with the ID it needs and then run the code for all the previous events?\\nThanks.'},\n",
       " {'text_field': 'Do it in a single filter. Collect all the lines related to the id, then split them when the timeout occurs.\\n    grok { match =&gt; { \"message\" =&gt; \"^%{NOTSPACE:execid} - \" } }\\n    grok { match =&gt; { \"message\" =&gt; \"ID=(?&lt;id&gt;\\\\d{6})\" } }\\n    aggregate {\\n        task_id =&gt; \"%{execid}\"\\n        timeout =&gt; 5\\n        push_map_as_event_on_timeout =&gt; true\\n        code =&gt; \\'\\n            map[\"message\"] ||= []\\n            map[\"message\"] &lt;&lt; event.get(\"message\")\\n            id = event.get(\"id\")\\n            if id\\n                map[\"id\"] = id\\n            end\\n            event.cancel\\n        \\'\\n    }\\n    split { field =&gt; \"message\" }'},\n",
       " {'text_field': \"I'm trying to run metric beats without having a configuration file.  I was wondering how do you enable modules?\\nI tried\\n-E modules.docker.enable=true\\n-E module.docker.enable=true\\n-E metricbeat.modules=[module.docker.enable=true]\\nBut does not appear to work.\"},\n",
       " {'text_field': 'Here is the solution I have used in my project that sets up an ELK stack with zero custom images or external configuration files.\\n\\n  \\n      github.com\\n  \\n  \\n    trajano/elk-swarm/blob/master/elk.yml#L50-L60\\n\\ncommand:\\n  - -E\\n  - |\\n    metricbeat.modules=[\\n      {\\n        module:docker,\\n        hosts:[unix:///var/run/docker.sock],\\n        period:10s,\\n        enabled:true\\n      }\\n    ] \\n\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nThe whole project is in https://github.com/trajano/elk-swarm/'},\n",
       " {'text_field': 'Hi,\\nI\\'m transitioning to logstash from an rsyslog config that ingests logs from a list of network devices (routers, switches, firewalls), then does something based on the source IP.  Here\\'s an abbreviated example:\\n\\nif ($fromhost-ip == \\'10.250.0.73\\' or $fromhost-ip == \\'10.253.1.161\\' ) then @10.250.0.208\\n\\nThe way I was thinking about implementing this in logstash is adding a tag if the source IP matched.  Would this be a similar config?\\nfilter {\\n  if [source] == \\'10.250.0.73\\' or [source] == \\'10.253.1.161\\' {\\n    mutate {\\n      add_tag =&gt; \"network_device\"\\n    }\\n  }\\n}\\n\\noutput {\\n  if \"network_device\" in [tags] {\\n    udp {\\n      host =&gt; \"10.250.0.208\"\\n      port =&gt; 514\\n    }\\n  }\\n}\\n\\nOr is there a better way to do this?  Can you do a regex =~ \"(10.250.0.73|10.253.1.161)\" or anything similar?\\nI\\'m testing looking for that specific tag and writing it out to a file but then also have a catch-all if nothing else matches and it\\'s hitting that catch-all so I\\'m apparently not doing something right.\\nThere are a lot of other log source types coming in so I didn\\'t know if applying tags determined by source IP, message string, etc, is the best methodology (sorry, I\\'m just getting started on learning this) but I\\'d love to learn some tips if possible.\\nThanks!'},\n",
       " {'text_field': 'Sorry, I should have tested it before asking.  But it does look like that\\'s the case.  Not sure if that\\'s the new syntax for 7.4, but [host] does work and [source] does not:\\n{\\n    \"@timestamp\" =&gt; 2019-10-10T20:50:33.916Z,\\n       \"message\" =&gt; \"Test\",\\n          \"host\" =&gt; \"10.190.200.30\",\\n      \"@version\" =&gt; \"1\",\\n          \"tags\" =&gt; [\\n        [0] \"Test\"\\n    ]\\n}\\n\\nThank you so much for your help!'},\n",
       " {'text_field': 'I installed xpack on an elasticsearch instance, and now the output from any curl is:\\n{\\n\"error\" : {\\n\"root_cause\" : [\\n{\\n\"type\" : \"security_exception\",\\n\"reason\" : \"missing authentication token for REST request [/?pretty]\",\\n\"header\" : {\\n\"WWW-Authenticate\" : \"Basic realm=\"security\" charset=\"UTF-8\"\"\\n}\\n}\\n],\\n\"type\" : \"security_exception\",\\n\"reason\" : \"missing authentication token for REST request [/?pretty]\",\\n\"header\" : {\\n\"WWW-Authenticate\" : \"Basic realm=\"security\" charset=\"UTF-8\"\"\\n}\\n},\\n\"status\" : 401\\n}\\nI understand this has to do with xpack security, but xpack.security.enabled is set to false in the elasticsearch.yml file. I have restarted the cluster and the issue persists. Any help would be appreciated.'},\n",
       " {'text_field': 'Wrong config files were being used. Thanks for your reply.'},\n",
       " {'text_field': 'On this page: https://www.elastic.co/what-is/elasticsearch-machine-learning/recipes/dns-data-exfiltration-tunneling\\n..when you click on the example there are directions: https://github.com/elastic/examples/blob/master/Machine%20Learning/Security%20Analytics%20Recipes/dns_data_exfiltration/EXAMPLE.md\\n..but these directions are outdated, I FINALLY figured out that x-pack is automatically installed in recent versions of elasticsearch, BUT I have been having trouble with the obviously dated instructions on this elasticsearch page... Do I need a license to run x-pack to be able to do this DNS Data Exfiltration machine learning? And if so, what license do I need??\\nhttps://www.elastic.co/subscriptions is not helpful.'},\n",
       " {'text_field': 'Hi\\nI have raised this issue regarding the supported versions of the recipes. https://github.com/elastic/examples/issues/264\\n\\nThe X-Pack features are included in the default distribution since 6.3.\\nMachine learning features are available with a trial or platinum license.\\n\\nAt lot has changed since v5.4 as I am sure you are experiencing. Looking at the ML job and datafeed config, I can see some paramaters which have since been deprecated or removed from the example job and datafeed json configs. When creating the ML job I would recommend using the Kibana UI, rather than the scripts provided.\\nIn the UI:\\n\\nCreate Job\\nSelect the packetbeat index (this should have been indexed already, and a kibana index pattern created)\\nSelect Advanced Job Wizard\\nCreate the job with the following config\\n\\nDetector: high_info_content(sub_domain) over domain exclude_frequent=all\\nBucketspan: 5m\\nInfluencer(s): client_ip, beat.name, domain\\n\\nIf you do not already have some familiarity with creating ML jobs, then I would recommend looking at the Single Metric Wizard first, and analyzing a simple event rate count of a data source.\\nBest wishes\\nSophie'},\n",
       " {'text_field': 'Коллеги, привет!\\nПомогите, пожалуйста, с решением.\\nЯ рассчитываю Duration Time между двумя разными Events в эластике.\\nИспользую следующий метод:\\nif \"COMMONROUTER.OUT.REMOTE\" in [grok_reciever] {\\nelasticsearch {\\n    hosts =&gt; [\"data_cluster_elk:9200\"]\\n    ca_file =&gt; \\'tmp/certs/ca.crt\\'\\n    user =&gt; \\'elastic\\'\\n    password =&gt; \\'secret\\'\\n    index =&gt; \"trans_log-*\"\\n    query =&gt; \\'grok_sender:\"COMMONROUTER.IN\" AND grok_procid:%{grok_procid}\\'\\n    fields =&gt; { \"@timestamp\" =&gt; \"timestamp_start\" }\\n    add_tag =&gt; [ok]\\n   }\\n  }\\n\\nНа выходе получаю следующее: 50% закрывающих events  (COMMONROUTER.OUT.REMOTE) имеют доп.поле время старта, но другие 50% evaents строки старт не имеют, при анализе выявил, что IN и OUT \"влетают\" в ELK в одно и тоже время (данных очень много и лаг между IN\\\\OUT может быть менее секунды).\\nВ связи с этим вопрос: Возможно ли проапгрейдить мой конфиг в LS, чтобы на выходе все 100% закрывающих events имели поля старта транзакции?\\nЕсли нет, подскажите, как проделать эту же самую операцию через API ElK. То есть, скажем я раз в 10 секунд обращаюсь к API ELK и ищу закрывающий event, добавляю в него данные о времени из открывающего event и рассчитываю Duration.'},\n",
       " {'text_field': '\\n\\n\\n Lebedev:\\n\\nТеперь главный вопрос, есть ли примеры запросов, как реализовать данную задачу?\\n\\n\\nПолных примеров нет, но можно что-то состряпать из https://stackoverflow.com/questions/38323392/logstash-configuration-how-to-call-the-partial-update-api-from-elastcisearch-ou с action=update и вычислением времени в script.'},\n",
       " {'text_field': 'I am using ElasticSearch 6.5, invoking commands by Kibana 6.5.\\nMy documents have user field which is an array of user structures. I want to construct a script, that will update some of the structures in this array based on the identifier stored in the structure.\\nCode I came to so far:\\nPOST test_idx1/_doc/1/_update\\n{\\n \"script\": \"\"\" \\n for (int i = 0; i &lt; params.src.size(); i++) {\\n        boolean f = false;\\n        for (int j = 0; j &lt; ctx._source.user.size(); j++) {\\n            if (ctx._source.user[j].uid == params.src[i].uid) {\\n                ctx._source.user[j].first = params.src[i].first;\\n                ctx._source.user[j].last = params.src[i].last;\\n                f=true;\\n                break;\\n            }\\n        }\\n    if(!f){ctx._source.user.add(params.src[i]);}\\n    }\"\"\",\\n \"params\": {\\n   \"src\": [\\n     {\\n       \"uid\" : 1,\\n       \"first\": \"John_u\",\\n       \"last\": \"Smith_u\"\\n     },\\n     {\\n       \"uid\" : 2,\\n       \"first\": \"Bob_u\",\\n       \"last\": \"Jones_u\"\\n     },\\n     {\\n       \"uid\" : 4,\\n       \"first\": \"Anna_u\",\\n       \"last\": \"Right_u\"\\n     }\\n   ]\\n }\\n}\\n\\nBut it ends with an error:\\n{\\n  \"error\": {\\n    \"root_cause\": [\\n      {\\n        \"type\": \"remote_transport_exception\",\\n        \"reason\": \"[T-Qm5ly][127.0.0.1:9300][indices:data/write/update[s]]\"\\n      }\\n    ],\\n    \"type\": \"illegal_argument_exception\",\\n    \"reason\": \"failed to execute script\",\\n    \"caused_by\": {\\n      \"type\": \"script_exception\",\\n      \"reason\": \"runtime error\",\\n      \"script_stack\": [\\n        \"i = 0; i &lt; params.src.size(); i++) {\\\\n        boolean \",\\n        \"                     ^---- HERE\"\\n      ],\\n      \"script\": \" for (int i = 0; i &lt; params.src.size(); i++) {\\\\n        boolean f = false;\\\\n        for (int j = 0; j &lt; ctx._source.user.size(); j++) {\\\\n            if (ctx._source.user[j].uid == params.src[i].uid) {\\\\n                ctx._source.user[j].first = params.src[i].first;\\\\n                ctx._source.user[j].last = params.src[i].last;\\\\n                f=true;\\\\n                break;\\\\n            }\\\\n        }\\\\n    if(!f){ctx._source.user.add(params.src[i]);}\\\\n    }\",\\n      \"lang\": \"painless\",\\n      \"caused_by\": {\\n        \"type\": \"null_pointer_exception\",\\n        \"reason\": null\\n      }\\n    }\\n  },\\n  \"status\": 400\\n}\\n\\nSo it seems, like params.src is null, while I am providing values for it in params section.\\nWhat am I doing wrong?'},\n",
       " {'text_field': 'Ok, I detected the issue. Script code and params section have to be nested inside the script section.\\nThis code is working:\\nPOST test_idx1/_doc/1/_update\\n{\\n    \"script\": {\\n        \"source\" : \"\"\" \\n            for (int i = 0; i &lt; params.src.size(); i++) {\\n                boolean f = false;\\n                for (int j = 0; j &lt; ctx._source.user.size(); j++) {\\n                    if (ctx._source.user[j].uid == params.src[i].uid) {\\n                        ctx._source.user[j].first = params.src[i].first;\\n                        ctx._source.user[j].last = params.src[i].last;\\n                f=true;\\n                break;\\n            }\\n            }\\n            if(!f){ctx._source.user.add(params.src[i]);}\\n        }\"\"\",\\n        \"lang\" : \"painless\",\\n        \"params\": {\\n            \"src\": [\\n                {\\n                    \"uid\" : 1,\\n                    \"first\": \"John_u\",\\n                    \"last\": \"Smith_u\"\\n                },\\n                {\\n                    \"uid\" : 2,\\n                    \"first\": \"Bob_u\",\\n                    \"last\": \"Jones_u\"\\n                },\\n                {\\n                    \"uid\" : 4,\\n                    \"first\": \"Anna_u\",\\n                    \"last\": \"Right_u\"\\n                }\\n            ]\\n        }\\n    }\\n}\\n'},\n",
       " {'text_field': 'Version 7.1.1\\nMy mapping setting consists of following:\\n\\n\"geometry\": {\\n\"type\": \"geo_shape\",\\n\"tree\": \"quadtree\",\\n\"precision\": \"8m\"\\n}\\n\\nWhen i check /_segments?verbose=true, i can see my maximum memory_in_bytes (memory occupied on heap) is occupied by geometry field. All other text fields occupy very less heap.\\nI understand that geo_shape comes at cost of more memory and disk space.\\nQuery:\\nDo we have a way wherein we can store this in a compressed format and still be able to query ?'},\n",
       " {'text_field': \"The recursive strategy is based on describing the shape using the grid provided (in your case a quad tree). That means the logic computes all the cells that intersects with the indexed shapes at the given precision and stores that information in the inverted index.\\nEvery cell is described as a prefix path and that goes into the terms dictionary. The higher the precision the more cells you need to describe your shape and the longer those paths will be. This dictionary is loaded into heap so that is the reason you see high heap usage for that field.  Unfortunately the only ways to decrease heap usage would be to index your shapes at a lower precision,  either using precision or distance_err_pct parameters.\\nThe new indexed strategy is based on Lucene's BKD tree. Shapes are vectorised using triangles and stored in the tree as a bounding box plus some extra information that helps reconstructing the original  triangle.  The precision of the shapes is only limited to the encoding used for storing those vectors (1e-7 decimal degree precision).\\nThe result is much faster indexing throughput, smaller index, smaller heap footprint and in most cases faster query throughput. And there is no need to set any extra parameter in order to get your data loaded into ES :).\"},\n",
       " {'text_field': 'I\\'m new in the ELK system building and configuration and have a few challenges.\\nI am unable to add index for parsed data from elastic search index management. I however can see the the parsed data from Kibana index pattern but when l try create the index it takes forever to create the index. Please assist.\\nI am pulling data from a SQL database using the below string in logstash\\ninput {\\njdbc {\\n    jdbc_connection_string =&gt; //HostName\\\\instanceName;database=DBName;user=UserName;password=Password\" \\n    jdbc_user =&gt; User\\n    jdbc_driver_library =&gt; \"C:\\\\Program Files\\\\Microsoft JDBC Driver 6.2 for SQL Server\\\\sqljdbc_6.2\\\\enu\\\\sqljdbc4-2.0.jar\"       \\n    jdbc_driver_class =&gt; \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"        \\n    statement =&gt; \"SELECT TOP 100 * FROM Example_table\"\\n}\\n\\n}\\noutput {\\n#stdout { codec =&gt; json_lines }\\nelasticsearch {\\nhosts =&gt; [\"localhost:9200\"]\\n}\\n}\\nBelow is the response from elastic search indices in the index management stats and health state is continuously yellow.\\n{\\n\"_shards\": {\\n\"total\": 2,\\n\"successful\": 1,\\n\"failed\": 0\\n},\\n\"stats\": {\\n\"uuid\": \"swDiyIk-TyWagC_vrSOOgg\",\\n\"primaries\": {\\n\"docs\": {\\n\"count\": 35320,\\n\"deleted\": 0\\n},\\n\"store\": {\\n\"size_in_bytes\": 10176016\\n},\\n\"indexing\": {\\n\"index_total\": 35320,\\n\"index_time_in_millis\": 15253,\\n\"index_current\": 0,\\n\"index_failed\": 0,\\n\"delete_total\": 0,\\n\"delete_time_in_millis\": 0,\\n\"delete_current\": 0,\\n\"noop_update_total\": 0,\\n\"is_throttled\": false,\\n\"throttle_time_in_millis\": 0\\n},\\nIn Kibana index pattern l can see the indices but when l try create the index it takes forever to create. Kindly assist not sure what could be the issue.\\nSee below for kibana screenshots\\n\\nimage.png1623×593 25.5 KB\\n\\nSystem keeps on saying creating index forever\\n\\nimage.png1715×347 21.6 KB\\n'},\n",
       " {'text_field': \"Can you check both the Kibana and Elasticsearch logs when you're attempting to create that index pattern?\"},\n",
       " {'text_field': 'Hi All,\\nPlease advise we have been asked to use filebeats as a container to monitor application that write the logs to the host itself is that logic, as I understand filebeat as a container is used to monitor other running containers.\\nanother question can we on the same host use filebeat as application to monitor the host logs and the running containers too?\\nThanks &amp; BRs,\\nMai'},\n",
       " {'text_field': 'Hi @Mai_Waly.\\nThis is perfectly possible. The only thing you will need to do is mounting the log files from the host that you want to ship, then point Filebeat to them.\\nSome more info about running Filebeat on Docker: https://www.elastic.co/guide/en/beats/filebeat/current/running-on-docker.html\\nBest regards'},\n",
       " {'text_field': \"Kibana version: 7.4\\nElasticsearch version: 7.4\\nAPM Server version: 7.4\\nAPM Agent language and version: Latest version, Ruby\\nBrowser version: N/A\\nUsing the Ruby agent, I know it's possible to exclude certain URL suffixes (ie. /healthcheck) from being reported (which we use with great success!) but in a few services we have we use Sidekiq which is background job processing.  There's a bunch of repetative jobs that just send health information to AWS cloudwatch that I'd like to have the agent not instrument but I can't quite see a config option for classes.\\nIn this example, I'd like to have the 2 boxed jobs not collected by the agent.  Is this do'able?\\n\\nimage.png1187×602 44.6 KB\\n\"},\n",
       " {'text_field': \"Hi @dnorth98!\\nYou can omit the payloads with a filter. It's not going to stop the events from being registered but it is going to stop them from being sent to APM Server.\\nLike so (untested):\\nElasticAPM.add_filter(:omit_cloudwatch_workers) do |payload|\\n  if payload.dig(:transaction, :name) =~ /CloudWatchWorker/\\n    return ElasticAPM::Transport::Filters::SKIP # =&gt; :skip\\n  end\\n\\n  payload\\nend\"},\n",
       " {'text_field': \"As mentioned in docs, when we define force-merge as part of an ILM policy, it also makes the index read-only. I want to force-merge an index to compact it and get some size benefit but I don't want to make it read-only. I want to continue accepting late events after the index has been compacted. Should I manually remove the read-only status for the indices after segment merging is complete ?\"},\n",
       " {'text_field': 'No, you should not write to an index after it has been force-merged. From the docs:\\n\\nWARNING: Force merge should only be called against an index after you have finished writing to it.  Force merge can cause very large (&gt;5GB) segments to be produced, and if you continue to write to such an index then the automatic merge policy will never consider these segments for future merges until they mostly consist of deleted documents. This can cause very large segments to remain in the index which can result in increased disk usage and worse search performance.\\n'},\n",
       " {'text_field': 'Hi everyone,\\ni have data for two different clusters in which the respective nodes have the same name - eg. clustername: henry, nodenames: node1, node2, node3; clustername: joe, nodename: node1, node2, node3.\\nNow i want to create a visualisation in which the data is separated (e.g. separate chart) in the form clustername.nodename in one single visualisation.\\nI think I need a way to create a kind of combined ID from clustername.nodename. Does anyone have any idea how to do this in Kibana?\\nThanks in advance\\ndenim'},\n",
       " {'text_field': 'You should be able to do this with scripted fields.\\nBe aware that doing this can negatively affect performance - if this becomes a problem for you, you can try to create the joined field during ingest (e.g. using logstash or an ingest pipeline)'},\n",
       " {'text_field': \"Hey forum,\\nI am currently working on a project with Kibana, and I was wondering if Kibana has the ability to listen for a webhook. I understand that you can send a webhook from Kibana, and I did some research into watcher, but I couldn't quite find anything on using it to wait for a webhook.\\nI would like to capture this webhook from an external site, and then upon successful delivery use its payload to filter the dashboard with. If anyone has any ideas or tips I would really appreciate them!\\nThank You,\\nDavid\"},\n",
       " {'text_field': 'To accomplish this, your going to have to write the code to handle the webhook yourself. There is a REST API for interacting with the objects Kibana persists. You could either create a plugin to handle the server-side aspect of accepting the webhook, in which case I would recommend checking out the plugin generator node scripts/generate_plugin, or just create an independent app.'},\n",
       " {'text_field': 'I am trying to use JSON struct from input inside the HTTP filter request.\\nThe part in my input looks like this:\\n\\n{\"elemId\":\"1\",\"user\":[{\"uid\" : 1,\"first\": \"John\",\"last\": \"Smith\"},{\"uid\" : 2,\"first\": \"Bob\",\"last\": \"Jones\"},{\"uid\" : 4,\"first\": \"Anna\",\"last\": \"Right\"}]}\\n\\nInside Logstash script, I tried to feed it as following:\\nfilter {\\n  http { \\n    (...)\\n    body =&gt; \\'{\"script\": { (...), \"params\": {\"par1\": [%{[user]}]}}\\'\\n  }\\n}\\n\\nwhich results in following in HTTP request body:\\n\\n{\"script\": { (...) \"params\": {\"par1\": [{last=Smith, uid=1, first=John},{last=Jones, uid=2, first=Bob},{last=Right, uid=4, first=Anna}]}}\\n\\nOf course, it is not proper JSON, as field names and literals are not quoted and produce the following error:\\n\\n\"error\":{\"root_cause\":[{\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\"}],\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse\",\"caused_by\":{\"type\":\"json_parse_exception\",\"reason\":\"Unexpected character (\\'l\\' (code 108)): was expecting double-quote to start field name\\n\\nThis unexpected l is the first letter of last=Smith from the HTTP request.\\nHow to get this JSON from input into the string literal, but with a quotation symbols? I do not know which exact fields and how many objects will be in the input file, so I cannot hardcode it.'},\n",
       " {'text_field': 'If you are starting off with a field on the event that looks like this\\n      \"user\" =&gt; [\\n    [0] {\\n          \"uid\" =&gt; 1,\\n         \"last\" =&gt; \"Smith\",\\n        \"first\" =&gt; \"John\"\\n    },\\n    [1] {\\n          \"uid\" =&gt; 2,\\n         \"last\" =&gt; \"Jones\",\\n        \"first\" =&gt; \"Bob\"\\n    },\\n    [2] {\\n          \"uid\" =&gt; 4,\\n         \"last\" =&gt; \"Right\",\\n        \"first\" =&gt; \"Anna\"\\n    }\\n],\\n\\nthen you can install the json_encode plugin and use\\njson_encode { source =&gt; \"user\" target =&gt; \"[@metadata][encodedUser]\" }\\n\\nto convert it into\\n    \"encodedUser\" =&gt; \"[{\\\\\"first\\\\\":\\\\\"John\\\\\",\\\\\"last\\\\\":\\\\\"Smith\\\\\",\\\\\"uid\\\\\":1},{\\\\\"first\\\\\":\\\\\"Bob\\\\\",\\\\\"last\\\\\":\\\\\"Jones\\\\\",\\\\\"uid\\\\\":2},{\\\\\"first\\\\\":\\\\\"Anna\\\\\",\\\\\"last\\\\\":\\\\\"Right\\\\\",\\\\\"uid\\\\\":4}]\",\\n\\nand then mutate that\\nmutate { gsub =&gt; [ \"[@metadata][encodedUser]\", \\'\"\\', \\'\\\\\"\\' ] }\\n\\nto escape the quotes and get you to\\n    \"encodedUser\" =&gt; \"[{\\\\\\\\\\\\\"first\\\\\\\\\\\\\":\\\\\\\\\\\\\"John\\\\\\\\\\\\\",\\\\\\\\\\\\\"last\\\\\\\\\\\\\":\\\\\\\\\\\\\"Smith\\\\\\\\\\\\\",\\\\\\\\\\\\\"uid\\\\\\\\\\\\\":1},{\\\\\\\\\\\\\"first\\\\\\\\\\\\\":\\\\\\\\\\\\\"Bob\\\\\\\\\\\\\",\\\\\\\\\\\\\"last\\\\\\\\\\\\\":\\\\\\\\\\\\\"Jones\\\\\\\\\\\\\",\\\\\\\\\\\\\"uid\\\\\\\\\\\\\":2},{\\\\\\\\\\\\\"first\\\\\\\\\\\\\":\\\\\\\\\\\\\"Anna\\\\\\\\\\\\\",\\\\\\\\\\\\\"last\\\\\\\\\\\\\":\\\\\\\\\\\\\"Right\\\\\\\\\\\\\",\\\\\\\\\\\\\"uid\\\\\\\\\\\\\":4}]\",\\n\\nwhich I think is what you want to insert into your request body.'},\n",
       " {'text_field': 'Hi All,\\nI´m trying to move from a single note cluster to the following configuration, esentially adding two ES notes for redundancy.\\nes1, es2, es3,\\nKibana\\nLogstash\\nMultiple filebeats\\nI have successfully configured kibana to receive via env variable all three ES urls so it can continue working even if one ES is lost.\\nMy issue is now with logstash, currently I have a single pipeline set up, that before now, pointed to ES like this:\\n\\nhosts =&gt; [ \"${ELASTICSEARCH_HOSTS}\" ]\\n\\nAnd I would later pass the value via compose or marathon using an environmental variable like:\\n\\n\"ELASTICSEARCH_HOSTS\": \"http://xxxxx.x.x:9200\",\\n\\nI now want to configure logstash to be able to continue pushing logs even if one ES instance is lost.\\nI tried everything but it wont work, if I set the pipeline directly like this:\\n\\nhosts =&gt; [ \"http://xxxxx.x.1:9200\", \"http://xxxxx.x.2:9200\", \"http://xxxxx.x.3:9200\" ]\\n\\nIt works just fine, but I tried for example:\\n\\nhosts =&gt; \"${ELASTICSEARCH_HOSTS}\"\\n\\nAnd passing:\\n\\n\"ELASTICSEARCH_HOSTS\": \"[ \"http://xxxxx.x.1:9200\", \"http://xxxxx.x.2:9200\", \"http://xxxxx.x.3:9200\" ]\",\\n\\nOr\\n\\nhosts =&gt; [ \"${ELASTICSEARCH_HOSTS}\" ]\\n\\nAnd passing:\\n\\n\"ELASTICSEARCH_HOSTS\": \" \"http://xxxxx.x.1:9200\", \"http://xxxxx.x.2:9200\", \"http://xxxxx.x.3:9200\" \",\\n\\nAnd I always get the same error:\\n\\n\"Illegal character in authority at index 7: http://xxxxx.x.1:9200\", \"http://xxxxx.x.2:9200\", \"http://xxxxx.x.3:9200\"\\n\\nI´ve look around on line and can´t find anything specific about being able to pass arrays in env variables, so IDK if this is possible at all.\\nThanks in advance,\\nRegards,'},\n",
       " {'text_field': '\\n\\n\\n Merlin_Nunez:\\n\\npass arrays in env variables\\n\\n\\nThat does not work.'},\n",
       " {'text_field': 'Всем привет.\\nПытаюсь связать filebeat с ELK 7.4\\nСоздаю в /etc/logstash/conf.d/postfix.conf\\ninput {\\n  beats {\\n    port =&gt; 5044\\n  }\\n}\\noutput {\\n        elasticsearch {\\n            hosts    =&gt; \"localhost:9200\"\\n            index    =&gt; \"postfix1-%{+YYYY.MM.dd}\"\\n        }\\n}\\n\\nВ /etc/filebeat/filebeat.yml\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n      - /var/log/maillog*\\noutput.logstash:\\n  hosts: [\"192.168.199.146:5044\"]\\nxpack.monitoring:\\n enabled: true\\n  elasticsearch:\\n    hosts: [\"http://192.168.199.146:9200\"]\\n\\nНо index не появляется в Kibana\\nВ логах filebeat\\nWARN    beater/filebeat.go:152  Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning.\\n\\nЛюбая помощь =)'},\n",
       " {'text_field': 'В /etc/filebeat/filebeat.yml\\n\\nBlockquote\\nfilebeat.inputs:\\n\\n\\ntype: log\\nenabled: true\\npaths:\\n- /var/log/maillog*\\noutput.logstash:\\nhosts: [\"192.168.199.146:5044\"]\\n\\nПорты не заблокированы, с filebeat 5044 телнетиться.\\nВот полный лог filbeat\\n\\nBlockquote\\n2019-10-11T03:30:30.292-0400    INFO    instance/beat.go:607    Home path: [/usr/share/filebeat] Config path: [/etc/filebeat] Data path: [/var/lib/filebeat] Logs path: [/var/log/filebeat]\\n2019-10-11T03:30:30.292-0400    INFO    instance/beat.go:615    Beat ID: 1a3f0e19-0b95-4d3e-881a-d37400468513\\n2019-10-11T03:30:30.292-0400    INFO    [beat]  instance/beat.go:903    Beat info       {\"system_info\": {\"beat\": {\"path\": {\"config\": \"/etc/filebeat\", \"data\": \"/var/lib/filebeat\", \"home\": \"/usr/share/filebeat\", \"logs\": \"/var/log/filebeat\"}, \"type\": \"filebeat\", \"uuid\": \"1a3f0e19-0b95-4d3e-881a-d37400468513\"}}}\\n2019-10-11T03:30:30.292-0400    INFO    [beat]  instance/beat.go:912    Build info      {\"system_info\": {\"build\": {\"commit\": \"f940c36884d3749901a9c99bea5463a6030cdd9c\", \"libbeat\": \"7.4.0\", \"time\": \"2019-09-27T07:45:44.000Z\", \"version\": \"7.4.0\"}}}\\n2019-10-11T03:30:30.292-0400    INFO    [beat]  instance/beat.go:915    Go runtime info {\"system_info\": {\"go\": {\"os\":\"linux\",\"arch\":\"amd64\",\"max_procs\":1,\"version\":\"go1.12.9\"}}}\\n2019-10-11T03:30:30.293-0400    INFO    [beat]  instance/beat.go:919    Host info       {\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2019-10-10T06:15:26-04:00\",\"containerized\":false,\"name\":\"mail.test.ru\",\"ip\":[\"127.0.0.1/8\",\"::1/128\",\"192.168.199.145/24\",\"fe80::277c:61b3:ac2:bc8c/64\"],\"kernel_version\":\"3.10.0-957.el7.x86_64\",\"mac\":[\"00:0c:29:cb:1a:3d\"],\"os\":{\"family\":\"redhat\",\"platform\":\"centos\",\"name\":\"CentOS Linux\",\"version\":\"7 (Core)\",\"major\":7,\"minor\":6,\"patch\":1810,\"codename\":\"Core\"},\"timezone\":\"EDT\",\"timezone_offset_sec\":-14400,\"id\":\"b27e5adbf8a1485faffe0eeec83f47f2\"}}}\\n2019-10-11T03:30:30.294-0400    INFO    [beat]  instance/beat.go:948    Process info    {\"system_info\": {\"process\": {\"capabilities\": {\"inheritable\":null,\"permitted\":[\"chown\",\"dac_override\",\"dac_read_search\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"linux_immutable\",\"net_bind_service\",\"net_broadcast\",\"net_admin\",\"net_raw\",\"ipc_lock\",\"ipc_owner\",\"sys_module\",\"sys_rawio\",\"sys_chroot\",\"sys_ptrace\",\"sys_pacct\",\"sys_admin\",\"sys_boot\",\"sys_nice\",\"sys_resource\",\"sys_time\",\"sys_tty_config\",\"mknod\",\"lease\",\"audit_write\",\"audit_control\",\"setfcap\",\"mac_override\",\"mac_admin\",\"syslog\",\"wake_alarm\",\"block_suspend\"],\"effective\":[\"chown\",\"dac_override\",\"dac_read_search\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"linux_immutable\",\"net_bind_service\",\"net_broadcast\",\"net_admin\",\"net_raw\",\"ipc_lock\",\"ipc_owner\",\"sys_module\",\"sys_rawio\",\"sys_chroot\",\"sys_ptrace\",\"sys_pacct\",\"sys_admin\",\"sys_boot\",\"sys_nice\",\"sys_resource\",\"sys_time\",\"sys_tty_config\",\"mknod\",\"lease\",\"audit_write\",\"audit_control\",\"setfcap\",\"mac_override\",\"mac_admin\",\"syslog\",\"wake_alarm\",\"block_suspend\"],\"bounding\":[\"chown\",\"dac_override\",\"dac_read_search\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"linux_immutable\",\"net_bind_service\",\"net_broadcast\",\"net_admin\",\"net_raw\",\"ipc_lock\",\"ipc_owner\",\"sys_module\",\"sys_rawio\",\"sys_chroot\",\"sys_ptrace\",\"sys_pacct\",\"sys_admin\",\"sys_boot\",\"sys_nice\",\"sys_resource\",\"sys_time\",\"sys_tty_config\",\"mknod\",\"lease\",\"audit_write\",\"audit_control\",\"setfcap\",\"mac_override\",\"mac_admin\",\"syslog\",\"wake_alarm\",\"block_suspend\"],\"ambient\":null}, \"cwd\": \"/var/log\", \"exe\": \"/usr/share/filebeat/bin/filebeat\", \"name\": \"filebeat\", \"pid\": 11533, \"ppid\": 10829, \"seccomp\": {\"mode\":\"disabled\"}, \"start_time\": \"2019-10-11T03:30:29.400-0400\"}}}\\n2019-10-11T03:30:30.294-0400    INFO    instance/beat.go:292    Setup Beat: filebeat; Version: 7.4.0\\n2019-10-11T03:30:30.296-0400    INFO    [publisher]     pipeline/module.go:97   Beat name: mail.test.ru\\n2019-10-11T03:30:30.296-0400    WARN    beater/filebeat.go:152  Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning.\\n\\nА как понять под каким пользователем запускается filebeat ?'},\n",
       " {'text_field': 'I am trying to install Elasticsearch from the APT repositories as per Elasticsearch Reference [7.4] and I am getting an error. See below for the commands issued. Are the Elastic APT repositories up at the moment because I am getting a  \"Not Found\" from https://artifacts.elastic.co/packages/7.x/apt ?\\n# wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -\\nOK\\n# apt-get install apt-transport-https\\nReading package lists... Done\\nBuilding dependency tree\\nReading state information... Done\\nThe following NEW packages will be installed:\\n  apt-transport-https\\n0 upgraded, 1 newly installed, 0 to remove and 64 not upgraded.\\nNeed to get 1692 B of archives.\\nAfter this operation, 153 kB of additional disk space will be used.\\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 apt-transport-https all 1.6.12 [1692 B]\\nFetched 1692 B in 0s (4570 B/s)\\nSelecting previously unselected package apt-transport-https.\\n(Reading database ... 40911 files and directories currently installed.)\\nPreparing to unpack .../apt-transport-https_1.6.12_all.deb ...\\nUnpacking apt-transport-https (1.6.12) ...\\nSetting up apt-transport-https (1.6.12) ...\\n# echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | tee -a /etc/apt/sources.list.d/elastic-7.x.list\\ndeb https://artifacts.elastic.co/packages/7.x/apt stable main\\n# apt-get -y install elasticsearch\\nReading package lists... Done\\nBuilding dependency tree\\nReading state information... Done\\nE: Unable to locate package elasticsearch'},\n",
       " {'text_field': 'That fixed it. Thank you.'},\n",
       " {'text_field': 'Hi, I\\'m starting with ECE/Elastic in general, and now I\\'m trying to create a watcher to send an e-mail based on a query that aggregates me a metric coming from APM.\\nThis watcher runs but never is triggered.\\n{\\n  \"trigger\": {\\n    \"schedule\": {\\n      \"interval\": \"1m\"\\n    }\\n  },\\n  \"input\": {\\n    \"search\": {\\n      \"request\": {\\n        \"search_type\": \"query_then_fetch\",\\n        \"indices\": [\\n          \"apm-*\"\\n        ],\\n        \"rest_total_hits_as_int\": true,\\n        \"body\": {\\n          \"size\": 0,\\n          \"query\": {\\n            \"bool\": {\\n              \"must\": [\\n                {\\n                  \"match\": {\\n                    \"service.name\": \"MY-SERVER\"\\n                  }\\n                }\\n              ],\\n              \"filter\": {\\n                \"range\": {\\n                  \"@timestamp\": {\\n                    \"gte\": \"now-1h\"\\n                  }\\n                }\\n              }\\n            }\\n          },\\n          \"aggs\": {\\n            \"jvm.thread.count\": {\\n              \"max\": {\\n                \"field\": \"jvm.thread.count\"\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"condition\": {\\n    \"compare\": {\\n      \"ctx.payload.aggregations.jvm.thread.count.value\": {\\n        \"gte\": 5\\n      }\\n    }\\n  },\\n  \"actions\": {\\n    \"my-logging-action\": {\\n      \"logging\": {\\n        \"level\": \"info\",\\n        \"text\": \"There are {{jvm.thread.count}} threads. Threshold is 5.\"\\n      }\\n    },\\n    \"send_email\": {\\n      \"email\": {\\n        \"profile\": \"standard\",\\n        \"to\": [\\n          \"my-email\"\\n        ],\\n        \"subject\": \"Alerta Watch - {{service.name}}\",\\n        \"body\": {\\n          \"text\": \"Quantidade de threads ativas é: {{jvm.thread.count}}. Threshold é 5.\"\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nIn the Watcher execution history I got:\\n{\\n  \"watch_id\": \"4cd11254-acb1-4c79-849e-6f9984ee91a4\",\\n  \"node\": \"MXBCDigARAaX1kdhaSAebQ\",\\n  \"state\": \"execution_not_needed\",\\n  \"user\": \"elastic\",\\n  \"status\": {\\n    \"state\": {\\n      \"active\": true,\\n      \"timestamp\": \"2019-10-10T17:36:21.275Z\"\\n    },\\n    \"last_checked\": \"2019-10-10T17:56:09.887Z\",\\n    \"actions\": {\\n      \"my-logging-action\": {\\n        \"ack\": {\\n          \"timestamp\": \"2019-10-10T17:36:21.275Z\",\\n          \"state\": \"awaits_successful_execution\"\\n        }\\n      },\\n      \"send_email\": {\\n        \"ack\": {\\n          \"timestamp\": \"2019-10-10T17:36:21.275Z\",\\n          \"state\": \"awaits_successful_execution\"\\n        }\\n      }\\n    },\\n    \"execution_state\": \"execution_not_needed\",\\n    \"version\": -1\\n  },\\n  \"trigger_event\": {\\n    \"type\": \"schedule\",\\n    \"triggered_time\": \"2019-10-10T17:56:09.887Z\",\\n    \"schedule\": {\\n      \"scheduled_time\": \"2019-10-10T17:56:09.546Z\"\\n    }\\n  },\\n  \"input\": {\\n    \"search\": {\\n      \"request\": {\\n        \"search_type\": \"query_then_fetch\",\\n        \"indices\": [\\n          \"apm-*\"\\n        ],\\n        \"rest_total_hits_as_int\": true,\\n        \"body\": {\\n          \"size\": 0,\\n          \"query\": {\\n            \"bool\": {\\n              \"must\": [\\n                {\\n                  \"match\": {\\n                    \"service.name\": \"MY-SERVER\"\\n                  }\\n                }\\n              ],\\n              \"filter\": {\\n                \"range\": {\\n                  \"@timestamp\": {\\n                    \"gte\": \"now-1h\"\\n                  }\\n                }\\n              }\\n            }\\n          },\\n          \"aggs\": {\\n            \"jvm.thread.count\": {\\n              \"max\": {\\n                \"field\": \"jvm.thread.count\"\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \"condition\": {\\n    \"compare\": {\\n      \"ctx.payload.aggregations.jvm.thread.count.value\": {\\n        \"gte\": 5\\n      }\\n    }\\n  },\\n  \"metadata\": {\\n    \"name\": \"Alert JVM Thread Count\",\\n    \"xpack\": {\\n      \"type\": \"json\"\\n    }\\n  },\\n  \"result\": {\\n    \"execution_time\": \"2019-10-10T17:56:09.887Z\",\\n    \"execution_duration\": 109,\\n    \"input\": {\\n      \"type\": \"search\",\\n      \"status\": \"success\",\\n      \"payload\": {\\n        \"_shards\": {\\n          \"total\": 165,\\n          \"failed\": 0,\\n          \"successful\": 165,\\n          \"skipped\": 161\\n        },\\n        \"hits\": {\\n          \"hits\": [],\\n          \"total\": 3838,\\n          \"max_score\": null\\n        },\\n        \"took\": 108,\\n        \"timed_out\": false,\\n        \"aggregations\": {\\n          \"jvm.thread.count\": {\\n            \"value\": 49\\n          }\\n        }\\n      },\\n      \"search\": {\\n        \"request\": {\\n          \"search_type\": \"query_then_fetch\",\\n          \"indices\": [\\n            \"apm-*\"\\n          ],\\n          \"rest_total_hits_as_int\": true,\\n          \"body\": {\\n            \"size\": 0,\\n            \"query\": {\\n              \"bool\": {\\n                \"must\": [\\n                  {\\n                    \"match\": {\\n                      \"service.name\": \"MY-SERVER\"\\n                    }\\n                  }\\n                ],\\n                \"filter\": {\\n                  \"range\": {\\n                    \"@timestamp\": {\\n                      \"gte\": \"now-1h\"\\n                    }\\n                  }\\n                }\\n              }\\n            },\\n            \"aggs\": {\\n              \"jvm.thread.count\": {\\n                \"max\": {\\n                  \"field\": \"jvm.thread.count\"\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    },\\n    \"condition\": {\\n      \"type\": \"compare\",\\n      \"status\": \"success\",\\n      \"met\": false,\\n      \"compare\": {\\n        \"resolved_values\": {\\n          \"ctx.payload.aggregations.jvm.thread.count.value\": null\\n        }\\n      }\\n    },\\n    \"actions\": []\\n  },\\n  \"messages\": []\\n}\\n\\nI tested the query on my elastic and I have results:\\n{\\n  \"took\" : 114,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 165,\\n    \"successful\" : 165,\\n    \"skipped\" : 161,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 3730,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : null,\\n    \"hits\" : [ ]\\n  },\\n  \"aggregations\" : {\\n    \"jvm.thread.count\" : {\\n      \"value\" : 39.0\\n    }\\n  }\\n}\\n\\nCan someone give me a hint on what\\'s wrong?\\nThanks!! '},\n",
       " {'text_field': 'Hey,\\nI think the dots in the fieldname are confusing for the mustache parser, as the mustache parser now thinks there is a JSON structure like jvm : { thread : { count : .. in your JSON and thus returns null. Can you try a fieldname without any dots and see if that is working?\\n--Alex'},\n",
       " {'text_field': 'Is there any way to tell the difference between an empty variable and a non-existent one? The code\\nif [v1] == \"\" {\\ndo something\\n}\\nacts exactly the same whether the variable exists and is empty or does not exist at all. The trouble is that the following code has two different results depending on whether the variable exists or not:\\nif [v1] == \"\" {\\nmutate { add_field =&gt; { \"[v2]\" =&gt; \"value2\" } }\\n}\\nIf [v1] does not exist then [v2] contains \"value2\". If [v1] exists and is empty then [v2] is an array [\"\", \"value2\"]. If I change the add_field to update then in the first case [v2] does not get created (cannot update a non-existent variable) and [v2] is a scalar \"value2\".'},\n",
       " {'text_field': 'The usual way to test for existence is\\nif [v1] { [...]\\n\\nI know of no way to distinguish between a nil value and non-existence.'},\n",
       " {'text_field': 'Hi Logstash Ninja Masters,\\nI recently spun up a Logstash docker container (v 7.4.0), to replace a local-app version of LS on the same Ubuntu box.  I‘m confused how to instruct the Docker LS to use the configuration that the local-app LS used?\\nSome details:\\nMy host machine is Ubuntu 16.04.4, and my Docker version is 17.09.0-ce.  For the moment, I want a simple standalone LS docker instance.  By that, I mean I don’t want to worry about pipeline files, I don’t want to use docker-compose.  Here’s my docker run command:\\ndocker run --rm -it -d \\\\\\n    -v /usr/share/logstash/config/logstash.conf:/home/me/logstash/logstash-7.4.0/config/myConfig.conf\\n    --name myLogstash \\\\\\n    --net myNet \\\\\\n    -e XPACK.MONITORING_ELASTICSEARCH_HOSTS=\"http://10.10.10.100:9200\" \\\\\\n    docker.elastic.co/logstash/logstash:7.4.0\\n\\nThat second line is my clumsy attempt to tell Docker LS, “Hey, be sure to import and use this local config file, okay?”\\nThe container comes up great:\\nme@ubuntu1:~# docker ps\\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                                                                              NAMES\\n8ec488b628f5        c2c1ac6b995b          \"/usr/local/bin/do...\"   2 hours ago         Up 2 hours          5044/tcp, 9600/tcp                                                                 myLogstash\\nme@ubuntu1:~#\\n\\nSo… the container comes up, but I don’t think its using my config file.\\nHow do I force the Docker LS to do that?  I’ve tried copying the desired file into the container’s /usr/share/logstash/config/ directory; no effect.  Can I do that and somehow restart the service?\\nSome other questions I have that aren’t address in the documentation…  From running the local version of LS, I learned that you launched the LS service like this:\\n~bin/logstash -r -f config/myConfig.conf\\nBasically, you execute the logstash app, suppling your config file as input.  The config file has a .conf suffix.  So why does the LS Docker documentation talk about .yml files as config files?  Are they the same?\\nFinal Jeopardy Question:  Once my config file is (hopefully) used, how do I verify Docker LS is running it?\\nAnyway, if you can see the error of my ways, I’d appreciate any help you can offer.  Thanks!'},\n",
       " {'text_field': 'Hey all,\\nI worked out the solution, which may help anyone else who is following in my footsteps.  All that I document below was in the online Logstash 7.4.0 documentation, just spread around in different locations.\\nThe first thing you must do is edit a settings file here:  /usr/share/logstash/config/logstash.yml\\nYou need to add a line telling Logstash where your configuration files is:\\nhttp.host: 0.0.0.0\\npath.config: /usr/share/logstash/config/logstash.conf                   &lt;&lt;========================\\nxpack.monitoring.elasticsearch.hosts: http://10.10.10.10:9200\\n\\nThat second line is what I added.  Then, I made sure the config file I wanted was in that location.  Finally, I restarted the LS docker container.  When the container restarted, it was using the config file I wanted.'},\n",
       " {'text_field': 'When I try to use the following query for Filter in Visualize, I am getting the following error.\\nRequest to Elasticsearch failed: {\"error\":{\"root_cause\":[{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [{ \\\\\"bool\\\\\": { \\\\t\\\\\"must\\\\\" :\\nHere is my query for Filters aggregation\\n{\\n\"bool\": {\\n\"must\" : {\\n\"match_all\":{}\\n}\\n\"filter\" : {\\n\"must_not\" : {\\n\"exists\" : {\\n\"field\" : \"object1.name\"\\n}\\n},\\n\"must\" : {\\n\"exist\" : {\\n\"field\" : \"object2.name\"\\n}\\n}\\n}\\n}\\n}'},\n",
       " {'text_field': 'This worked\\n{\\n\"bool\": {\\n\"must_not\" : {\\n\"exists\" : {\\n\"field\" : \"responderWithLevel.loginName\"\\n}\\n},\\n\"must\" :  {\\n\"exists\" : {\\n\"field\" : \"responder.loginName\"\\n}\\n}\\n}\\n}'},\n",
       " {'text_field': 'Hi Admin,\\nI have a grok pattern from nginx which is working perfectly when the data is coming.\\ngrok {\\n  match =&gt; { \"message\" =&gt; \\'%{DATA:clientip} %{DATA:VirtualHost} %{DATA:User} \\\\[%{HTTPDATE:timestamp}\\\\] %{DATA:nginx.ssl.protocol}:%{DATA:nginx.ssl.cipher} \"(?:%{WORD:request_method} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent} rt=(%{NUMBER:nginx.access.request_time:float}|-) uct=\"(%{NUMBER:nginx.access.upstream.connect_time:float}|-)\" uht=\"(%{NUMBER:nginx.access.upstream.header_time:float}|-)\" urt=\"(%{NUMBER:nginx.access.upstream.response_time:float}|-)\" csi=\"(%{DATA:CookieServiceID})\" ua=\"(%{DATA:nginx.access.upstream.addr})\" us=\"(%{INT:upstreamstatus:int})\"\\' }\\n}\\n\\nBut some time nginx writes as - for any value then its failing how to handle that, especially below us is coming always \"-\" and only some time 200. whenever i am getting - its failing and\\nuct=\"-\" uht=\"-\" urt=\"-\" csi=\"-\" ua=\"-\" us=\"-\"\\nus=\"(%{INT:upstreamstatus:int})\"\\'\\nplease help.'},\n",
       " {'text_field': 'I don\\'t think it can be inside the brace. It should be\\nus=\"(%{INT:upstreamstatus:int}|-)\"\\n\\nin the same way that you have uct=\"(%{NUMBER:nginx.access.upstream.connect_time:float}|-)\"'},\n",
       " {'text_field': 'Hi all,\\nSeem to be having trouble getting some of my logs into logstash. Have been able to sucessfully work through most of my logs, but this last one is giving me grief.\\nExample line from logfile;\\n2019-10-01 22:25:54,903|1.2.5 Unhandled exception in ASP.NET\\n\\nLogstash configuration section for file;\\n    grok {\\n  match =&gt; [ \"message\", \"%{TIMESTAMP_ISO8601:log_timestamp}\\\\|%{GREEDYDATA:messagedata\"]\\n}\\n\\ndate {\\n  match =&gt; [ \"log_timestamp\", \"YYYY-MM-dd HH:mm:ss,SSS\"]\\n  target =&gt; \"@timestamp\"\\n}\\n\\nOutput from STDOUT\\n{\\n\"@timestamp\" =&gt; 2019-10-10T22:43:41.120Z,\\n   \"message\" =&gt; \"2019-10-01 22:25:54,903|1.2.5 Unhandled exception in ASP.NET\",\\n    \"fields\" =&gt; {\\n    \"log_type\" =&gt; \"episerver\"\\n},\\n       \"ecs\" =&gt; {\\n    \"version\" =&gt; \"1.0.1\"\\n},\\n     \"agent\" =&gt; {\\n    \"ephemeral_id\" =&gt; \"441f3686-545d-4772-8bf0-62c8ecbe80f4\",\\n        \"hostname\" =&gt; \"AKL-LT-IT3\",\\n              \"id\" =&gt; \"28a84e54-a366-43d5-bc07-2172be9b6518\",\\n         \"version\" =&gt; \"7.3.1\",\\n            \"type\" =&gt; \"filebeat\"\\n},\\n     \"input\" =&gt; {\\n    \"type\" =&gt; \"log\"\\n},\\n  \"@version\" =&gt; \"1\",\\n      \"tags\" =&gt; [\\n    [0] \"beats_input_codec_plain_applied\",\\n    [1] \"_grokparsefailure\"\\n],\\n       \"log\" =&gt; {\\n    \"offset\" =&gt; 526394,\\n      \"file\" =&gt; {\\n        \"path\" =&gt; \"C:\\\\\\\\Temp\\\\\\\\EpiServer\\\\\\\\EPiServerLog.txt.2019-10-01\"\\n    }\\n}\\n\\n}\\nThoughts?\\nCheers, Andrew'},\n",
       " {'text_field': 'Turned out to be a missing parenthesis in the grok pattern\\nmatch =&gt; [ \"message\", \"%{TIMESTAMP_ISO8601:log_timestamp}\\\\|%{GREEDYDATA:messagedata\"]\\n\\n}\\nUpdated to\\nmatch =&gt; [ \"message\", \"%{TIMESTAMP_ISO8601:log_timestamp}\\\\|%{GREEDYDATA:messagedata}\"]\\n}\\n\\nLogstash did not pick up on the missing } and still loaded the config'},\n",
       " {'text_field': 'I\\'m not quite sure why this isn\\'t working but I\\'m trying to post to an alias instead of an index and it\\'s not working. I have indexes called\\nindex-september &amp; index-october and they are both under the alias index. When I put my logstash index =&gt; \"index\" it fails but when I put  index =&gt; \"index-september\" it succeeds. Can I post to an alias?\\nI\\'m doing post processing on data in my indices and I\\'d like the documents to reroute via their _id to the correct index. I\\'m pulling them in via the elasticsearch input plugin. Any suggestion on how to do this?\\nThanks!'},\n",
       " {'text_field': 'If you had indices index-january thru index-september (the is_write_alias: true).  Read via the alias \"index\" would read from all of them, just like using a wildcard \"index-*\".  Writes to \"index\" would go to index-september.  I think if you post directly to any, (index-june) you could write to them (unless set to read only).\\nThe idea of rollover is at 01/01/2019 00:00:00 you create index-october and set is_write_alias: true, writes to \"index\" go to the new month.  (I don\\'t think the rollover api or ILM can do rollover at a specific time however, they do it by size, age or # events.)'},\n",
       " {'text_field': 'The following was correctly output to the \"date_time_1\" field after adding 9 hours to the date and time of \"Timestamp\".The following is the source\\n    ruby {\\n\\t\\t\\t code =&gt; \"event.set(\\'tt\\',event.get(\\'[@timestamp]\\').time.localtime(\\'+09:00\\').strftime(\\'%Y-%m-%d %H:%M:%S\\'))\" \\n    }\\n\\n    mutate {\\n        add_field =&gt; {\"date_time_1\" =&gt; \"%{[tt]}\"}\\n\\t}\\n\\nThe problem is this\\nit was not output correctly after adding 9 hours to the date and time of IISLog output to the log.The value of the “date_time_1” field is “% {[tt]}”.\\nThe following is the source\\n    #Input data byIISlog\\n    grok {\\n        match =&gt; [\"message\", \"%{TIMESTAMP_ISO8601:date_time} %{GREEDYDATA:etc}\"]\\n\\t}\\n\\n    date {\\n    \\tmatch =&gt; [ \"date_time\", \"YYYY-MM-dd HH:mm:ss\"]\\n    \\ttimezone =&gt; \"Etc/GMT\"\\n    \\ttarget =&gt; \"date_time\"\\n\\t}\\n\\n    ruby {\\n\\t\\t\\t code =&gt; \"event.set(\\'tt\\',event.get(\\'[date_time]\\').time.localtime(\\'+09:00\\').strftime(\\'%Y-%m-%d %H:%M:%S\\'))\" \\n    }\\n\\n    mutate {\\n        add_field =&gt; {\"date_time_1\" =&gt; \"%{[tt]}\"}\\n\\t}\\n\\nWhat format does the IISLog date and time calculate with the Ruby filter? Please tell me.\\nSupplement\\nThe IIS log \"2019-01-01 01:23\" UTC time.write\\nI want to convert this to \"2019-01-01 10:23\" and outputing.\\nPlease let me know if you have any alternative means.\\nthanks'},\n",
       " {'text_field': 'Badger\\nThe reply was delayed during the holidays.\\nThank you for your reply.\\nEven without using a Ruby script, we were able to achieve what we wanted to do with the description you taught us.\\nThank you for teaching me.'},\n",
       " {'text_field': 'Hi team,\\nI configured email alert in ConfigMap.yml (Elasticsearch.yml file) with below components\\nxpack.watcher.enabled: true\\nxpack.notification.email.account:\\napple_account:\\nprofile: apple\\nsmtp:\\nauth: true\\nstarttls.enable: true\\nhost: bz.apple.com\\nport: 25\\nuser: smende@apple.com\\nAfter that I deployed in Kubernetes by using Kubectl apply -f es-master\\n$ kubectl get pods\\nNAME          READY   STATUS             RESTARTS   AGE\\nes-master-0   1/2     CrashLoopBackOff   3          99s\\nwhile checking logs\\n$ kubectl logs -f es-master-0 -c es-master\\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\\nException in thread \"main\" 2019-10-11 05:32:57,681 main ERROR No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property \\'log4j2.debug\\' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2\\nElasticsearchParseException[null-valued setting found for key [xpack.notification.email.account] found at line number [16], column number [34]]\\nat org.elasticsearch.common.settings.Settings.validateValue(Settings.java:736)\\nat org.elasticsearch.common.settings.Settings.fromXContent(Settings.java:700)\\nat org.elasticsearch.common.settings.Settings.fromXContent(Settings.java:650)\\nat org.elasticsearch.common.settings.Settings.access$500(Settings.java:82)\\nat org.elasticsearch.common.settings.Settings$Builder.loadFromStream(Settings.java:1135)\\nat org.elasticsearch.common.settings.Settings$Builder.loadFromPath(Settings.java:1112)\\nat org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:100)\\nat org.elasticsearch.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:95)\\nat org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86)\\nat org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124)\\nat org.elasticsearch.cli.Command.main(Command.java:90)\\nat org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:116)\\nat org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93)\\nI am facing the above error , Could you please help me out on this . how configure the email alert.\\nThanks,\\nshivudu.M'},\n",
       " {'text_field': 'can you share the exact yaml file in a gist, so that the indendation of the configuration YAML file is preserved?\\nalso, can you share the elasticsearch version and the java version you are using?\\nThanks!'},\n",
       " {'text_field': 'Hi, I would like to ask you for help. We are already using Elasticsearch for search on  the webpages. I want to  start with Elastic Stack: Is it possible to use this Elasticsearch also for Elastic Stack .. ? Or do I have to install new elasticsearch?'},\n",
       " {'text_field': \"You can just install Kibana and others and connect to the existing cluster.\\nBut depending on the use cases it might be worthwhile to create a new cluster.\\nIf for example one cluster is used for your website search (so for your customers) and the new use case you want to add is for your internal logs, I'd create another cluster.\\nSo the log use case won't have an impact on the web site if the former is overloaded.\\nMakes sense?\"},\n",
       " {'text_field': 'Dear community,\\nI\\'m new to elastic &amp; i\\'m trying to cloud-ize my application via docker then kubernetes.\\nI\\'ve built the same Springboot app as:\\n\\n  \\n      \\n      Medium – 18 Sep 18\\n  \\n  \\n    \\n\\nHow to use Java High Level Rest Client with Spring Boot to talk to AWS...\\n\\nRecently, for one of my projects, I needed to use Elasticsearch running on AWS Elasticsearch Service domain. When I created the cluster in…\\n\\n  Reading time: 7 min read\\n    \\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nTo test this app:\\n1. Pull docker image\\na. docker pull docker.elastic.co/elasticsearch/elasticsearch:6.8.2\\nb. docker pull docker.elastic.co/elasticsearch/kibana:6.8.2\\n2. Run image on docker\\na. docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.8.2\\nb. docker run --link :elasticsearch -p 5601:5601 docker.elastic.co/kibana/kibana:6.8.2\\n3. both locahost:9200 &amp; 5601 are ping-able\\n4. mvn package app into jar &amp; run it\\njava -jar elastic-client-0.0.1-LOCAL.jar\\n4.5. application.properties\\nelasticsearch.host=http://localhost:9200\\n5. CRUD successful via curl or \"postman\" (a restful client) works too\\n6. build docker image successfully then push to private repo by:\\ndocker build -t hironwayj/elastic-client-local .\\ndocker push hironwayj/elastic-client-local\\nhere\\'s my dockerfile:\\nFROM maven:3.5-jdk-8-alpine as builder\\nWORKDIR /app\\nCOPY pom.xml .\\nCOPY src ./src\\nRUN mvn package -DskipTests\\nFROM openjdk:8-jre-alpine\\nCOPY --from=builder /app/target/elastic-client-0.0.1-LOCAL.jar /elastic-client-local.jar\\nCMD [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/elastic-client-local.jar\"]\\n7. run docker image\\ndocker run -p 8080:8080 hironwayj/elastic-client-local\\n8. Failed when CRUD with connection refused\\njava.net.ConnectException: Connection refused\\nat org.elasticsearch.client.RestClient$SyncResponseListener.get(RestClient.java:943) ~[elasticsearch-rest-client-6.4.3.jar!/:6.8.2]\\nat org.elasticsearch.client.RestClient.performRequest(RestClient.java:227) ~[elasticsearch-rest-client-6.4.3.jar!/:6.8.2]\\nat org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1764) ~[elasticsearch-rest-high-level-client-6.8.2.jar!/:6.8.2]\\nat org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1734) ~[elasticsearch-rest-high-level-client-6.8.2.jar!/:6.8.2]\\nat org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1696) ~[elasticsearch-rest-high-level-client-6.8.2.jar!/:6.8.2]\\nat org.elasticsearch.client.RestHighLevelClient.search(RestHighLevelClient.java:1092) ~[elasticsearch-rest-high-level-client-6.8.2.jar!/:6.8.2]\\nat com.hackthon.tk.elasticclient.service.ProfileService.findAllProfile(ProfileService.java:70) ~[classes!/:0.0.1-LOCAL]\\nat com.hackthon.tk.elasticclient.controller.ProfileController.findAll(ProfileController.java:40) ~[classes!/:0.0.1-LOCAL]\\nDid i configure somewhere wrong? Why is java -jar runnable while failed in docker image?\\nThank you so much!'},\n",
       " {'text_field': 'The problem has been solved, thanks a lot!'},\n",
       " {'text_field': 'I am trying to configure the logstash to my ELK stack and i keep running into this error.\\nI have Java version 8.\\nD:\\\\workplace\\\\Elasticsearch\\\\logstash-7.4.0&gt;cd bin\\nD:\\\\workplace\\\\Elasticsearch\\\\logstash-7.4.0\\\\bin&gt;logstash -e \\'input{stdin{}}output{stdout{}}\\'\\nThread.exclusive is deprecated, use Thread::Mutex\\nSending Logstash logs to D:/workplace/Elasticsearch/logstash-7.4.0/logs which is now configured via log4j2.properties\\n[2019-10-11T03:34:50,828][WARN ][logstash.config.source.multilocal] Ignoring the \\'pipelines.yml\\' file because modules or command line options are specified\\n[2019-10-11T03:34:50,858][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"7.4.0\"}\\n[2019-10-11T03:34:52,143][ERROR][logstash.agent           ] Failed to execute action {:action=&gt;LogStash::PipelineAction::Create/pipeline_id:main, :exception=&gt;\"LogStash::ConfigurationError\", :message=&gt;\"Expected one of #, input, filter, output at line 1, column 1 (byte 1)\", :backtrace=&gt;[\"D:/workplace/Elasticsearch/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:41:in compile_imperative\\'\", \"D:/workplace/Elasticsearch/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:49:incompile_graph\\'\", \"D:/workplace/Elasticsearch/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:11:in block in compile_sources\\'\", \"org/jruby/RubyArray.java:2584:inmap\\'\", \"D:/workplace/Elasticsearch/logstash-7.4.0/logstash-core/lib/logstash/compiler.rb:10:in compile_sources\\'\", \"org/logstash/execution/AbstractPipelineExt.java:153:ininitialize\\'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize\\'\", \"D:/workplace/Elasticsearch/logstash-7.4.0/logstash-core/lib/logstash/java_pipeline.rb:26:ininitialize\\'\", \"D:/workplace/Elasticsearch/logstash-7.4.0/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute\\'\", \"D:/workplace/Elasticsearch/logstash-7.4.0/logstash-core/lib/logstash/agent.rb:326:inblock in converge_state\\'\"]}\\n[2019-10-11T03:34:52,311][INFO ][logstash.runner          ] Logstash shut down.'},\n",
       " {'text_field': '@rishikaswarnkar,\\n\\n\\n\\n rishikaswarnkar:\\n\\n\"LogStash::ConfigurationError\", :message=&gt;\"Expected one of #, input, filter, output at line 1,\\n\\n\\nPlease check as per above log or provide your logstash config file.\\nThanks.'},\n",
       " {'text_field': 'I have many properties files with the following structures key = Value , for example :\\nrejets.cache.temporaire.time=60\\ndata.cache.referentiel.time=10\\ndata.cache.temporaire.time=60\\ndata.cache.getinfos.time=200\\n\\nI want to index this file as a document elasticsearch dynamicaly (the name of the field ) like this :\\n {\\n   rejets.cache.temporaire.time:60\\n   data.cache.referentiel.time:10\\n   data.cache.temporaire.time:60\\n    data.cache.getinfos.time:200\\n  }\\n\\nI used the following configuration but i found in my index each field is indexed in one document ,so in my case i have 4 documents indexed :\\nmutate {\\n             gsub =&gt; [ \"message\", \"[\\\\\\\\\\\\\\\\]r\", \"esp\" ]\\n             gsub =&gt; [ \"message\", \"[\\\\\\\\\\\\\\\\]n\", \"ace\" ]\\n        }\\n\\n        ruby {\\n             code =&gt; \"begin; event[\\'message\\'] = event[\\'message\\'].split(/espace/); rescue Exception; end\"\\n        }\\n\\n        kv {\\n            source =&gt; \"message\"\\n            value_split =&gt; \"=\"\\n            field_split =&gt; \"\\\\n\"\\n        }\\n\\n        grok {\\n                        match =&gt; { \"message\" =&gt; \"(?&lt;param&gt;[^=]*)=%{GREEDYDATA:value}\" }\\n                }\\n\\n        if \"_grokparsefailure\" in [tags] {\\n                        drop {}\\n                }\\n\\n        mutate {\\n            remove_field =&gt; [\"message\"]\\n        }  \\n\\nI want to have one  document indexed with all the fields of the properties file\\nHow can i make this please ? thanks for Help'},\n",
       " {'text_field': 'Use a multiline codec on the input to combine all of the lines into a single event. Do this by using a pattern that never matches and a timeout\\n codec =&gt; multiline { pattern =&gt; \"^Spalanzani\" negate =&gt; true what =&gt; previous auto_flush_interval =&gt; 1 multiline_tag =&gt; \"\" }\\n\\nThen use a kv filter with a literal newline in the field_split option\\n    kv { field_split =&gt; \"\\n\" value_split =&gt; \"=\" }'},\n",
       " {'text_field': 'Hello, i have the following watcher action, sending to MS Teams, everything works fine.  But now i want to add some additional formatting to the payload and need to add keys with the @ symbol.\\nie: How do i add pl.@type = \"MessageCard\"; and pl.@context = \\'http://schema.org/extensions\\'; to the script transform pl object?\\n\"actions\": {\\n    \"teams_webhook\": {\\n      \"transform\":{\\n        \"script\":{\\n          \"source\":\"\"\"\\n            def pl = ctx.payload;\\n            def s = \"\";\\n            for (i in ctx.payload.aggregations.users.buckets) {\\n              s += \\'- \\' + i.key + \\' (\\' + i.doc_count + \\')\\\\\\\\r\\'\\n            }\\n            pl.themeColor = \"0076D7\";\\n            pl.summary = \"4625 User auth attempt with **expired** account!\";\\n            pl.text = s;\\n            return pl; \\n          \"\"\"\\n        }\\n      },\\n       \"webhook\": {\\n        \"scheme\": \"https\",\\n        \"host\": \"outlook.office.com\",\\n        \"port\": 443,\\n        \"method\": \"post\",\\n        \"path\": \"/webhook/id,\\n        \"params\": {},\\n        \"headers\": {\\n          \"Content-Type\": \"application/json\"\\n        },\\n        \"body\": \"{{#toJson}}ctx.payload{{/toJson}}\"\\n      }\\n    }\\n  }'},\n",
       " {'text_field': 'try something like pl[\"@type\"] = \\'MessageCard\\';'},\n",
       " {'text_field': 'Hello,\\nI have somes index with data containing a date field, a geo_point field and a value field.\\nI develop a method with c# and NEST.\\nIt must be able to search  values corresponding to a geo point and a date a multiple time.\\nSo, I have a search request foreach geo_point, for this geo_point, I have multiple date to search on.\\nI saw that with DateRange, we can ask for a range, but how must I do to search for every exact date I have for each geo point ?\\nI tried with the .Terms method but doesn\\'t return me anything.\\nDo you have some advise to have the most fastest result.\\nThank you.\\nHere is my request : (times variable is List)\\n  var searchResults = client.Search&lt;dynamic&gt;(s =&gt; s.AllIndices().From(0).Size(1000).ScriptFields(sf =&gt; sf.ScriptField(\"distance\", d =&gt; d.Source(\"if(doc[\\'geo\\'].size() &gt; 0) { doc[\\'geo\\'].arcDistance(\" + lat.ToString().Replace(\\',\\', \\'.\\') + \",\" + lon.ToString().Replace(\\',\\', \\'.\\') + \")}\")))\\n                                                                                            .DocValueFields(l =&gt; l.Fields(new List&lt;Field&gt;() { new Field(\"value\", null, null), new Field(\"geo\", null, null), new Field(\"date\", null, null) }))\\n                                                                                            .Query(\\n           q =&gt; q.Bool(\\n           b =&gt; b.Must(\\n                    //n =&gt; n.Field(\"forecastDate\").Query())).Filter(\\n                    f =&gt; f.GeoDistance(\\n                     g =&gt; g.Distance(distance, DistanceUnit.Meters).DistanceType(GeoDistanceType.Arc).Field(\"geo\").Location(lat, lon))) \\n                     ) &amp;&amp; q.Terms(t =&gt; t.Field(\"date\").Terms(times))));\\n'},\n",
       " {'text_field': 'I wrote the request with Kibana console with aggregation and use the low level API to execute it.\\nThe low lvl api is really more efficient.'},\n",
       " {'text_field': 'Hello,\\nI work on a map in Vega in Kibana (the map of kibana config=map), and I plot some positions with the projection:\"projection\".\\nI wan\\'t to add areas to show on the map, like timezones. Similar to the region map visualization in Kibana. Moreover Kibana knows information like countries (like in https://maps.elastic.co/#file/world_countries) .\\nSo, how can I add areas?\\nI\\'ve tried used TopoJSON but it use width and heigth, but kibana map doesn\\'t use that.\\nThank you in advance.'},\n",
       " {'text_field': 'I can\\'t use the Map application because I want Region Map and Coordinate Map at same time.\\nBut finally, I succeed using this tutorial (https://vega.github.io/vega/tutorials/airports/) , and drawing boundaries like for the states in the US with a geojson file of the timezones in the tuto. You just need to use the projection automatically created by kibana named \"projection\" instead of defining a custom one.\\nThank you.'},\n",
       " {'text_field': 'Hi, i\\'m trying to set up a cluster with 3 nodes\\nes-node1\\nes-node2\\nes-node3\\ninstalled elasticsearch 7.4 on 3 nodes con configure elasticsearch.yml with the following:\\ncluster.name: es-cluster\\nnode.name: es-node1\\nnode.master: true\\nnode.data: true\\nbootstrap.memory_lock: true\\nnetwork.host: 0.0.0.0\\ndiscovery.seed_hosts: [\"es-node1\"]\\ncluster.initial_master_nodes: [\"es-node1\"]\\n\\nhowever when i start nodes each node starts its own cluster:\\n\\n[root@es-node1 elasticsearch]# curl -XGET http://192.168.90.225:9200/_cluster/health?pretty\\n{\\n  \"cluster_name\" : \"es-cluster\",\\n  \"status\" : \"yellow\",\\n  \"timed_out\" : false,\\n  \"number_of_nodes\" : 1,\\n  \"number_of_data_nodes\" : 1,\\n  \"active_primary_shards\" : 85,\\n  \"active_shards\" : 85,\\n  \"relocating_shards\" : 0,\\n  \"initializing_shards\" : 0,\\n  \"unassigned_shards\" : 44,\\n  \"delayed_unassigned_shards\" : 0,\\n  \"number_of_pending_tasks\" : 0,\\n  \"number_of_in_flight_fetch\" : 0,\\n  \"task_max_waiting_in_queue_millis\" : 0,\\n  \"active_shards_percent_as_number\" : 65.89147286821705\\n}\\n\\n\\nSelinux is in permissive and firewall down to avoid trouble.\\nAny idea?'},\n",
       " {'text_field': 'Finally made it work, had to start each node with a slightliy different configuration\\nnode1:\\ncluster.name: es-cluster\\nnode.name: es-node1\\nnode.master: true\\nnode.data: true\\nbootstrap.memory_lock: true\\nnetwork.host: 192.168.90.225\\n#network.host: 0.0.0.0\\ndiscovery.seed_hosts:\\n - es-node1\\n - es-node2\\n - es-node3\\ncluster.initial_master_nodes:\\n - es-node1\\n\\nnode2: \\ncluster.name: es-cluster\\nnode.name: es-node2\\nnode.master: true\\nnode.data: true\\nbootstrap.memory_lock: true\\nnetwork.host: 192.168.90.226\\n#network.host: 0.0.0.0\\ndiscovery.seed_hosts:\\n - es-node1\\n - es-node2\\n - es-node3\\ncluster.initial_master_nodes:\\n - es-node1\\n - es-node2\\n\\nnode3:\\ncluster.name: es-cluster\\nnode.name: es-node3\\nnode.master: true\\nnode.data: true\\nbootstrap.memory_lock: true\\nnetwork.host: 192.168.90.227\\n#network.host: 0.0.0.0\\ndiscovery.seed_hosts:\\n - es-node1\\n - es-node2\\n - es-node3\\ncluster.initial_master_nodes:\\n - es-node1\\n - es-node2\\n - es-node3'},\n",
       " {'text_field': 'Hi, I recently tried to install Elasticsearch and Kibana 7.4. After starting both, I am receiving the following Red status on Kibana console.\\n\\nimage.png1366×768 62.5 KB\\n\\nThe error message is saying \"This version of Kibana requires Elasticsearch v6.1.2 on all nodes. I found the following incompatible nodes in your cluster: v7.4.0 @ 127.0.0.1:9200 (127.0.0.1)\" but I downloaded 7.4.\\nCan someone help me fix this issue?  Thanks in advance. \\nRegards,\\nJulius'},\n",
       " {'text_field': 'Hi Rashmi,\\nThank you for your reply. I have now found out the issue. Apparently, I have another application running and its using elastic stack as a built in logging functionality.\\nRegards,\\nJulius'},\n",
       " {'text_field': 'I am ingesting 8 different CSV file schemas using 8 logstash pipelines and reading from a Windows file share. IT has been recommending to not read across the file share but use Filebeat.\\nNot finding just a whole bunch of info in this area.\\nHere are 2 of the \\'inputs\\' section:\\n--------------------------------------------------------------\\nMemoryLeak Input\\n\\ntype: log\\nenabled: false\\npaths:\\n\\nC:\\\\MemoryLeakTest\\\\v03*-memleak*.log\\ntags: [\"memleak\"]\\nfields: {log_type: memleak}\\n\\n\\n\\n--------------------------------------------------------------\\nAM-300 TaskStat Input\\n\\ntype: log\\nenabled: false\\npaths:\\n\\nC:\\\\MemoryLeakTest\\\\v03*_am-300-taskstat*.log\\ntags: [\"taskstat-am-300\"]\\nfields: {log_type: taskstat-am-300}\\n\\n\\n\\nThe output is:\\noutput.logstash:\\nThe Logstash hosts\\nhosts: [\":5044\"]\\nNow, how the heck do I get this to work with my 8 pipeline files?\\nI put this at the top of one of the pipeline files:\\ninput {\\nbeats {\\nhost =&gt; \"0.0.0.0\"\\nport =&gt; \"5044\"\\nclient_inactivity_timeout =&gt; 180\\n}\\n}\\nMy question is, does this look right so far?\\nAnd then, how do I split the data across the 8 filters in the 8 pipeline files?\\nThanks, Mike'},\n",
       " {'text_field': 'Logstash syntax is:\\nIf [fields][log_type] == \"memleak\" {\\n     do something\\n}\\n\\nI think you are close '},\n",
       " {'text_field': 'Hi All,\\nI have installed filebeat as a docker container with the below file:\\nfilebeat.config:\\nmodules:\\npath: ${path.config}/modules.d/*.yml\\nreload.enabled: false\\nfilebeat.autodiscover:\\nproviders:\\n- type: docker\\nhints.enabled: true\\nfilebeat.inputs:\\n\\ntype: docker\\nenabled: true\\ncontainers:\\nids:\\n\\n\\'*\\'\\njson.keys_under_root: true\\njson.add_error_key: true\\njson.message_key: log\\nprocessors:\\nadd_docker_metadata: ~\\nsetup.kibana:\\nhost: \"kibana:5601\"\\noutput.logstash:\\nhosts: [\"logstash:5044\"]\\n\\n\\n\\nIts working Ok, question is if I removed line below, its will still be working , why do we need filebeat.input then, can anyone please explain\\nfilebeat.inputs:\\n\\ntype: docker\\nenabled: true\\ncontainers:\\nids:\\n\\n\\'*\\'\\njson.keys_under_root: true\\njson.add_error_key: true\\njson.message_key: log\\n\\n\\n'},\n",
       " {'text_field': 'Hi @Mai_Waly.\\nAs far as I know you should not need the static inputs definition if using Autodiscover. Unless, of course, you want some static settings to be applied.\\nBest regards'},\n",
       " {'text_field': 'Hello,\\nI have a problem with parsing the CSV file in Logstash.\\nThe structure of my template in index looks like this:\\n{\\n\\t\"mappings\": {\\n\\t\\t\"doc\": {\\n\\t\\t\\t\"properties\": {\\n\\n\\t\\t\\t\\t\"ip_src\": { \"type\": \"ip\" },\\n\\t\\t\\t\\t\"ip_dst\": { \"type\": \"ip\" },\\n\\t\\t\\t\\t\"hops_no\": { \"type\": \"byte\" },\\n\\n\\t\\t\\t\\t\"v_route\": {\\n\\t\\t\\t\\t\\t\"type\": \"nested\",\\n\\t\\t\\t\\t\\t\"properties\": {\\n\\t\\t\\t\\t\\t\\t\"vip\":  { \"type\": \"ip\" },\\n\\t\\t\\t\\t\\t\\t\"vdelay\": { \"type\": \"float\"  }\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t},\\n\\n\\t\\t\\t\\t\"m_time\":   { \"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss||epoch_second\"  }\\n\\t\\t\\t}\\n\\t\\t}\\n\\t},\\n\\n\\t\"version\": 1\\n}\\n\\nAn example of one line from an input file is:\\n89.73.142.84;89.73.142.84;\"2002-12-12 12:12:12\";3;89.73.142.84@0.1#89.73.142.85@0.3#89.73.142.86@0.5\\n\\nThe contents of the Logstash configuration file is:\\ninput {\\n\\tfile {\\n\\t\\tpath =&gt; \"/home/user/data/data_pre/*_0\"\\n\\t\\tstart_position =&gt; \"beginning\"\\n\\t\\tsincedb_path =&gt; \"/dev/null\"\\n\\t\\tfile_completed_action =&gt; \"delete\"\\n\\t\\tfile_completed_log_path =&gt; \"/home/user/data/data_post/archive\"\\n\\t}\\n}\\n\\nfilter {\\n\\tcsv {\\n\\t\\tseparator =&gt; \";\"\\n\\t\\tcolumns =&gt; [\"ip_src\", \"ip_dst\", \"m_time\", \"hops_no\", \"vhops\"]\\n\\t}\\n\\n\\truby {\\n\\t\\tcode =&gt; \\'\\n\\t\\t\\tpb = event.get(\"vhops\");\\n\\t\\t\\tb = pb.split(\"#\");\\n\\t\\t\\tary = Array.new;\\n\\t\\t\\tfor c in b;\\n\\t\\t\\t\\tkeyvar = c.split(\"@\")[0];\\n\\t\\t\\t\\tvaluevar = c.split(\"@\")[1];\\n\\t\\t\\t\\td = \"{vip : \" &lt;&lt; keyvar &lt;&lt; \", vdelay : \" &lt;&lt; valuevar &lt;&lt; \"}\";\\n\\t\\t\\t\\tary.push(d);\\n\\t\\t\\tend;\\n\\n\\t\\tevent.set(\"v_route\", ary);\\n\\t\\t\\'\\n\\t}\\n\\n\\tmutate { remove_field =&gt; [ \"message\", \"vhops\" ] }\\n}\\n\\n\\noutput {\\n\\telasticsearch {\\n\\t\\thosts =&gt; [\"http://192.168.X.Y:9200\"]\\n\\t\\tindex =&gt; \"index00\"\\n\\t}\\n\\n\\tstdout {}\\n}\\n\\nThere is a problem with the v_route field.\\nIn Logstash logs I have:\\n...\\n[2019-10-12T14:22:10,380][INFO ][logstash.pipeline        ] Pipeline started successfully {:pipeline_id=&gt;\"main\", :thread=&gt;\"#&lt;Thread:0x1f45a99e run&gt;\"}\\n[2019-10-12T14:22:10,429][INFO ][filewatch.observingtail  ] START, creating Discoverer, Watch with file and sincedb collections\\n[2019-10-12T14:22:10,429][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]}\\n[2019-10-12T14:22:10,699][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n[2019-10-12T14:22:11,319][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"index00\", :_type=&gt;\"doc\", :routing=&gt;nil}, #&lt;LogStash::Event:0x606b0f93&gt;], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"routes_ipv4\", \"_type\"=&gt;\"doc\", \"_id\"=&gt;\"i3pZwG0BJFV1L_wCJ0wp\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"object mapping for [v_route] tried to parse field [null] as object, but found a concrete value\"}}}}\\n\\nI\\'m building the v_route field wrong?\\nWhere is the problem?\\nMy Logstash &amp; ES has version 6.8.'},\n",
       " {'text_field': 'If you change the loop to be\\n        for c in b;\\n            keyvar = c.split(\"@\")[0];\\n            valuevar = c.split(\"@\")[1];\\n            h = Hash.new\\n            h[\"vip\"] = keyvar\\n            h[\"vdelay\"]= valuevar\\n            ary &lt;&lt; h\\n        end;\\n\\nYou will get an array of hashes\\n   \"v_route\" =&gt; [\\n    [0] {\\n           \"vip\" =&gt; \"89.73.142.84\",\\n        \"vdelay\" =&gt; \"0.1\"\\n    },\\n    [1] {\\n           \"vip\" =&gt; \"89.73.142.85\",\\n        \"vdelay\" =&gt; \"0.3\"\\n    },\\n    [2] {\\n           \"vip\" =&gt; \"89.73.142.86\",\\n        \"vdelay\" =&gt; \"0.5\"\\n    }\\n],'},\n",
       " {'text_field': 'Hello,\\nMy Logstash works fine. However, I can\\'t force deleting old files.\\nPart of the configuration file:\\ninput {\\n\\tfile {\\n\\t\\tmode =&gt; read\\n\\t\\tpath =&gt; \"/home/user/data/data_pre/*_0\"\\n\\t\\tstart_position =&gt; \"beginning\"\\n\\t\\tfile_completed_action =&gt; delete\\n\\t}\\n}\\n\\nlogstash-input-file version:\\n...    \\nUpdating logstash-input-file\\nUpdated logstash-input-file 4.1.10 to 4.1.11\\n\\nHow can I force deleting files?\\nfile_completed_action =&gt; delete not work?'},\n",
       " {'text_field': 'Do you have write access to /home/user/data/data_pre? Is that mount writeable?'},\n",
       " {'text_field': 'I use stable/logstash helm chart (chart version 2.3.0, app version 7.1.1) to deploy logstash. I have no issue using elasticsearch output, but always get SIGTERM with s3 output. I don\\'t see any obvious cause of this in the logstash log. Any help would be appreciated.\\nThis is how I configured the elasticsearch output, which works:\\noutputs:\\n  main: |-\\n    output {\\n      elasticsearch {\\n        hosts =&gt; [\"${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}\"]\\n        index =&gt; \"%{[@metadata][type]}-%{+YYYY.MM.dd}\"\\n        manage_template =&gt; true\\n        template_overwrite =&gt; true\\n        template =&gt; \"/usr/share/logstash/files/edge-template.json\"\\n        template_name =&gt; \"logstash-edge\"\\n      }\\n    }\\n\\nelasticsearch:\\n  host: \"es-prod-master\"\\n\\nThis is how I configured s3 output, which always got SIGTERM:\\noutputs:\\n  main: |-\\n    output {\\n      s3 {\\n        access_key_id =&gt; \"xxxxxxx\"\\n        secret_access_key =&gt; \"xxxxxxxx\"\\n        region =&gt; \"eu-central-1\"\\n        bucket =&gt; \"xxxx\"\\n        size_file =&gt; 1048576\\n        time_file =&gt; 5\\n        prefix =&gt; \"%{+yyyy.MM.dd.HH.mm}\"\\n      }\\n    }\\n\\nFollowing is the logstash log when SIGTERM is received using s3 output:\\n2019/10/13 04:55:04 Setting \\'queue.max_bytes\\' from environment.\\n2019/10/13 04:55:04 Setting \\'path.config\\' from environment.\\n2019/10/13 04:55:04 Setting \\'queue.drain\\' from environment.\\n2019/10/13 04:55:04 Setting \\'http.port\\' from environment.\\n2019/10/13 04:55:04 Setting \\'http.host\\' from environment.\\n2019/10/13 04:55:04 Setting \\'path.data\\' from environment.\\n2019/10/13 04:55:04 Setting \\'queue.checkpoint.writes\\' from environment.\\n2019/10/13 04:55:04 Setting \\'pipeline.batch.size\\' from environment.\\n2019/10/13 04:55:04 Setting \\'queue.type\\' from environment.\\n2019/10/13 04:55:04 Setting \\'pipeline.workers\\' from environment.\\n2019/10/13 04:55:04 Setting \\'config.reload.automatic\\' from environment.\\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\\nWARNING: An illegal reflective access operation has occurred\\nWARNING: Illegal reflective access by com.headius.backport9.modules.Modules (file:/usr/share/logstash/logstash-core/lib/jars/jruby-complete-9.2.7.0.jar) to field java.io.FileDescriptor.fd\\nWARNING: Please consider reporting this to the maintainers of com.headius.backport9.modules.Modules\\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\\nWARNING: All illegal access operations will be denied in a future release\\nThread.exclusive is deprecated, use Thread::Mutex\\nSending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties\\n[2019-10-13T04:55:16,870][WARN ][logstash.config.source.multilocal] Ignoring the \\'pipelines.yml\\' file because modules or command line options are specified\\n[2019-10-13T04:55:16,880][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"7.3.0\"}\\n[2019-10-13T04:55:18,362][INFO ][org.reflections.Reflections] Reflections took 33 ms to scan 1 urls, producing 19 keys and 39 values\\n[2019-10-13T04:55:22,420][WARN ][logstash.runner          ] SIGTERM received. Shutting down.'},\n",
       " {'text_field': \"I've found the issue. This is due to the pod liveness check in K8s, as pointed out by https://stackoverflow.com/questions/56593504/receive-sigterm-on-logstash-startup-version-7-1-1/56784774#56784774\"},\n",
       " {'text_field': 'I am reading through https://www.elastic.co/what-is/open-x-pack but I don\\'t see ILM specifically mentioned.  Just wanted to confirm whether it is too new a feature and it came out after or if it is actually not available in the free X-Pack\\nActually, what is the free X-Pack anyway in the context of \"Docker\" because when I was building https://github.com/trajano/elk-swarm I was under the impression that X-Pack was a paid feature so I tried to disable it.'},\n",
       " {'text_field': 'Whether you download a Elasticseach directly or use a docker image, there are two distributions. The OSS distribution contains only open-source code and does not include any other components. The default distribution comes bundled with X-Pack, and the license level you are using determines the feature set available. The default here is the free Basic license, but you can upgrade that to a commercial license without having to change distribution/software.'},\n",
       " {'text_field': 'Hi,\\nI have a number of dangling indices across the cluster which all contain data that is no longer required. Is it safe to simply delete the particular directories under path.data that these dangling indices refer to, whilst the node is running? Or do I need to stop the node first?\\nThanks.'},\n",
       " {'text_field': \"The best thing to do is allow the dangling indices to be imported and then delete them through the API.\\nI would not recommend making any manual changes to the contents of the data path. Certainly not when the node is running, but even when it's down it's a risky business.\"},\n",
       " {'text_field': 'I have configured the Kibana being connected to ES 7.3.0 with HTTPS enabled. But I got this error \"java.io.IOException: Host name \\'localhost\\' does not match the certificate subject provided by the peer (CN=Elastic Certificate Tool Autogenerated CA)\"\\nActually I encountered the same issue in Kibana and then I set this in the kibana.yml and it worked: elasticsearch.ssl.verificationMode: certificate\\nI did not find the corresponding setting in fscrawler: https://fscrawler.readthedocs.io/en/latest/admin/fs/elasticsearch.html#ssl-configuration\\nAny one managed to do that?\\nThanks'},\n",
       " {'text_field': \"\\n\\n\\n Nee_Defeng:\\n\\nCaused by: javax.net.ssl.SSLPeerUnverifiedException: Host name 'localhost' does not match the certificate subject provided by the peer (CN=Elastic Certificate Tool Autogenerated CA)\\n\\n\\nCan you share your elasticsearch configuration ? It seems like you are using the CA certificate for TLS on the http layer instead of the node-sni.p12\"},\n",
       " {'text_field': 'お世話になっております。\\n現在、以下のようなnestedフィールドのインデックスを作成し、\\nPUT my_index\\n{\\n  \"mappings\": {\\n    \"properties\": {\\n      \"user\": {\\n        \"type\": \"nested\" \\n      }\\n    }\\n  }\\n}\\n\\nnested内のオブジェクト数が1,2,3となるようにドキュメントを入れた際に、\\nPUT my_index/_doc/1\\n{\\n  \"user\" : [\\n    {\\n      \"first\" : \"John\",\\n      \"last\" :  \"Smith\"\\n    }\\n  ]\\n}\\n\\nPUT my_index/_doc/2\\n{\\n  \"user\" : [\\n    {\\n      \"first\" : \"John\",\\n      \"last\" :  \"Smith\"\\n    },\\n    {\\n      \"first\" : \"Alice\",\\n      \"last\" :  \"White\"\\n    }\\n  ]\\n}\\n\\nPUT my_index/_doc/3\\n{\\n  \"user\" : [\\n    {\\n      \"first\" : \"John\",\\n      \"last\" :  \"Smith\"\\n    },\\n    {\\n      \"first\" : \"Alice\",\\n      \"last\" :  \"White\"\\n    },\\n    {\\n      \"first\" : \"hoge\",\\n      \"last\" :  \"fuga\"\\n    }\\n  ]\\n}\\n\\nこれらをuserフィールド内のオブジェクト数でソートをして取得することは可能でしょうか？\\n※Elasticsearchのバージョンは6.5になります\\nお手数をおかけしますが、もし方法がありましたらご教示頂ければ幸いです。\\n以上、よろしくお願い致します。'},\n",
       " {'text_field': 'https://www.elastic.co/guide/en/elasticsearch/reference/6.5/search-request-sort.html#_script_based_sorting\\nscript_based_sortingはどうでしょうか？提示されたテストデータだと期待したような結果になるかと思います。\\nPOST my_index/_search\\n{\\n  \"sort\": {\\n    \"_script\": {\\n      \"type\": \"number\",\\n      \"order\": \"desc\",\\n      \"script\": \"params._source[\\'user\\'].size()\"\\n    }\\n  }\\n}\\n\\n'},\n",
       " {'text_field': 'お世話になります。\\n一部のDashboardでPDF Reportが失敗します。\\n成功するDashboardもあるので基本的な設定ができていないわけではないと思っています。\\nエラーメッセージとして、ERR_CONNECTION_TIMED_OUT が表示されていました。\\nこの解決策として、xpack.reporting.queue.timeout の設定を600000に設定してみたのですが、\\n設定が効いていないように見えます。\\nDashboard → PDF Report → Generate PDFボタン押下から、画面右下に失敗のポップアップがでるまでの時間をあくまで画面表示ベースですが計測したところ、\\nxpack.reporting.queue.timeout のデフォルト値の120000（2分）前後で失敗となるようです。\\n設定方法などに誤りはありますでしょうか。\\n原因、解決策などご教示いただけますと幸いです。\\nエラーメッセージ\\nERR_CONNECTION_TIMED_OUT\\n\\nunable to generate report.jpg705×124 15.2 KB\\n\\n動作環境\\n\\nElasticsearch 7.3.0 (on Elastic Cloud)\\nKibana 7.3.0 (on nginx - EC2(Ubuntu 16.04 LTS) )\\n\\nkibana.yml（抜粋）\\nxpack.reporting.encryptionKey: \"my_encryptionKey\"\\nxpack.reporting.queue.timeout: 600000\\nxpack.reporting.kibanaServer.port: 80\\nxpack.reporting.kibanaServer.protocol: http\\nxpack.reporting.kibanaServer.hostname: \"my_domain\"\\nxpack.reporting.kibanaApp: \"/app/kibana\"\\n補足\\n\\nごくまれにですが、下記エラーで失敗する事もあります。原因がわかりません。\\nError: Unable to spawn Chromium: [Error: Page crashed!]\\n\\n\\n上記EC2上のKibanaのほかに、Elatic Cloud上でもKibanaを動作させていますが、下記設定は同様に入れております。\\nxpack.reporting.encryptionKey: \"my_encryptionKey\"\\nxpack.reporting.queue.timeout: 600000\\n'},\n",
       " {'text_field': '解決いたしました。\\nどうやら、リソース不足が原因だったようです。\\nEC2のインスタンスタイプも、Elastic CloudのKibanaのリソース割り当ても、\\nどちらもほぼ最小構成で動かしていたために発生していたようです。\\n該当環境をスケールアップすることで、問題なくPDF Reportすることができました。\\nご回答頂きありがとうございました。\\nお騒がせしました。'},\n",
       " {'text_field': \"I'm getting Sing In issue after encryption setup for Clusters. I have no idea how to remove password or how to set password.\\nElastic Version : 7.4\"},\n",
       " {'text_field': 'This issue was passed. I was generated password in non master nodes. And another issue is coming. I got 403 forbidden error when I log in in Kibana. I already set elastic password in kibana.yml'},\n",
       " {'text_field': 'Hi,\\nWe are running a 5 node cluster with 3 data nodes (where 1 is master eligible) and 2 master (non-data eligible nodes) as windows services.\\nEach of the data node has 6 GB allocated heap memory and runs on machines with 20 GB RAM available.  (from startup log: heap size [5.8gb], compressed ordinary object pointers [true])\\nAfter a  long period of stability we decided a couple of days ago to enable HTTP compression from the client we use for requesting ES data (NEST .net client). Later the same night we experienced unexpected shutdowns of the data nodes. This has happened several times since then.  In the logs I can find that the shutdown is caused by a OOM (OutOfMemoryError: Direct buffer memory).\\nThe monitoring graphs does not indicate that we are using all the allocated heap memory when the exception occurs:\\n\\n2019_10_31_08_18_35_Stack_Monitoring_elasticsearch_production_7_3_Elasticsearch_Nodes_krs02e.png902×279 23.1 KB\\n\\nAny help is highly appreciated!'},\n",
       " {'text_field': \"@Odd_Erik_Gronberg\\nyea it looks like it's just 512M indeed (that would be the auto-calculated value based on 1G heap).\\n=&gt; Yes, I'd try setting it higher to 2 or 3 GB. 512M might be a little low indeed.\"},\n",
       " {'text_field': 'Hi All,\\nGood afternoon\\nI have tried userauthenication  by referring the docs\\nhttps://www.elastic.co/guide/en/elasticsearch/reference/7.4/built-in-users.html\\nhad run the command\\n\\nbin/elasticsearch-setup-passwords interactive\\n\\nreset all the users password to \"123456\"\\nNow when I try to login to the kiaba interface with \"user\" \"kibana\" with password \"123456\" , it is not allowing me to login, it is saying  message as\\n\\n\\n\\n\\n\\nstatusCode\\n403\\n\\n\\n\\n\\nerror\\n\"Forbidden\"\\n\\n\\nmessage\\n\"Forbidden\"\\n\\n\\n\\n\\n\\nScreen shot attached!,\\nGuidance requested on pointing out where I could have gone wrong\\n\\nKibanaUserNotallowing.png926×288 14.2 KB\\n\\nKibana version:\\n7.4.1\\t\\nElasticsearch version:\\n7.4.1\\nAPM Server version:\\n7.4.1\\n**filebeat version **\\n7.4.1\\nAPM Agent language and version:\\nNA\\nLogstash version\\n7.4.1-1\\nBrowser version:\\nFirefox 70'},\n",
       " {'text_field': 'Solved\\nYou should login as \"elastic\"\\nnot as kibana user'},\n",
       " {'text_field': 'logstashを使用してElasticsearch Service上のdeploymentに対して\\nindexの作成や更新等を行いたいと考えています。\\ndocker等でサーバを立ててlogstashのイメージをpullしてきた場合、logstashからindexをElasticsearch Service上のdeploymentへ流し込む際の指定はlogstash.yml上にCloud IDとUser ID、PWだけでよいのでしょうか。\\nElasticsearchのendpoint URLの指定は特に必要ないのでしょうか。\\nlogstashとElasticsearch Serivice上のdeploymentでどのように疎通を行うのでしょうか。'},\n",
       " {'text_field': 'outputのelasticsearcにhosts、ユーザ名、パスワードを指定しておくのが良いと思います。\\noutput {\\n    \\n    elasticsearch {\\n        hosts =&gt; [\"https://123456789asdfhujiko.eastus2.azure.elastic-cloud.com:9243\"]\\n        user =&gt; \"elastic\"\\n        password =&gt; \"tadashiipassword\"\\n    }\\n\\n}\\n\\n\\n  \\n    \\n    \\n    How to define cloud id in logstash config file? Logstash\\n  \\n  \\n    Will this \"data-out.conf\" find the cloud ES instance by checking against main \"logstash.conf\"? \\n\\nI assume you mean .yml, not .conf ? (if so...) \\nThe Elasticsearch settings in logstash.yml are only applicable to monitoring, management, and modules. You still have to define your hosts (and credentials if needed) in the output { section of your configuration [1]. \\nIf have 1 instance/cluster of Elasticsearch , it does become a bit redundant to redefine the same connection everywhere. Also note, the…\\n  \\n\\n\\nこちらのElasticメンバーの回答をみると、cloud.idやauthは、モニタリングやモジュール用であって、outputには直接hostを書いてね、と書いてあるように見えるからです。\\n\\nyou will need to use the host name(s) directly.\\n\\nご参考になれば幸いです。'},\n",
       " {'text_field': 'Elasticsearch and Kibana version 7.4.1 (same machine)\\nBuild type RPM\\nHello! Just some background - I have gotten Active Directory authentication working for Kibana, I can log in using AD username/password. I have also set up PKI authentication between Elasticsearch and Kibana and it works. Elasticsearch is also operating in FIPS 140-2 mode successfully.\\nI am having an issue where I cannot send a cURL POST command to Elasticsearch using AD credentials via the terminal. I am able, however, to log into Kibana using an AD user and use the developer tools section to create a POST command and it works just fine. I am also able to use a cURL POST command from the terminal and specify the local server\\'s PKI certificate &amp; key and this works just fine.\\nThe cURL command I am using is:\\ncurl -u  POST \":9200/es_test_1/messages\" -H \\'Content-Type: application/json\\' -d\\'{  \"text\": \"Testing 1234\"}\\nI enter my password and I get this error: SSL certificate problem: unable to get local issuer certificate\\nIf I change the command to include the --cacert  option, I get this error: OpenSSL SSL_read: error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate, errno 0\\nI\\'m not sure why cURL is attempting to use PKI authentication here. I have added the --ntlm option on a cURL different try and got the same bad certificate error. I also have tried --ntlm-proxy, --proxy-user, &amp; --proxy options while specifying my Windows domain controller. I can see the Elasticsearch is connecting to the domain controller by using netstat on the domain controller. However, nothing happens after I put in my domain password.\\n\\nTrying (IP of DC):636...\\nTCP_NODELAY set\\nConnected to (FQDN of DC) (IP) port 636 (#0)\\nallocate connect buffer!\\nEstablish HTTP proxy tunnel to (elasticsearch):9200\\nProxy auth using NTLM with user \\'(domainUser)\\'\\nCONNECT (elasticsearch):9200 HTTP/1.1\\nHost: (elasticsearch):9200\\nProxy-Authorization: NTLM TlRMTVNTUAABAAAABoIIAAAAAAAAAAAAAAAAAAAAAAA=\\nUser-Agent: curl/7.66.0\\nProxy-Connection: Keep-Alive\\n\\n\\n\\nAnd that\\'s all that happens. It never finishes the HTTP request.\\nDoes anyone have any insight into this? Seen this before?\\nPlease let me know if more information is needed.\\nThanks!\\n\\nTyler\\n'},\n",
       " {'text_field': \"\\n\\n\\n Ty9000:\\n\\nElasticsearch is also operating in FIPS 140-2 mode successfully.\\n\\n\\nSlightly unrelated to this topic, and mostly out of curiosity , which Crypto Provider do you use for your FIPS 140 setup ?\\n\\n\\n\\n Ty9000:\\n\\nI'm not sure why cURL is attempting to use PKI authentication here.\\n\\n\\nIt would become obvious if you share your configuration, but it sounds like you have xpack.security.http.ssl.client_authentication: required instead of xpack.security.http.ssl.client_authentication: optional in your configuration\\nIf this is not the case, please share your configuration with us and relevant parts from the Elasticsearch.log when you attempt to authenticate with cURL.\\n\\n\\n\\n Ty9000:\\n\\nI'm not sure why cURL is attempting to use PKI authentication here. I have added the --ntlm option on a cURL different try and got the same bad certificate error. I also have tried --ntlm-proxy, --proxy-user, &amp; --proxy options while specifying my Windows domain controller.\\n\\n\\nYou shouldn't need any of these. Elasticsearch will take care of validating your users credentials against your domain controller.\"},\n",
       " {'text_field': \"I'm installing the second instance (step 3 here: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-topology-example2.html) after the first instance seemed to install without problems.\\nThe second instance complains:\\nChecking runner ip connectivity... FAILED\\nCan't connect $RUNNER_HOST_IP [10.5.3.195:22000]: Connection refused\\n[...]\\nErrors have caused Elastic Cloud Enterprise installation to fail\\nSome of the prerequisites failed: [runner ip connectivity], please fix before continuing\\n\\nI checked on the first instance and there isn't anything listening on this port:\\nnetstat -anpt|grep LISTEN|grep 22000\\nAny advice?\\nPossibly relevant context. I'm using r5.xlarge instances. I had to manually set up docker as the cloud-init script to do this failed.\\nFailed running /var/lib/cloud/scripts/per-instance/00-format-drives-enable-docker\\nPossibly because of the storage being /dev/nvme1n1\\nInstead I manually did parted mklabel, mkpart, mkfs.xfs, mkdir /mnt/data, install /mnt/data, mount, make the sysctl edits, systemctl restart docker...\"},\n",
       " {'text_field': \"So we use HOST_IP as an internal environment variable to pass around the IP of the host currently being installed, eg $host2 in the discussion above\\nIt looks like you are maybe overriding it in your scripting! It should work if you either:\\n\\nUnset it\\nSet it to $host2's IP\\nUse --host-ip in the install CLI\\n\"},\n",
       " {'text_field': 'My query looks like this\\n{\\n\"query\": {\\n\"bool\": {\\n\"must\": [\\n{\\n\"has_parent\": {\\n\"parent_type\": \"doc\",\\n\"query\": {\\n\"bool\": {\\n\"must\": [\\n{\\n\"terms\": {\\n\"id\": [\\n713\\n]}},\\n{\\n\"range\": {\\n\"created\": {\\n\"lte\": \"now/d\"\\n}}},\\n{\\n\"range\": {\"expires\": { \"gte\": \"now/d\"\\n}}}]\\n}}}},\\n{\\n\"term\": {\\n\"doc_type\": \"item\"\\n}},\\n{\"bool\": {\"should\": [\\n{\\n\"term\": {\\n\"have_prices\": true}},\\n{\\n\"term\": {\\n\"is_folder\": true\\n}}]}}],\\n\"must_not\": {\\n\"exists\": {\\n\"field\": \"folder\"\\n}}}\\n},\\n\"sort\": [\\n{\\n\"is_folder\": {\\n\"order\": \"desc\"}},\\n{\\n\"title_low.order\": {\\n\"order\": \"asc\"\\n}\\n}\\n],\\n\"size\": 1000\\n}\\nand I get some result, like this one\\n\"hits\": {\\n\"total\": 19,\\n\"max_score\": null,\\n\"hits\": [\\n{\\n\"_index\": \"prices\",\\n\"_type\": \"doc\",\\n\"_id\": \"item-6800004\",\\n\"_score\": null,\\n\"_routing\": \"1\",\\n\"_source\": {\\n\"id\": 6800004,\\n\"id_pricedoc\": 713,\\n\"title\": \"\"водка №1\" 1\",\\n\"title_low\": \"\"водка №1\" 1\",\\n\"supplier\": {\\n\"id\": 7831,\\n\"type\": null\\n},\\n\"supplier_nom\": {\\n\"id\": 1375697,\\n\"market_nom\": {\\n\"id\": null\\n},\\n\"codes\": null,\\n\"sup_code\": \"7a6713a5-73c1-3acb-9b62-9e38b2314dce\",\\n\"manufacturer\": {\\n\"id\": null,\\n\"title\": null\\n}\\n},\\n\"is_folder\": false,\\n\"folder\": null,\\n\"path\": null,\\n\"pricedoc_created\": \"2016-03-21\",\\n\"prices\": [\\n{\\n\"currency\": \"RUR\",\\n\"id_offer\": 15735967,\\n\"id_prcknd\": 167,\\n\"value\": \"391.50\"\\n}\\n],\\n\"have_prices\": true,\\n\"market\": {\\n\"title\": null,\\n\"uuid\": null,\\n\"folder\": null,\\n\"path\": null\\n},\\n\"_join_field_name\": \"doc_type\",\\n\"doc_type\": {\\n\"name\": \"item\",\\n\"parent\": \"doc-713\"\\n}\\n},\\n\"sort\": [\\n0,\\n\"\"водка №1\" 1\"\\n]\\n}\\nNow I would like to modify query that will matches docs witch only \"id_prcknd\": 167. I wrote something like this\\n{\\n\"query\": {\\n\"bool\": {\\n\"must\": [\\n{\\n\"has_parent\": {\\n\"parent_type\": \"doc\",\\n\"query\": {\\n\"bool\": {\\n\"must\": [\\n{\\n\"terms\": {\\n\"id\": [\\n713\\n]\\n}\\n},\\n{\\n\"range\": {\\n\"created\": {\\n\"lte\": \"now/d\"\\n}\\n}\\n},\\n{\\n\"range\": {\\n\"expires\": {\\n\"gte\": \"now/d\"\\n}}}]}}}},\\n{\\n\"term\": {\\n\"doc_type\": \"item\"\\n}\\n},\\n{\\n\"bool\": {\\n\"should\": [\\n{\\n\"term\": {\\n\"have_prices\": true\\n}\\n},\\n{\\n\"term\": {\\n\"is_folder\": true\\n}}]}\\n}\\n],\\n\"must_not\": {\\n\"exists\": {\\n\"field\": \"folder\"\\n}\\n}\\n},\\n\"nested\": {\\n\"path\": \"prices\",\\n\"query\": {\\n\"bool\": {\\n\"must\": [\\n{\\n\"match\": {\\n\"prices.id_prcknd\": 167\\n}}]}\\n}\\n},\\n\"sort\": [\\n{\\n\"is_folder\": {\\n\"order\": \"desc\"\\n}\\n},\\n{\\n\"title_low.order\": {\\n\"order\": \"asc\"\\n}\\n}\\n],\\n\"size\": 1000\\n}\\nBut I got an error Elasticsearch malformed query, expected [END_OBJECT] but found [FIELD_NAME] Where am I wrong? I wanna match objects where \"id_prcknd\": 167'},\n",
       " {'text_field': 'You can\\'t just stick a nested query inside the query clause. It has to be part of one of the bool query\\'s clauses - in your case the must clause.\\nBy the way, instead of must you may want to consider using filter. It will be faster as it does not cause Elasticsearch to calculate scores, and it may lead to caching.\\nThe resulting query would become:\\nGET my_index/_search\\n{\\n  \"query\": {\\n    \"bool\": {\\n      \"filter\": [\\n        {\\n          \"has_parent\": {\\n            \"parent_type\": \"doc\",\\n            \"query\": {\\n              \"bool\": {\\n                \"filter\": [\\n                  {\\n                    \"terms\": {\\n                      \"id\": [\\n                        713\\n                      ]\\n                    }\\n                  },\\n                  {\\n                    \"range\": {\\n                      \"created\": {\\n                        \"lte\": \"now/d\"\\n                      }\\n                    }\\n                  },\\n                  {\\n                    \"range\": {\\n                      \"expires\": {\\n                        \"gte\": \"now/d\"\\n                      }\\n                    }\\n                  }\\n                ]\\n              }\\n            }\\n          }\\n        },\\n        {\\n          \"term\": {\\n            \"doc_type\": \"item\"\\n          }\\n        },\\n        {\\n          \"bool\": {\\n            \"should\": [\\n              {\\n                \"term\": {\\n                  \"have_prices\": true\\n                }\\n              },\\n              {\\n                \"term\": {\\n                  \"is_folder\": true\\n                }\\n              }\\n            ]\\n          }\\n        },\\n        {\\n          \"nested\": {\\n            \"path\": \"prices\",\\n            \"query\": {\\n              \"bool\": {\\n                \"filter\": [\\n                  {\\n                    \"match\": {\\n                      \"prices.id_prcknd\": 167\\n                    }\\n                  }\\n                ]\\n              }\\n            }\\n          }\\n        }\\n      ],\\n      \"must_not\": {\\n        \"exists\": {\\n          \"field\": \"folder\"\\n        }\\n      }\\n    }\\n  },\\n  \"sort\": [\\n    {\\n      \"is_folder\": {\\n        \"order\": \"desc\"\\n      }\\n    },\\n    {\\n      \"title_low.order\": {\\n        \"order\": \"asc\"\\n      }\\n    }\\n  ],\\n  \"size\": 1000\\n}\\n\\nAlso, please format your code in this forum using the &lt;/&gt; button. It makes it much easier to read and as a result it\\'s much more likely someone will help you. '},\n",
       " {'text_field': 'Hi Elastic Sorcerers,\\nI’ve recently set up a network monitoring system where data about network activity is archived in an Elasticsearch v 7.4.0 Docker container.  On the same physical host (Ubuntu 16.04), I also have a Kibana v 7.4.0 Docker container, which manages and displays the ES data.  As I’m new to the Elastic suite of products, everything is pretty much running with minimal (and default) configurations.\\nThe Kibana application works great… except I’ve noticed there is about a three minute lag between when data arrives in Elasticsearch and when that same data appears in my Kibana dashboard.\\nIn other words:  Suppose I run a test where Host A sends some traffic to Host B across my network.  When I run this test and then do an SQL-like query against Elasticsearch, I can see data about the traffic appear almost immediately.  But I must wait three minutes before I see that same data appear in my Kibana Visualizations.\\nI’ve run a lot of tests, and there is no discrepancy between the ES-reported data and the Kibana-reported data.  The only issue is that Kibana seems to be three minutes behind Elasticsearch.\\nI hope to use my system to monitor network activity in real time, so this three minute lag is a serious problem.  But I have no idea where to start troubleshooting.\\nAs I mentioned, I set up my ES / Kibana containers using a tutorial, doing a minimal configuration.  I’m sure I’m using a lot of default settings.  Is this a Kibana configuration issue, and if so, where would I look?\\nOr could this be a system problem?  When I do a simple “uptime” on the command line of both containers, it seems that they have the same system time.\\nAny advice or help is appreciated!  Thanks….'},\n",
       " {'text_field': \"Hi Nathan,\\nI'm going to repost this question with more specific details.  Thank you for your help, I do appreciate it.  \"},\n",
       " {'text_field': \"Hello,\\nI installed Elasticsearch and Kibana 7.4 on a VM running Ubuntu 18.04.  Following the instructions in the guide for that version.  After finishing I set this single option in kibana.yml\\nserver.host:0.0.0.0\\nFrom what I have read this should cause Kibana to start listening on that address.  However a netstat shows it is only still listening to localhost.  I've restarted the service, the VM, and tried setting the value for server.host to the VMs IP address, with and without quotes.  In my logs, the service explicitly states its running at http://localhost:5601\\nI have also left the default for the server.host and tried adjusting the port.  And the server will not recognize this change either, remaining 5601.\\nAm I missing something?\"},\n",
       " {'text_field': 'So, I had a co-worker lend a second set of eyes.  He noticed that every other configuration option had a whitespace after the colon.  I added a whitespace, and it started the listener on the correct network/interface.\\nI did not realize the yaml tokenizer was  that picky.  Sorry for wasting your time.  Thanks Nathan.'},\n",
       " {'text_field': 'Hello,\\nI\\'m using ELK 6.8.2.\\nI have a set of data where the field \"system\"  can either have a value or doesn\\'t exist at all.\\nThe queries NOT _exists_: system and system: value works fine. However,  as soon as I try to combine them at a single query NOT _exists_: system OR system: value, ES returns 0 results.\\nWhat\\'s wrong with the query  and how can I get the desired results?'},\n",
       " {'text_field': \"When using multiple boolean operators, it's a good idea to use parentheses as these operators do not honor the usual precedence rules. Try writing your query like this:\\n(NOT _exists_: system) OR (system: value)\\n\"},\n",
       " {'text_field': 'What is the best way to persist data between two messages using the ruby filter?  For example to include the previous message parsed I can use global variables like this:\\ninput {\\n  generator {\\n    lines =&gt; [\\n      \"line 1\",\\n      \"line 2\",\\n      \"line 3\"\\n    ]\\n    count =&gt; 1\\n  }\\n}\\n\\nfilter {\\n  ruby {\\n    init =&gt; \"$last_event = \\'\\'\"\\n    code =&gt; \"\\n      event.set(\\'last_message\\', $last_event) unless $last_event.empty?\\n      $last_event = event.get(\\'message\\') unless event.get(\\'message\\').empty?\\n      \"\\n  }\\n}\\n\\noutput { stdout { codec =&gt; rubydebug } }\\n\\nOutput:\\n{\\n        \"sequence\" =&gt; 0,\\n    \"last_message\" =&gt; \"line 1\",\\n         \"message\" =&gt; \"line 2\",\\n        \"@version\" =&gt; \"1\",\\n            \"host\" =&gt; \"de9b70b8812b\",\\n      \"@timestamp\" =&gt; 2019-10-31T14:36:11.669Z\\n}\\n{\\n        \"sequence\" =&gt; 0,\\n    \"last_message\" =&gt; \"line 2\",\\n         \"message\" =&gt; \"line 3\",\\n        \"@version\" =&gt; \"1\",\\n            \"host\" =&gt; \"de9b70b8812b\",\\n      \"@timestamp\" =&gt; 2019-10-31T14:36:11.669Z\\n}\\n{\\n    \"@timestamp\" =&gt; 2019-10-31T14:36:11.648Z,\\n      \"@version\" =&gt; \"1\",\\n      \"sequence\" =&gt; 0,\\n          \"host\" =&gt; \"de9b70b8812b\",\\n       \"message\" =&gt; \"line 1\"\\n}\\n\\nI understand the limitations of message ordering but was wondering what the recommended way of doing this was?  I\\'d rather not use global variables unless absolutely necessary.\\nThanks!'},\n",
       " {'text_field': \"\\n\\n\\n jong99:\\n\\nI'd rather not use global variables unless absolutely necessary.\\n\\n\\nYou should never need to use global variables. You can do this using an instance variable (@last_event). You can only use a single worker thread, so the solution does not scale, and you have to use the ruby execution engine because with java_execution enabled events get re-ordered.\"},\n",
       " {'text_field': 'We\\'ve been trying to create a pipeline for logs from a security tool using the recently released CEF module, but we\\'ve been getting an error about the log format and its parsing.\\nThe tool would pull its logs via an API call and then it will send it over syslog to a localhost. Filebeat with the CEF module would run on the same host listing on the syslog port.\\nSo, we\\'re seeing this.\\n$sudo lsof -i :514\\n\\nvendor_agent 1132 daemon 8u  IPv4  17481 0t0  UDP localhost:53376-&gt;localhost:syslog \\nfilebeat  2281    root   6u  IPv4  21113 0t0  UDP localhost:syslog \\n\\nNext, we looked into the the log format being generated by the vendor tool, and it looks like this, good we want CEF formatted logs.\\n\"CEF:0|Vendor|1.0|xxx|xxx|1|cat=AuthActivityAuditEvent destinationTranslatedAddress=xxxx duser=xxxx deviceProcessName=xxx Authentication cn3Label=Offset cn3=11105 outcome=true\\\\n\"\\n\\nFinally, reading this document: https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-cef.html, and just looking into the first paragraph, this should work.\\n....This is a module for receiving Common Event Format (CEF) data over Syslog. When messages are received over the syslog protocol the syslog input will parse the header and set the timestamp value. Then the  decode_cef  processor is applied to parse the CEF encoded data. The decoded data is written into a  cef  object field. Lastly any Elastic Common Schema (ECS) fields that can be populated with the CEF data are populated.\\nBut, we\\'re getting an error for the above pipeline.\\n\\nERROR   [syslog]    syslog/input.go:134     can\\'t parse event as syslog rfc3164     {\"message\": xxxxxxxx}\\n'},\n",
       " {'text_field': \"The syslog input is failing to parse the syslog header. We're seeing this problem a lot because Filebeat's syslog input is too strict and only supports BSD-style RFC3164 messages.\\nIn your case it might be related to the date format that your CEF exporter is using. Do you have a config option to change it?\\nAs an alternative, you can modify the module to use the udp input instead of the syslog input, which does no parsing. See this message:\\n\\n  \\n    \\n    \\n    Filebeat syslog parse error Beats\\n  \\n  \\n    I don't have an ASA Firewall to play with, but by looking at a manual I found online, it doesn't look like you can change the timestamp format. We have to update the syslog message parser to make it support more formats. \\nIn the mean time, I had some success by doing this change: \\ndiff --git a/x-pack/filebeat/module/cisco/asa/config/input.yml b/x-pack/filebeat/module/cisco/asa/config/input.yml\\nindex 32e87abc8..9d23b77f2 100644\\n--- a/x-pack/filebeat/module/cisco/asa/config/input.yml\\n+++ b/x-pack/…\\n  \\n\\n\\nThe file you need to change is module/cef/log/config/input.yml under /usr/share/filebeat/....\"},\n",
       " {'text_field': \"Am I missing something here or are the directions to change the Snapshot settings interval just that poor.  The 3rd, 4h and 5th steps dont even seem to exist when I'm on the deployment that I want to snapshot.  I don't need the default 30 minute settings for this deployment.\\nhttps://www.elastic.co/guide/en/cloud-enterprise/current/ece-snapshots.html#ece-change-snapshot-interval\"},\n",
       " {'text_field': 'Version 2.3\\nAlex,\\nYou\\'re correct, I started working on this right before I left for work and got so focused on looking for the Snapshot settings link that I didnt read all of the line on the Snapshot page.  \" After you have added a snapshot repository, configure the Elasticsearch cluster to send them to the repository and select the snapshot interval.\"  A quick update to the docs would be nice but it is on the ECE admin page.\\nNow that I\\'ve found that part is there a way to set a time for the snapshot to take place?  We only need daily for one index and would prefer off hours if possible.\\nRyan'},\n",
       " {'text_field': 'Hi, I recently start to work on a JSON&gt; logstash &gt; influx pipeline , I wonder if there is a way to make logstash to read every numeric value from JSON into float and write them into Influx. For now, some of the fields are Integer and others are Float.\\nThank you'},\n",
       " {'text_field': \"You can convert every top level integer to a float using\\n    ruby {\\n        code =&gt; '\\n            event.to_hash.each { |k, v|\\n                if v.is_a? Integer\\n                    event.set(k, v.to_f)\\n                end\\n            }\\n        '\\n    }\\n\\nThat will not be limited that were parsed from the JSON.\"},\n",
       " {'text_field': \"I have an index with multiple geo_point data types that I'd like to visualize in Maps using the new point to point data source.  However, it tells me Selected index pattern does not contain source and destination fields.  Do I need to reindex my data with another field type or create a scripted field?\"},\n",
       " {'text_field': 'I looked into this closer and it is the second one: your mapping and index pattern must contain two geo_point fields such as this:\\n{\\n  \"source\": { \"type\": \"geo_point\" },\\n  \"dest\": { \"type\": \"geo_point\" },\\n}\\n\\nIf your mapping contains that, but your index pattern doesn\\'t, then you need to refresh your index pattern in the Management section.\\nBased on the screenshot here, this would let you connect the centroids of \"source\" and \"dest\" per tile: https://www.elastic.co/guide/en/kibana/current/point-to-point.html'},\n",
       " {'text_field': 'below is the error i get while running logstash\\n\\nSending Logstash logs to C:/busapps/rrsb/gbl1/logstash/7.0.0/logs which is now configured via log4j2.properties\\n[2019-11-01T10:04:00,538][WARN ][logstash.config.source.multilocal] Ignoring the \\'pipelines.yml\\' file because modules or command line options are specified\\n[2019-11-01T10:04:00,703][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"7.0.0\"}\\n[2019-11-01T10:04:13,129][ERROR][logstash.agent           ] Failed to execute action {:action=&gt;LogStash::PipelineAction::Create/pipeline_id:main, :exception=&gt;\"LogStash::ConfigurationError\", :message=&gt;\"Expected one of #, { at line 69, column 5 (byte 1460) after output {\\\\r\\\\n\\\\tstdout {}\\\\r\\\\n  \\\\tif (\"total\" in [tags]) {\\\\r\\\\n  \\\\t\\\\telasticsearch {\\\\r\\\\n    \\\\t\\\\t\\\\thosts =&gt; [\"localhost:9200\"]\\\\r\\\\n    \\\\t\\\\t\\\\tindex =&gt; \"totalexecution-%{+YYYY}\"\\\\r\\\\n\\\\t\\\\t\\\\t\\\\tuser =&gt; elastic\\\\r\\\\n\\\\t\\\\t\\\\t\\\\tpassword =&gt; 3wUwULD3QJaKke\\\\r\\\\n\\\\t\\\\t\\\\t\\\\r\\\\n  \\\\t\\\\t\", :backtrace=&gt;[\"C:/busapps/rrsb/gbl1/logstash/7.0.0/logstash-core/lib/logstash/compiler.rb:41:in compile_imperative\\'\", \"C:/busapps/rrsb/gbl1/logstash/7.0.0/logstash-core/lib/logstash/compiler.rb:49:in compile_graph\\'\", \"C:/busapps/rrsb/gbl1/logstash/7.0.0/logstash-core/lib/logstash/compiler.rb:11:in block in compile_sources\\'\", \"org/jruby/RubyArray.java:2577:in map\\'\", \"C:/busapps/rrsb/gbl1/logstash/7.0.0/logstash-core/lib/logstash/compiler.rb:10:in compile_sources\\'\", \"org/logstash/execution/AbstractPipelineExt.java:151:in initialize\\'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize\\'\", \"C:/busapps/rrsb/gbl1/logstash/7.0.0/logstash-core/lib/logstash/java_pipeline.rb:23:in initialize\\'\", \"C:/busapps/rrsb/gbl1/logstash/7.0.0/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute\\'\", \"C:/busapps/rrsb/gbl1/logstash/7.0.0/logstash-core/lib/logstash/agent.rb:325:in block in converge_state\\'\"]}\\n[2019-11-01T10:04:15,907][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n[2019-11-01T10:04:19,360][INFO ][logstash.runner          ] Logstash shut down.\\n\\nconfig  file \\nconfig file has 68 lines , but log stash is showing error in line number 69 . am not sure whether log stash is picking the config file.\\nlog stash version used is 7.0 .  please  help.\\nP.S  my previous config file was bigger , it had a else section in the out put.  i removed just to identify whether there is really problem with line number 69'},\n",
       " {'text_field': '[DEBUG][logstash.runner          ] *path.config: \"C:\\\\\\\\busapps\\\\\\\\rrsb\\\\\\\\gbl1\\\\\\\\logstash\\\\\\\\7.0.0\\\\\\\\bin\\\\\\\\pipelines\"\\n\\nThat is a directory. logstash will concatenate all of the files in that directory to form the configuration. Every file. No exceptions. If there is a java heap dump in the directory then logstash will try to parse it as a configuration file.\\nOther messages then logged are\\nConfig string {:protocol=&gt;\"file\", :id=&gt;\"C:/busapps/rrsb/gbl1/logstash/7.0.0/bin/pipelines/logstash - Copy.conf\"}\\nConfig string {:protocol=&gt;\"file\", :id=&gt;\"C:/busapps/rrsb/gbl1/logstash/7.0.0/bin/pipelines/logstash.conf\"}\\nConfig string {:protocol=&gt;\"file\", :id=&gt;\"C:/busapps/rrsb/gbl1/logstash/7.0.0/bin/pipelines/logstash_bkp.conf\"}\\n\\nSo it merges those three files to form the configuration. The error is at line 68, which is in the first file. Adding quotes around the passwords in logstash - Copy.conf fixes the errors, but you probably want to move the backup files to a different directory.'},\n",
       " {'text_field': \"Hi,\\nI have removed half of nodes of cluster, and the removed nodes and machines have been totally destroyed.\\nthe existing cluster log like:\\nmaster not discovered or elected yet, an election requires at least nodes with ids from ...\\n\\nhave discovered [] which is not quorum;\\n\\ndiscovery will continue using...\\n\\n\\nAfter seeking the docs, there is a way to reinstate removed nodes to let the node discovery the master eligible to form a quorum. However, the removed nodes have been permanently removed, can not reinstate,  we hope to don't delete the whole path.data to re-build the cluster, we want to keep the indices shard data in path.data, and only wipe the previously saved master eligible states in path.data, voting configuration in the path.data.\\nIs there any ways to achieve this?\"},\n",
       " {'text_field': \"If you lost half or more of the master nodes then you have lost the cluster metadata, without which Elasticsearch cannot correctly interpret the data held in all the shards. Since you don't have a snapshot either the only sensible thing to do is rebuild the cluster from data held elsewhere as best as you can.\"},\n",
       " {'text_field': \"Hi,\\nI've observed my kibana having some of the doc missing as compared to the elasticsearch query.\\nI did a query on the _id in elasticsearch and return 1 hits\\n\\nimage.png1037×759 70.6 KB\\n\\nBut I did the same query in Kibana Discover, it did not return any result.\\nI've check the index, _id and the dates are correct\\n\\nimage.png1366×629 56.7 KB\\n\\nAnyone know how should I troubleshoot this, please help.\\nThanks\"},\n",
       " {'text_field': 'Thanks @Nathan_Reese for the enlightening response.\\n\\nI\\'m managed to identify the root caused via the \"Inspect\".\\nThe Discover query is using time \"timestamp\", but the doc not shown in Discover is using \"@timestamp\".\\nI\\'ve resolved the issue by recreating the index pattern using \"@timestamp\" field as the time filter field.\\n\\nimage.png1656×719 83.9 KB\\n'},\n",
       " {'text_field': 'I have hotel documents that each have rooms of nested type.\\n{\\nid: hotel_id,\\n   rooms: [\\n     {\\n        id: room_id_1,\\n        name: \"room 1 name\"\\n      },\\n      {\\n        id: room_id_2,\\n        name: \"room 2 name\"\\n      },\\n      ....\\n  ]\\n}\\n\\nAnd I want to update only single field from a specific room. I am trying with the Update api, update the room with id 2 from the hotel document with id 1:\\nPOST hotels/_update/1\\n{\\n  \"script\" : {\\n    \"source\" : \"if(ctx._source.rooms.id == 2) { ctx._source.rooms.name = params.new_name }\",\\n    \"lang\" : \"painless\",\\n    \"params\" : {\\n        \"new_name\" : \"new room name\"\\n    }  \\n  } \\n}\\n\\nI get this error \"Illegal list shortcut value [id]\" from ES.\\nThanks!'},\n",
       " {'text_field': 'it was so obvious.. I had to loop through the rooms..\\n\\n  \\n      stackoverflow.com\\n  \\n  \\n      \\n    \\n  \\n\\n  elastic search - update specific nested object\\n\\n\\n\\n  elasticsearch\\n\\n\\n\\n  asked by\\n  \\n  \\n    Vlad Dogarescu\\n  \\n  on 12:50PM - 01 Nov 19 UTC\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n'},\n",
       " {'text_field': \"Hi\\nI upgraded my cluster (3 master nodes) from 7.2 to 7.4 last night after which it completely fell apart.\\nIt looks like there is some communication issue between the nodes in the cluster and they keep on trying to elect a master. Some requests time out, the master election fails and they restart the master election.\\nHere's the log from a node which was a master that failed\"},\n",
       " {'text_field': \"Ok, we see el-bauni-lean be elected as master here:\\n[2019-11-04T09:28:43,083][INFO ][o.e.c.s.MasterService    ] [el-bauni-lean] elected-as-master ([2] nodes joined)[{el-bauni-lean}{bYCS8DxfQeOo4tTM86MAXw}{gYOGHMf2QJi6PiuOURl6CA}{lean.snerpa.local}{172.24.8.152:9300}{dim}{xpack.installed=true} elect leader, {el-bauni-bean}{RPgnf3XBS1abbt7YMPz0HQ}{GqLbSXBkRrqVY-bgoiNYeg}{bean.snerpa.local}{172.24.8.150:9300}{dim}{xpack.installed=true} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 229, version: 14737, reason: master node changed {previous [], current [{el-bauni-lean}{bYCS8DxfQeOo4tTM86MAXw}{gYOGHMf2QJi6PiuOURl6CA}{lean.snerpa.local}{172.24.8.152:9300}{dim}{xpack.installed=true}]}, added {{el-bauni-bean}{RPgnf3XBS1abbt7YMPz0HQ}{GqLbSXBkRrqVY-bgoiNYeg}{bean.snerpa.local}{172.24.8.150:9300}{dim}{xpack.installed=true},}\\n\\nIt looks ok for a bit but then it times out publishing a cluster state update:\\n[2019-11-04T09:32:44,386][DEBUG][o.e.c.s.MasterService    ] [el-bauni-lean] publishing cluster state version [14744]\\n[2019-11-04T09:32:50,229][TRACE][o.e.g.MetaStateService   ] [el-bauni-lean] [[.watches/0zCWWk5lRK6KXxv-bUThWw]] writing state, reason [version changed from [194] to [195]]\\n[2019-11-04T09:33:15,749][TRACE][o.e.g.MetaStateService   ] [el-bauni-lean] [[.watches/0zCWWk5lRK6KXxv-bUThWw]] state written\\n[2019-11-04T09:33:15,749][TRACE][o.e.g.MetaStateService   ] [el-bauni-lean] [_meta] writing state, reason [changed]\\n[2019-11-04T09:33:30,144][TRACE][o.e.g.MetaStateService   ] [el-bauni-lean] [_meta] state written (generation: 10147)\\n[2019-11-04T09:33:43,773][INFO ][o.e.c.s.ClusterApplierService] [el-bauni-lean] master node changed {previous [{el-bauni-lean}{bYCS8DxfQeOo4tTM86MAXw}{gYOGHMf2QJi6PiuOURl6CA}{lean.snerpa.local}{172.24.8.152:9300}{dim}{xpack.installed=true}], current []}, term: 229, version: 14743, reason: becoming candidate: Publication.onCompletion(false)\\n[2019-11-04T09:33:46,463][WARN ][o.e.c.s.MasterService    ] [el-bauni-lean] failing [shard-started StartedShardEntry{shardId [[.watches][0]], allocationId [5q1c6ne5RuS-MEYawB902w], primary term [67], message ...: failed to commit cluster state version [14744]\\norg.elasticsearch.cluster.coordination.FailedToCommitClusterStateException: publication failed\\n  at org.elasticsearch.cluster.coordination.Coordinator$CoordinatorPublication$4.onFailure(Coordinator.java:1429) ~[elasticsearch-7.4.2.jar:7.4.2]\\n  at org.elasticsearch.action.ActionRunnable.onFailure(ActionRunnable.java:60) ~[elasticsearch-7.4.2.jar:7.4.2]\\n...\\nCaused by: org.elasticsearch.ElasticsearchException: publication cancelled before committing: timed out after 30s\\n\\nAgain the reason for the failure is that it is writing metadata egregiously slowly: it took 40 seconds to write the two small metadata files involved in this cluster state update. This still very much looks like misbehaviour on the IO front: this should be closer to milliseconds than minutes.\\nI also see timeout messages like these which are suggestive of network issues:\\n[2019-11-04T09:32:37,278][WARN ][o.e.t.TransportService   ] [el-bauni-lean] Received response for a request that has timed out, sent [22822ms] ago, timed out [0ms] ago, action [cluster:monitor/nodes/stats[n]], node [{el-bauni-bean}{RPgnf3XBS1abbt7YMPz0HQ}{GqLbSXBkRrqVY-bgoiNYeg}{bean.snerpa.local}{172.24.8.150:9300}{dim}{xpack.installed=true}], id [485]\\n[2019-11-04T09:32:43,710][WARN ][o.e.t.TransportService   ] [el-bauni-lean] Received response for a request that has timed out, sent [14929ms] ago, timed out [0ms] ago, action [internal:coordination/fault_detection/follower_check], node [{el-bauni-mean}{T2AxMn8sQTmBwe3g-5GGow}{LcYFhs0SRoGeGzuuvwmNcA}{mean.snerpa.local}{172.24.8.151:9300}{dim}{xpack.installed=true}], id [491]\\n[2019-11-04T09:33:09,656][WARN ][o.e.t.TransportService   ] [el-bauni-mean] Received response for a request that has timed out, sent [20927ms] ago, timed out [10962ms] ago, action [internal:coordination/fault_detection/leader_check], node [{el-bauni-lean}{bYCS8DxfQeOo4tTM86MAXw}{gYOGHMf2QJi6PiuOURl6CA}{lean.snerpa.local}{172.24.8.152:9300}{dim}{xpack.installed=true}], id [51]\\n[2019-11-04T09:33:09,735][WARN ][o.e.t.TransportService   ] [el-bauni-lean] Received response for a request that has timed out, sent [32524ms] ago, timed out [17595ms] ago, action [internal:coordination/fault_detection/follower_check], node [{el-bauni-bean}{RPgnf3XBS1abbt7YMPz0HQ}{GqLbSXBkRrqVY-bgoiNYeg}{bean.snerpa.local}{172.24.8.150:9300}{dim}{xpack.installed=true}], id [490]\\n[2019-11-04T09:34:10,510][WARN ][o.e.t.TransportService   ] [el-bauni-lean] Received response for a request that has timed out, sent [5208ms] ago, timed out [2201ms] ago, action [internal:discovery/request_peers], node [{el-bauni-mean}{T2AxMn8sQTmBwe3g-5GGow}{LcYFhs0SRoGeGzuuvwmNcA}{mean.snerpa.local}{172.24.8.151:9300}{dim}{xpack.installed=true}], id [523]\\n[2019-11-04T09:34:30,966][WARN ][o.e.t.TransportService   ] [el-bauni-bean] Received response for a request that has timed out, sent [12618ms] ago, timed out [2601ms] ago, action [internal:coordination/fault_detection/follower_check], node [{el-bauni-mean}{T2AxMn8sQTmBwe3g-5GGow}{LcYFhs0SRoGeGzuuvwmNcA}{mean.snerpa.local}{172.24.8.151:9300}{dim}{xpack.installed=true}], id [266]\\n\\nI think the TLS messages Insufficient buffer remaining for AEAD cipher fragment are also due to network issues. I'm not familiar with Windows Storage Spaces -- is this some kind of network-attached storage? Would network issues explain the super-slow IO that you're seeing?\"},\n",
       " {'text_field': 'I\\'m trying to use Logstash 7.4.1 to parse XML files that are being generated each minute upon request to the Dynatrace server via REST api. The output will be sent to ElasticSearch.\\nDynatrace Dashboard Report\\nMy goal is to be able to generate a visualisation in Kibana similar to the Dynatrace Dashboard.\\nx-axis : timestamps\\ny-axis: values (request_count or average_response_time)\\nMy approach is to aggreate all measurements belonging to the same measure according to their aggregation attribute. If aggregation type is Count, i\\'ll sum up all count values and write it to ES as a single record, if aggregation type is Average, i\\'ll compute the average using sum and count and write it to ES as another record.\\nI expect to have one document for each measure and the output should be similar to the following:\\ndoc/1\\nchartdashlet: Operasyon Adet\\nmeasure : SGT_1\\nmeasure_type: operation_count\\nmeasure_time: 01.11.2019 14:01\\nvalue: somenumber\\ndoc/2\\nchartdashlet: Operasyon Adet\\nmeasure : SGT_2\\nmeasure_type: operation_count\\nmeasure_time: 01.11.2019 14:01\\nvalue: somenumber\\ndoc/3\\nchartdashlet: Operasyon Sure\\nmeasure : SGT_3\\nmeasure_type: operation_responsetime\\nmeasure_time: 01.11.2019 14:01\\nvalue: somenumber\\nHow can i do it with logstash? In order to join parent and child nodes, i need Xpath 2.0 string_join function but the used library does only support Xpath 1.0 according to the documentation. Do i need to write Ruby scripts to make for loops over measures etc?\\nFor now I\\'m able to get the whole file as a single event and i can extract measures of count type with the following configuration in a testing environment: (with a single input file)\\n$ /home/someuser/logstash-7.4.1/bin/logstash -f dynatrace-dashboard.conf\\n$ cat dynatrace-dashboard.conf\\ninput {\\n   file {\\n     id =&gt; \"dynatrace_dashboard_values\"\\n     mode =&gt; \"read\"\\n     path =&gt; \"/home/someuser/dynatrace-input.xml\"\\n     codec =&gt; multiline {\\n       pattern =&gt; \"&lt;?xml\"\\n       negate =&gt; \"true\"\\n       what =&gt; \"previous\"\\n      }\\n  }\\n}\\nfilter {\\n  xml {\\n    source =&gt; \"message\"\\n    xpath =&gt; [        \"/dashboardreport/data/chartdashlet/measures/measure[@aggregation=\\'Count\\']\", \"measure_count\" ]\\n    store_xml =&gt; \"false\"\\n  }\\n}\\noutput {\\n  file {\\n    path =&gt; \"/home/someuser/logst.out\"\\n    codec =&gt; \"rubydebug\"\\n  }\\n}\\n\\nI\\'m stucked at this point.\\nAnother alternative for me is to write a Python script to do all this stuff that I\\'ve described above and send it directly to ES but since we have so many pipelines centralized on logstash containers, I would prefer to do it with Logstash rather than this custom solution.\\nAny alternative recommendation to satify my goal will be appreciated. Maybe it is possible to write all xml file to ES and do some high level querying to achieve my goal.'},\n",
       " {'text_field': 'Here is the final version of my script handling the cases where\\nmeasurement.count == 0 and measurement.count == 1\\ndef register(params)\\n\\nend\\n\\ndef filter(event)\\n    theEvents = []\\n    xml =  event.get(\"[@metadata][theXML]\")\\n    recordTimestamp = event.get(\"record_timestamp\")\\n\\n    xml[\"data\"][\"chartdashlet\"].each { |dashlet|\\n        dashlet[\"measures\"][\"measure\"].each { |y|\\n            #logger.trace(\"measure_name is: #{y[\"measure\"]}\")\\n            measurement = y[\"measurement\"]\\n           \\n            anEvent = Hash[ \"inputtype\", \"dynatrace_dashboard_report\", \"dashboard\", xml[\"name\"], \"chartdashlet\", dashlet[\"name\"], \"measure\", y[\"measure\"], \"measure_agg_type\", y[\"aggregation\"] ]\\n            anEvent[\"record_timestamp\"] = recordTimestamp\\n            \\n            if measurement.nil? #handles when there is no measurement under a measure\\n                anEvent[\"value\"] = 0\\n            elsif y[\"aggregation\"] == \"Average\"\\n                #do if more than 1 measurement\\n                if measurement.kind_of?(Array)\\n                    sum = 0.0\\n                    count = 0\\n                    measurement.each { |z|\\n                        sum += z[\"sum\"].to_f\\n                        count += z[\"count\"].to_i\\n                    }\\n                    anEvent[\"value\"] = sum/count\\n                else #do when there is only 1 record\\n                    anEvent[\"value\"] = measurement[\"avg\"]\\n                end\\n\\n            elsif y[\"aggregation\"] == \"Count\"\\n                if measurement.kind_of?(Array)\\n                    count = 0\\n                    measurement.each { |z|\\n                        count += z[\"count\"].to_i\\n                    }\\n                    anEvent[\"value\"] = count\\n                else #handles when there is only 1 record\\n                   anEvent[\"value\"] = measurement[\"count\"]\\n                end\\n            end\\n\\n            theEvents &lt;&lt; LogStash::Event.new(anEvent)\\n        }\\n    }\\n    theEvents\\nend\\n\\nFor other file based exceptions, logstash adds _rubyexception tag to the input object and my strategy is to directly push these records to my index in order to keep track of exception counts.\\nFor others who may need this kind of event processing, please note that all the input fields (except the ones existing at the time i instantiate a new event in Ruby) are discarded in case of successfull ruby script execution since i produce 5 events out of a single input event, but when it fails, all the input fields are output with an extra _rubyexception tag:\\n        {\\n          \"@timestamp\": \"2019-11-15T11:00:59.138Z\",\\n          \"input\": {\\n            \"type\": \"log\"\\n          },\\n          \"inputtype\": \"dynatrace_dashboard_report\",\\n          \"tags\": [\\n            \"beats_input_codec_plain_applied\",\\n            **\"_rubyexception\"**\\n          ],\\n          \"@version\": \"1\",\\n          \"record_timestamp\": \"2019-11-15T07:14:00.000Z\",\\n          \"log\": {\\n            \"file\": {\\n              \"path\": \"/home/xxx/input-filebeat/dynatrace_048_(SGT)_BANKA_SIGORTACILIĞI_20191115101400.out\"\\n            },\\n            \"offset\": 0,\\n            \"flags\": [\\n              \"multiline\"\\n            ]\\n          },\\n          \"from_millis\": \"1573802040000\",\\n          \"agent\": {\\n            \"type\": \"filebeat\",\\n            \"id\": \"8dcd2d3e-be27-4cc7-ae38-14a7a16874fb\",\\n            \"version\": \"7.4.2\",\\n            \"hostname\": \"somehost\",\\n            \"ephemeral_id\": \"430243c8-e494-407b-b1c0-57155a4cbc45\"\\n          }\\n        }\\n\\nthe final conf:\\n filter {\\n  xml {\\n    source =&gt; \"message\"\\n    target =&gt; \"[@metadata][theXML]\"\\n    force_array =&gt; false\\n    remove_field =&gt; [ \"message\", \"host\", \"ecs\" ]\\n    add_field =&gt; { \"inputtype\" =&gt; \"dynatrace_dashboard_report\" }\\n    xpath =&gt; [ \"/dashboardreport/source/filters/filter/text()\" , \"[@metadata][report_filter_text]\" ]\\n  }\\n  dissect {\\n    mapping =&gt; { \"[@metadata][report_filter_text]\" =&gt; \"%{?filter_type}?%{from_millis}:%{?end_millis}\" }\\n  }\\n  date {\\n    match =&gt; [ \"from_millis\", \"UNIX_MS\" ]\\n    target =&gt; \"record_timestamp\"\\n  }\\n  ruby {\\n    path =&gt; \"/home/xxx/code/dynatrace.rb\"\\n\\n  }\\n}\\noutput {\\n  stdout {\\n    codec =&gt; rubydebug { metadata =&gt; true }\\n  }\\n  if [inputtype] == \"dynatrace_dashboard_report\" {\\n    file {\\n      path =&gt; \"/home/xxx/debug-logstash1.out\"\\n      codec =&gt; \"json\"\\n    }\\n  }\\n}'},\n",
       " {'text_field': \"I'm aware that shard allocation needs to be disabled and then re-enabled back during rolling upgrades.\\nMy question is: Does this need to be done for master nodes?\\nI reckon this is only needed for data nodes and NOT needed for master nodes.  Can someone confirm? The main purpose of disabling and enabling is to prevent un-necessary movement of shards across nodes but master nodes don't contain any shards in first place.\"},\n",
       " {'text_field': 'I see, sorry, I misunderstood your question. Yes, disabling shard allocation is only really meaningful when restarting a data node.'},\n",
       " {'text_field': 'Hello Elasticsearch Sorcerers,\\nI am running Logstash v7.4.0 and Elasticsearch v7.4.0 in a nicely-working pipeline.  My chain of fruit stores are sending my sales information to Logstash; Logstash then pushes that data to Elasticsearch.  Here’s a subsample of my Elasticsearch fruit_sales index, plus a few example data records:\\nproduct_code   qnty\\n====================\\n100            2039\\n200             382\\n300            1028\\n400             128\\n\\nThis means: I sold 2,039 units of product 100, 382 units of product 200, and so on.\\nMy setup works great, but the trouble is it’s a pain to remember what Product Code is assigned for which fruit.  On a piece of paper in my office, I have a list:\\nproduct_code   name\\n======================\\n100            apples\\n200            oranges\\n300            bananas\\n\\nThe list currently has several hundred items.  It does not change much, but I might need to modify it from time to time.  Unfortunately, there is no way to modify the data that my stores send into Logstash to include these names.\\nWhat I’d love is a solution where I can input my product_code-to-name list into ElasticSearch.  And then, when ES is processing the data from Logstash, ES adds a new string field into each data record:\\nproduct_code   *name*       qnty\\n================================\\n100            “apples”     2039\\n200            “oranges”     382\\n300            “bananas”    1028\\n400            “other”       128\\n\\nNow in my index, I have both the numerical product code plus a human-readable string with the fruits’ name.  This makes checking queries and reading Kibana Visualizations much, much easier.  Also note that there is a default value (“other”) in case I have a product which appears in the sales data, but not on the name list.\\nAnother consideration:  I don’t think I’ll need to update the product_code-to-name list much, but I can’t have a solution where I would need to stop and restart the ES service whenever I make an edit to that list.  Of course, if I have 10,000 data records with a Name field already populated, and then, say, I change Product_Code 100 from “apples” to “papayas,” there’s no need to traverse through all existing records to change that.  But every new data record from that point out needs to have the new string assignment.\\nIs there a graceful, non-computational way to do this?  In SQL, you could do a join between two tables, in C you could create a product_code-to-name hash table, etc.\\nIn ES, I assume I would use the “put mapping” API to create the “name” field in my index, and I could create it as a string plus set all initial values to “other” to enforce the default value.  But I’m baffled how I could do the product_code-to-name lookup…?\\nAny advice on this will be wildly appreciated.\\nFULL DISCLOSURE:  I am also posting a variation of this question to the Logstash forum, as this task might be easier on LS than ES.'},\n",
       " {'text_field': 'For now, Logstash is indeed the better tool to do this. However, work is underway to allow you do this in Elasticsearch soon, using the new enrich processor that you can use with an enrich policy. The master branch of the documentation will give you a sneak peak of how this will work.'},\n",
       " {'text_field': 'Hi there,\\nI\\'ve created a TSVB-visualization that plots the dynamics of an average rang (of all players) and individual rangs. It looks like that:\\n\\npH147qD.png1517×236 46.8 KB\\n\\nThis visualization is used in a dashboard filtered by a single player (i.e. a player\\'s ID is indicated so that all data &amp; graphs for this player are shown). And when the TSVB becomes also filtered, a \"common\" average value coincides with the \"personal\" one (see a screenshot).\\n\\nIn TSVB \"Panel options\" there is \"Ignore global filter\", but this option conserves the values for  ALL players. How can I \"freeze\" a common average value and then compare it with the one of a specified player?'},\n",
       " {'text_field': 'The solution is to build 2 corresponding visualizations and to place them near each other'},\n",
       " {'text_field': 'Hello Logstash Sorcerers,\\nI am running Logstash v7.4.0 and Elasticsearch v7.4.0 in a nicely-working pipeline.  My chain of fruit stores are sending my sales information to Logstash; Logstash then pushes that data to Elasticsearch.  Here’s a subsample of my Elasticsearch fruit_sales index, plus a few example data records:\\nproduct_code   qnty\\n====================\\n100            2039\\n200             382\\n300            1028\\n400             128\\n\\nThis means: I sold 2,039 units of product 100, 382 units of product 200, and so on.\\nMy setup works great, but the trouble is it’s a pain to remember what Product Code is assigned for which fruit.  On a piece of paper in my office, I have a list:\\nproduct_code   name\\n======================\\n100            apples\\n200            oranges\\n300            bananas\\n\\nThe list currently has several hundred items.  It does not change much, but I might need to modify it from time to time.  Unfortunately, there is no way to modify the data that my stores send into Logstash to include these names.\\nWhat I’d love is a solution where I can input my product_code-to-name list into Logstash.  And then, when LS is processing the raw data it is receiving from my stores, LS adds a new string field into each data record:\\nproduct_code   *name*      qnty\\n===============================\\n100            “apples”    2039\\n200            “oranges”    382\\n300            “bananas”   1028\\n400            “other”      128\\n\\nNow in each data record, I have both the numerical product code plus a human-readable string with the fruits’ name.  This makes checking queries and reading Kibana Visualizations much, much easier.  Also note that there is a default value (“other”) in case I have a product which appears in the sales data, but not on the name list.\\nAnother consideration:  I don’t think I’ll need to update the product_code-to-name list much, but I can’t have a solution where I would need to stop and restart the LS service whenever I make an edit to that list.  Of course, if I have 10,000 data records with a Name field already populated, and then, say, I change Product_Code 100 from “apples” to “papayas,” there’s no need to traverse through all existing records to change that.  But every new data record from that point out needs to have the new string assignment.\\nIs there a graceful, non-computational way to do this?  In SQL, you could do a join between two tables, in C you could create a product_code-to-name hash table, etc.\\nI assume in my LS config file, I would set up a filter that would do something crude like this:\\nfilter {\\n  ruby {\\n    # Bounce each data record to a ruby script which does\\n    # product_code lookup and adds new field accordingly\\n    path =&gt; \"/home/me/addNameField.rb\"\\n  }\\n}\\n\\nI don’t have much Ruby experience, but this doesn’t seem that hard.\\nBut I’d like to ask the Forum: (A) is this the best solution?  And (B), if I use the Ruby script, is there an example of another Ruby script which adds a new field into a record?  I’d rather adapt something that works than reinvent the wheel from scratch.\\nAny advice on this will be wildly appreciated.  Thank you!\\nFULL DISCLOSURE:  I am also posting a variation of this question to the Elasticsearch forum, as this task might be easier on ES than LS.'},\n",
       " {'text_field': 'Yes!  You are right!  That worked!  I can see my new field in Logstash\\'s output, plus in Kibana too.  I\\'m so good, this is most excellent indeed!  Thank you!\\nI will repost the final solution below, for the benefit of anyone who may be reading this post.\\nSo to add a \"name\" field into my data based on the value of A.AA.product_code (where the data\\'s structure is as described above), I modified my LS config file with the following:\\nfilter {\\n  translate {\\n    field =&gt; \"[A][AA][product_code]\"\\n    destination =&gt; \"name\"\\n    dictionary_path =&gt; \"/home/me/myNameList.yaml\"\\n    fallback =&gt; \"unknown\"\\n    refresh_interval =&gt; 60\\n    refresh_behaviour =&gt; replace\\n  }\\n}\\n\\nAnd where the /home/me/myNameList.yaml looks like this:\\n\"100\": apples\\n\"200\": oranges\\n\"300\": bananas\\n\\nThis instructs LG to examine every value of A.AA.product_code and do its best to match that value against the strings listed in the myNameList.yaml file.  If no value can be found, LS populates the new \"name\" field with the string \"unknown\"\\nThanks again Badger!  You rock!'},\n",
       " {'text_field': 'I have an index that I\\'ve created that follows the ECS standard.  Within the index I have the source.as.* and destination.as.* fields that are populated using the GeoLite2ASN database.  The fields are being populated in the index, but the network section of SIEM doesn\\'t display them.  The main page nor the drilled down view of the IP shows this information.  I verified with the \"inspect\" feature that I am populating the right fields.  This is on version 7.4.0 if that matters.\\nHere\\'s a sample that doesn\\'t show the ASN in SIEM, but does show in searches.  Any ideas what I might be missing/doing incorrectly?\\n{\\n  \"source\": {\\n    \"ip\": \"192.168.1.1\",\\n    \"interface\": \"X16\",\\n    \"mac\": \"00:00:00:00:00:00\",\\n    \"bytes\": 119311,\\n    \"port\": \"62190\"\\n  },\\n  \"agent\": {\\n    \"type\": \"logstash\",\\n    \"name\": \"vDEVLOG1\"\\n  },\\n  \"observer\": {\\n    \"type\": \"firewall\",\\n    \"ip\": \"1.1.1.1\",\\n    \"serial_number\": \"NA\",\\n    \"vendor\": \"unknown\"\\n  },\\n  \"message\": \"%{[message][1]}\",\\n  \"destination\": {\\n    \"port\": \"443\",\\n    \"as\": {\\n      \"asn\": 46489,\\n      \"as_org\": \"Twitch Interactive Inc.\",\\n      \"ip\": \"52.223.226.232\"\\n    },\\n    \"interface\": \"X5\",\\n    \"ip\": \"52.223.226.232\",\\n    \"geo\": {\\n      \"location\": {\\n        \"lat\": 47.6348,\\n        \"lon\": -122.3451\\n      },\\n      \"longitude\": -122.3451,\\n      \"country_code3\": \"US\",\\n      \"postal_code\": \"98109\",\\n      \"region_code\": \"WA\",\\n      \"continent_code\": \"NA\",\\n      \"city_name\": \"Seattle\",\\n      \"ip\": \"52.223.226.232\",\\n      \"latitude\": 47.6348,\\n      \"dma_code\": 819,\\n      \"region_name\": \"Washington\",\\n      \"country_code2\": \"US\",\\n      \"timezone\": \"America/Los_Angeles\",\\n      \"country_name\": \"United States\"\\n    },\\n    \"bytes\": 17384244,\\n    \"mac\": \"40:a6:e8:5f:ba:d6\"\\n  },\\n  \"network\": {\\n    \"protocol\": \"https\",\\n    \"transport\": \"tcp\",\\n    \"bytes\": 17503555\\n  },\\n  \"host\": {\\n    \"ip\": \"192.168.1.1\"\\n  },\\n  \"url\": {\\n    \"domain\": \"video-edge-eddcc8.ord02.abs.hls.ttvnw.net\",\\n    \"path\": \"/v1/\"\\n  },\\n  \"@version\": \"1\",\\n  \"id\": \"a-fw\",\\n  \"tags\": [\\n    \"syslog\",\\n    \"unknown\",\\n    \"privateip_source\"\\n  ],\\n  \"@timestamp\": \"2019-10-31T23:00:02.000Z\",\\n  \"_index\": \"syslog-1.0-000028\",\\n  \"_type\": \"_doc\",\\n  \"_id\": \"aKUMJG4B3alxQ_lAFsNy\",\\n  \"_score\": 1\\n}'},\n",
       " {'text_field': \"The destination.as.* fields you have populated are not what is defined in Elastic Common Schema. See https://www.elastic.co/guide/en/ecs/current/ecs-as.html.\\nYou'll want to have a destination.as.number and destination.as.organization.name.\"},\n",
       " {'text_field': 'Ситуация такая: есть TSVB-визуализация, отображающая среднее значение по всем игрокам (синяя) и отдельные показатели игроков (красные линии):\\n\\npH147qD.png1517×236 46.8 KB\\n\\nДанная визуализация затем используется в дешборде, отфильтрованном по одному игроку (то есть указывается ID игрока для демонстрации всей статистики по нему). Но в этом случае общее среднее значение совпадает со средним показателем игрока:\\n\\nВ TSVB \"Panel options\" есть пункт \"Ignore global filter\", однако эта опция сохраняет значения по ВСЕМ игрокам. Можно ли как-то отдельно \"заморозить\" общее среднее значение и потом сравнивать его с показателями отдельно взятого игрока?'},\n",
       " {'text_field': 'К сожалению единственная идеа это зделать два отдельных графа и расположить их один под другим. Этот подход доволно стандартный в мире ДатаВиз.'},\n",
       " {'text_field': 'Hello,\\ni am new to ELK. when i onboarded the below log file, it is going to \"dead letter queue\" in logstash because logstash couldn\\'t able to process the events.  I am not sure which type of plugin to use whether KV plugin or CSV plugin because first half of the events are normal and next half is in KV pair. Any help would be appreciated on how to write the filters.\\nBelow is the sample log format.\\n25193662345 [http-nio-8080-exec-44] DEBUG c.s.b.a.m.PerformanceMetricsFilter - method=PUT status=201 appLogicTime=1, streamInTime=0, blobStorageTime=31, totalTime=33 tenantId=b9sdfs-1033-4444-aba5-csdfsdfsf, immutableBlobId=bss_c_586331/Sample_app12-sdas-157123148464.txt, blobSize=2862, domain=abc\\n\\n2519366789 [http-nio-8080-exec-47] DEBUG q.s.b.y.m.PerformanceMetricsFilter - method=PUT status=201 appLogicTime=1, streamInTime=0, blobStorageTime=32, totalTime=33 tenantId=b0csdfsd-1066-4444-adf4-ce7bsdfssdf, immutableBlobId=bss_c_586334/Sample_app15-615223-157sadas6648465.txt, blobSize=2862, domain=cde\\n\\nThanks'},\n",
       " {'text_field': 'Try using a dissect filter to parse the first part of the line, and a kv filter for the rest.\\n    dissect { mapping =&gt; { \"message\" =&gt; \"%{someNumber} [%{thread}] %{level} %{class} - %{[@metadata][restOfLine]}\" } }\\n    kv { source =&gt; \"[@metadata][restOfLine]\" field_split =&gt; \",\" }'},\n",
       " {'text_field': \"We create a dynamic kibana url to the user\\nhttp://localhost:5601/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:'2019-10-16T06:00',to:'2019-10-18T06:00'))&amp;_a=(columns:!(_source),index:'logstash-*',interval:auto,query:(language:lucene,query:'Error%20Fetching,%20Shipping%20Details'),sort:!(!('@timestamp',desc)))\\nour index is logstash-*  and it is valid and we would like it be selected when the user clicks on the url ....but we always get this exception that index is not valid .......![scnshot|564x134] even though it is a configured index pattern\\n\\nWe are fairly new to kibana communtity's help in resolving this issue would be of great help\"},\n",
       " {'text_field': 'the index url parameter needs to specify the saved object id, not the index pattern title. To view your index pattern id, go to management =&gt; index patterns and select your index pattern. The URL will contain the id of the saved object. This is the value that should be used in your discover link.\\n\\nScreen Shot 2019-11-01 at 2.54.38 PM.png4096×2224 650 KB\\n'},\n",
       " {'text_field': 'FSCrawler log says this.\\n[Elasticsearch exception [type=mapper_parsing_exception, reason=failed to parse]]; nested: ElasticsearchException[Elasticsearch exception [type=illegal_argument_exception, reason=Malformed content, found extra data after parsing: FIELD_NAME]];\\n16:10:16,190 ^[[36mDEBUG^[[m [f.p.e.c.f.c.v.ElasticsearchClientV7] Error caught for [ips-internal-doc-index]/[_doc]/[8f531bfbb22847e4c87c31a17a6284]: ElasticsearchException[Elasticsearch exception [type=mapper_parsing_exception, reason=failed to parse]]; nested: ElasticsearchException[Elasticsearch exception [type=illegal_argument_exception, reason=Malformed content, found extra data after parsing: FIELD_NAME]];\\n16:10:16,191 ^[[33mWARN ^[[m [f.p.e.c.f.c.v.ElasticsearchClientV7] Got [3] failures of [4] requests\\n\\nElasticsearch log says this.\\n\"Caused by: java.lang.IllegalArgumentException: Malformed content, found extra data after parsing: FIELD_NAME\",\\n\"at org.elasticsearch.index.mapper.DocumentParser.validateEnd(DocumentParser.java:146) ~[elasticsearch-7.3.0.jar:7.3.0]\",\\n\"at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:72) ~[elasticsearch-7.3.0.jar:7.3.0]\",\\n\"... 34 more\"] }\\n\\nThe same configuration worked fine for me with the same documents in FSCrawler 2.6. I am using all default configuration of Elasticsearch.'},\n",
       " {'text_field': '@dadoonet It worked for me after I deleted the existing index and recreated it as per the discussion in https://github.com/dadoonet/fscrawler/issues/755. Will come back if I get any more issues. For now, the issue seems resolved.'},\n",
       " {'text_field': 'Hi All,\\nI would have a question about parsing the logs from DNS server logs.\\nSample of DNS logs:\\nmessage:\\t3. 11. 2019 12:33:14 0958 PACKET  000000C97B8371C0 UDP Snd 192.168.5.202   4cbf   Q [0000       NOERROR] SOA    (4)mell(2)com(0)\\nGROK:\\n%{MS_DNS_DATE:date}\\\\s+%{TIME:time}\\\\s+%{DATA:thread_id}\\\\s+%{WORD:record_type}\\\\s+%{BASE16NUM:packet_id}\\\\s+%{WORD:dns_protocol}\\\\s+%{WORD:dns_direction}\\\\s+%{IP:dns_ip}\\\\s+%{BASE16NUM:xid}\\\\s+%{DATA:response}\\\\s+%{WORD:dns_query_type}\\\\s+[%{BASE16NUM:hex_flags}\\\\s+%{WORD:rcode_name}]\\\\s+%{WORD:query_type_name}\\\\s+%{GREEDYDATA:dns_domain}\\nIt was successfully parsed. I am using \\\\s+ to catch one or more spaces. I dont undersand why following message is parsed wrong.\\n\\n\\n\\n2019 19:31:12 A08 Note: got GQCS failure on a dead socket context status=995, socket=532, pcon=000000C9772D4180, state=-1, IP=0.0.0.0\\n\\n\\n\\n%{MS_DNS_DATE:date}\\\\s+%{TIME:time}\\\\s+%{DATA:thread_id}\\\\s+%{WORD:record_type}\\\\s+%{GREEDYDATA:description}\\nResult of this is:\\n{\\n\"date\": \"2. 11. 2019\",\\n\"thread_id\": \"A08 Note:\",\\n\"description\": \"GQCS failure on a dead socket context status=995, socket=532, pcon=000000C9772D4180, state=-1, IP=0.0.0.0\",\\n\"time\": \"19:31:12\",\\n\"record_type\": \"got\"\\n}\\nthere is a space between A08 and Note: So it should parse it to fields:\\nthread_id: A08\\nrecord_type: Note:\\nThanks\\nJan'},\n",
       " {'text_field': 'Use a more specific pattern than DATA, or parse it using dissect instead of grok.'},\n",
       " {'text_field': 'i tried the exec plugin of logstash:\\noutput {\\nexec {\\ncodec =&gt; line { format =&gt; \"%{message}\"}\\ncommand =&gt; \"echo %{message}\"\\n}\\n}\\nbut nothing print out , only shown:\\n[2019-10-30T22:18:42,715][INFO ][logstash.outputs.exec    ][main]\\nwhy nothing printed ? (it can be sure that the \"message\" is not null)'},\n",
       " {'text_field': 'Try enabling log.level debug'},\n",
       " {'text_field': 'My first post, please be gentle.\\nI have 100 Million records in an AWS ElasticSearch instance running 6.8, with the mapping:\\n\"mappings\":{\\n\\t\"message\":{\\n\\t\\t\"properties\":{\\n\\t\\t\\t\"messageID\":{\\n\\t\\t\\t\\t\"type\":\"long\"\\n\\t\\t\\t},\\n\\t\\t\\t\"content\":{\\n\\t\\t\\t\\t\"type\":\"text\"\\n\\t\\t\\t},\\n\\t\\t\\t\"datetime\":{\\n\\t\\t\\t\\t\"type\":\"date\"\\n\\t\\t\\t},\\n\\t\\t\\t\"statuses\":{\\n\\t\\t\\t\\t\"type\":\"nested\",\\n\\t\\t\\t\\t\"properties\": {\\n\\t\\t\\t\\t\\t\"datetime\":{\\n\\t\\t\\t\\t\\t\\t\"type\":\"date\"\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t\"status\":{\\n\\t\\t\\t\\t\\t\\t\"type\":\"keyword\"\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n\\nMy query:\\n\"query\": {\\n    \"nested\": {\\n\\t\\t\"path\": \"statuses\",\\n\\t\\t\"query\": {\\n\\t\\t\\t\"bool\": {\\n\\t\\t\\t\\t\"must\": [\\n\\t\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\t\"match\": {\\n\\t\\t\\t\\t\\t\\t\\t\"statuses.status\": \"sent\"\\n\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t]\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n\\nHave confirmed there are records with a nested item with \"status\" = \"sent\" yet this query returns no matches.\\nES Response:\\n{ \\n\\ttook: 6,\\n\\ttimed_out: false,\\n\\t_shards: { total: 1, successful: 1, skipped: 0, failed: 0 },\\n\\thits: { total: 0, max_score: null, hits: [] }\\n}\\n\\nHave I done something wrong in the mapping?\\nOr is my query not correct for 6.8?\\nQueries being run from AWS Lambda via Node 10 if that helps in any way.\\nExample of an existing record that I expected would have matched:\\n{\\n     messageID: 654602,\\n     content: \\'Test Content 7642145684296\\',\\n     datetime: 1572315557,\\n     statuses: [\\n         {\\n             datetime: 1572315557,\\n             status: \\'submitted\\'\\n         },\\n         {\\n             datetime: 1572315557,\\n             status: \\'sent\\'\\n         }\\n     ]\\n}'},\n",
       " {'text_field': 'Okay, found out what\\'s going on.\\nMy nested property was created, but I had been inserting \"statuses\" into an unmapped property called: \"Statuses\" (The capital letter ruined it for me).\\nClosing.\\n(If anyone has any tips on moving data from a non nested \"Statuses\" to a nested \"statuses\" property that would be appreciated.)'},\n",
       " {'text_field': 'Mapping defined as this:\\n  \"source\": {\\n    \"properties\": {\\n      \"address\": {\\n        \"type\": \"keyword\",\\n        \"fields\": {\\n          \"text\": { \"type\" : \"text\" }\\n        }\\n      },\\n      \"ip\": { \"type\": \"ip\" },\\n      \"port\": { \"type\": \"long\" },\\n      \"bytes\": { \"type\": \"long\" },\\n      \"geo\": {\\n        \"properties\": {\\n          \"city_name\": { \"type\": \"keyword\" },\\n          \"country_name\": { \"type\": \"keyword\" },\\n          \"continent_code\": { \"type\" : \"keyword\" },\\n          \"location\": { \"type\": \"geo_point\" }\\n        }\\n      },\\n      \"as\": {\\n        \"properties\": {\\n          \"number\": { \"type\": \"long\" },\\n          \"organization\": {\\n            \"properties\": {\\n              \"name\": { \"type\": \"keyword\" }\\n            }\\n          }\\n        }\\n      }\\n    }\\n\\nErrors received when ingesting data:\\n[2019-11-03T23:26:36,449][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"esxi_network-2019.11.02\", :_type=&gt;\"_doc\", :routing=&gt;nil}, #&lt;LogStash::Event:0x88ada96&gt;], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"esxi_network-2019.11.02\", \"_type\"=&gt;\"_doc\", \"_id\"=&gt;\"2BNgM24BZDNolIqijqjw\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"failed to parse field [source.geo.location] of type [geo_point]\", \"caused_by\"=&gt;{\"type\"=&gt;\"parse_exception\", \"reason\"=&gt;\"unsupported symbol [.] in geohash [12.5341]\", \"caused_by\"=&gt;{\"type\"=&gt;\"illegal_argument_exception\", \"reason\"=&gt;\"unsupported symbol [.] in geohash [12.5341]\"}}}}}}\\n[2019-11-03T23:26:36,449][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=&gt;400, :action=&gt;[\"index\", {:_id=&gt;nil, :_index=&gt;\"esxi_network-2019.11.02\", :_type=&gt;\"_doc\", :routing=&gt;nil}, #&lt;LogStash::Event:0xeddc1fc&gt;], :response=&gt;{\"index\"=&gt;{\"_index\"=&gt;\"esxi_network-2019.11.02\", \"_type\"=&gt;\"_doc\", \"_id\"=&gt;\"2RNgM24BZDNolIqijqjw\", \"status\"=&gt;400, \"error\"=&gt;{\"type\"=&gt;\"mapper_parsing_exception\", \"reason\"=&gt;\"failed to parse field [source.geo.location] of type [geo_point]\", \"caused_by\"=&gt;{\"type\"=&gt;\"parse_exception\", \"reason\"=&gt;\"unsupported symbol [-] in geohash [-78.3715]\", \"caused_by\"=&gt;{\"type\"=&gt;\"illegal_argument_exception\", \"reason\"=&gt;\"unsupported symbol [-] in geohash [-78.3715]\"}}}}}}\\n\\nSample of data entry:\\n ...\\n\"source\" =&gt; {\\n    \"geo\" =&gt; {\\n         \"city_name\" =&gt; \"SÃ£o Paulo\",\\n      \"country_name\" =&gt; \"Brazil\",\\n    \"continent_code\" =&gt; \"SA\",\\n          \"location\" =&gt; [\\n        [0] \"-46.6417\",\\n        [1] \"-23.5733\"\\n    ]\\n},\\n     \"as\" =&gt; {\\n          \"number\" =&gt; \"28666\",\\n    \"organization\" =&gt; {\\n        \"name\" =&gt; \"HOSTLOCATION LTDA\"\\n    }\\n},\\n...\\n\\ndata is picked from geoip lookups like this:\\n geoip {\\n   default_database_type =&gt; \\'City\\'\\n   database =&gt; \\'/usr/share/GeoIP2/GeoLite2-City.mmdb\\'\\n   cache_size =&gt; 5000\\n   source =&gt; \\'[source][ip]\\'\\n   target =&gt; \\'[@metadata][geo]\\'\\n   fields =&gt; [\\'city_name\\',\\'continent_code\\',\\'country_name\\',\\'latitude\\',\\'longitude\\']\\n   add_field =&gt; [\\n     \\'[source][geo][city_name]\\', \\'%{[@metadata][geo][city_name]}\\',\\n     \\'[source][geo][country_name]\\', \\'%{[@metadata][geo][country_name]}\\',\\n     \\'[source][geo][continent_code]\\', \\'%{[@metadata][geo][continent_code]}\\',\\n     \\'[source][geo][location]\\', \\'%{[@metadata][geo][longitude]}\\',\\n     \\'[source][geo][location]\\', \\'%{[@metadata][geo][latitude]}\\'\\n   ]\\n   remove_field =&gt; [\\'[@metadata][geo]\\']\\n }\\n # sometimes city still is unknown on successful lookups\\n if [source][geo][city_name] == \\'%{[@metadata][geo][city_name]}\\' {\\n   mutate { remove_field =&gt; [\\'[source][geo][city_name]\\'] }\\n }\\n\\nTIA for any hints as to what I might be doing wrong!'},\n",
       " {'text_field': \"Seem if I do this then it ingests fine \\nmutate { convert =&gt; { '[source][geo][location]' =&gt; 'float' } }\"},\n",
       " {'text_field': 'I am trying to install it on a Windows system (Windows 10 Pro). I got the service to start running and it\\'s even sending output over to Kibana. Unfortunately, it isn\\'t finding any file systems to report on.\\nE, L, K, &amp; Metricbeat are all on version 7.4\\nHere\\'s the system.yml file (almost straight out of the box):\\n- module: system\\n  period: 10s\\n  metricsets:\\n    - cpu\\n    #- load\\n    - memory\\n    - network\\n    - process\\n    - process_summary\\n    - socket_summary\\n    #- entropy\\n    #- core\\n    #- diskio\\n    #- socket\\n  process.include_top_n:\\n    by_cpu: 5      # include top 5 processes by CPU\\n    by_memory: 5   # include top 5 processes by memory\\n\\n- module: system\\n  period: 1m\\n  metricsets:\\n    - filesystem\\n    - fsstat\\n#  processors:\\n#  - drop_event.when.regexp:\\n#      system.filesystem.mount_point: \\'^/(sys|cgroup|proc|dev|etc|host|lib)($|/)\\'\\n\\n- module: system\\n  period: 15m\\n  metricsets:\\n    - uptime\\n\\n#- module: system\\n#  period: 5m\\n#  metricsets:\\n#    - raid\\n#  raid.mount_point: \\'/\\'\\n\\nHere\\'s the output from the Metricbeat log file:\\n2019-11-03T19:46:10.424-0600\\tINFO\\tinstance/beat.go:607\\tHome path: [C:\\\\program files\\\\metricbeat] Config path: [C:\\\\program files\\\\metricbeat] Data path: [C:\\\\ProgramData\\\\metricbeat] Logs path: [C:\\\\ProgramData\\\\metricbeat\\\\logs]\\n2019-11-03T19:46:10.428-0600\\tINFO\\tinstance/beat.go:615\\tBeat ID: d4e03197-8725-4b4f-8059-f6d7c6c53858\\n2019-11-03T19:46:10.455-0600\\tINFO\\t[beat]\\tinstance/beat.go:903\\tBeat info\\t{\"system_info\": {\"beat\": {\"path\": {\"config\": \"C:\\\\\\\\program files\\\\\\\\metricbeat\", \"data\": \"C:\\\\\\\\ProgramData\\\\\\\\metricbeat\", \"home\": \"C:\\\\\\\\program files\\\\\\\\metricbeat\", \"logs\": \"C:\\\\\\\\ProgramData\\\\\\\\metricbeat\\\\\\\\logs\"}, \"type\": \"metricbeat\", \"uuid\": \"d4e03197-8725-4b4f-8059-f6d7c6c53858\"}}}\\n2019-11-03T19:46:10.460-0600\\tINFO\\t[beat]\\tinstance/beat.go:912\\tBuild info\\t{\"system_info\": {\"build\": {\"commit\": \"15075156388b44390301f070960fd8aeac1c9712\", \"libbeat\": \"7.4.2\", \"time\": \"2019-10-28T19:49:39.000Z\", \"version\": \"7.4.2\"}}}\\n2019-11-03T19:46:10.460-0600\\tINFO\\t[beat]\\tinstance/beat.go:915\\tGo runtime info\\t{\"system_info\": {\"go\": {\"os\":\"windows\",\"arch\":\"amd64\",\"max_procs\":8,\"version\":\"go1.12.9\"}}}\\n2019-11-03T19:46:10.475-0600\\tINFO\\t[beat]\\tinstance/beat.go:919\\tHost info\\t{\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2019-11-02T13:54:25.52-05:00\",\"name\":\"XXXXXXX\",\"ip\":[\"XXX.XXX.XXX.XXX/24\",\"XXXX::XXXX:XXXX:XXXX:XXXX/64\",\"XXX.XXX.XXX.XXX/16\",\"XXXX::XXXX:XXXX:XXXX:XXXX/64\",\"XXX.XXX.XXX.XXX/16\",\"XXXX::XXXX:XXXX:XXXX:XXXX/64\",\"XXXX.XXXX.XXXX.XXXX/16\",\"XXXX::XXXX:XXXX:XXXX:XXXX/64\",\"XXX.XXX.XXX.XXX4/16\",\"::1/128\",\"127.0.0.1/8\"],\"kernel_version\":\"10.0.18362.418 (WinBuild.160101.0800)\",\"mac\":[\"10:05:01:48:9e:18\",\"0c:54:15:fc:1f:7f\",\"0c:54:15:fc:1f:80\",\"0e:54:15:fc:1f:7f\",\"0c:54:15:fc:1f:83\"],\"os\":{\"family\":\"windows\",\"platform\":\"windows\",\"name\":\"Windows 10 Pro\",\"version\":\"10.0\",\"major\":10,\"minor\":0,\"patch\":0,\"build\":\"18362.418\"},\"timezone\":\"CST\",\"timezone_offset_sec\":-21600,\"id\":\"dfab0da7-da8c-4d61-b7cb-d855f144c284\"}}}\\n2019-11-03T19:46:10.477-0600\\tINFO\\t[beat]\\tinstance/beat.go:948\\tProcess info\\t{\"system_info\": {\"process\": {\"cwd\": \"C:\\\\\\\\WINDOWS\\\\\\\\system32\", \"exe\": \"C:\\\\\\\\Program Files\\\\\\\\Metricbeat\\\\\\\\metricbeat.exe\", \"name\": \"metricbeat.exe\", \"pid\": 14228, \"ppid\": 980, \"start_time\": \"2019-11-03T19:46:02.761-0600\"}}}\\n2019-11-03T19:46:10.477-0600\\tINFO\\tinstance/beat.go:292\\tSetup Beat: metricbeat; Version: 7.4.2\\n2019-11-03T19:46:10.477-0600\\tINFO\\t[index-management]\\tidxmgmt/std.go:178\\tSet output.elasticsearch.index to \\'metricbeat-7.4.2\\' as ILM is enabled.\\n2019-11-03T19:46:10.477-0600\\tINFO\\telasticsearch/client.go:170\\tElasticsearch url: http://192.168.200.54:9200\\n2019-11-03T19:46:10.477-0600\\tINFO\\t[publisher]\\tpipeline/module.go:97\\tBeat name: XXXXXX\\n2019-11-03T19:46:10.478-0600\\tINFO\\tinstance/beat.go:422\\tmetricbeat start running.\\n2019-11-03T19:46:10.478-0600\\tINFO\\t[monitoring]\\tlog/log.go:118\\tStarting metrics logging every 30s\\n2019-11-03T19:46:10.478-0600\\tINFO\\tcfgfile/reload.go:171\\tConfig reloader started\\n2019-11-03T19:46:13.455-0600\\tINFO\\tadd_cloud_metadata/add_cloud_metadata.go:87\\tadd_cloud_metadata: hosting provider type not detected.\\n2019-11-03T19:46:20.484-0600\\tINFO\\thelper/privileges_windows.go:79\\tMetricbeat process and system info: {\"OSVersion\":{\"Major\":6,\"Minor\":2,\"Build\":9200},\"Arch\":\"amd64\",\"NumCPU\":8,\"User\":{\"SID\":\"S-1-5-21-2830785992-2897304774-1924465196-1001\",\"Account\":\"XXXXX\",\"Domain\":\"XXXXX\",\"Type\":1},\"ProcessPrivs\":{\"SeBackupPrivilege\":{\"enabled\":false},\"SeChangeNotifyPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeCreateGlobalPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeCreatePagefilePrivilege\":{\"enabled\":false},\"SeCreateSymbolicLinkPrivilege\":{\"enabled\":false},\"SeDebugPrivilege\":{\"enabled\":false},\"SeDelegateSessionUserImpersonatePrivilege\":{\"enabled\":false},\"SeImpersonatePrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeIncreaseBasePriorityPrivilege\":{\"enabled\":false},\"SeIncreaseQuotaPrivilege\":{\"enabled\":false},\"SeIncreaseWorkingSetPrivilege\":{\"enabled\":false},\"SeLoadDriverPrivilege\":{\"enabled\":false},\"SeManageVolumePrivilege\":{\"enabled\":false},\"SeProfileSingleProcessPrivilege\":{\"enabled\":false},\"SeRemoteShutdownPrivilege\":{\"enabled\":false},\"SeRestorePrivilege\":{\"enabled\":false},\"SeSecurityPrivilege\":{\"enabled\":false},\"SeShutdownPrivilege\":{\"enabled\":false},\"SeSystemEnvironmentPrivilege\":{\"enabled\":false},\"SeSystemProfilePrivilege\":{\"enabled\":false},\"SeSystemtimePrivilege\":{\"enabled\":false},\"SeTakeOwnershipPrivilege\":{\"enabled\":false},\"SeTimeZonePrivilege\":{\"enabled\":false},\"SeUndockPrivilege\":{\"enabled\":false}}}\\n2019-11-03T19:46:20.489-0600\\tINFO\\thelper/privileges_windows.go:111\\tSeDebugPrivilege is now enabled. SeDebugPrivilege=(Enabled)\\n2019-11-03T19:46:20.496-0600\\tINFO\\tmodule/wrapper.go:252\\tError fetching data for metricset system.fsstat: filesystem list: GetAccessPaths failed: failed to get list of access paths for volume \\'\\\\\\\\?\\\\Volume{2d8ef6be-b9a4-11e8-9c5a-0019860014e4}\\\\\\': GetVolumePathNamesForVolumeNameW failed to get needed buffer length: The system cannot find the file specified.\\n2019-11-03T19:46:20.496-0600\\tINFO\\tmodule/wrapper.go:252\\tError fetching data for metricset system.filesystem: error getting filesystem list: GetAccessPaths failed: failed to get list of access paths for volume \\'\\\\\\\\?\\\\Volume{2d8ef6be-b9a4-11e8-9c5a-0019860014e4}\\\\\\': GetVolumePathNamesForVolumeNameW failed to get needed buffer length: The system cannot find the file specified.\\n\\nThe service is running as a user with SeDebugPrivilege enabled (as I read in some other posts). At this point I\\'m stuck to understanding what is going wrong here. I\\'m assuming there is a config parameter or option I\\'m leaving out, but I haven\\'t been able to figure out which one.\\nDoes anyone have any ideas on how to move forward with this?\\nThanks in advance.'},\n",
       " {'text_field': 'Answering my own issue. The service had to be running as a user with administrative rights, not as the \"system\" account under Windows. Changed the login for the service and all is running properly now.'},\n",
       " {'text_field': 'Trying to setup the SAML configuration towards Azure AD for an elastic cloud setup, but fails to access Kibana when using SAML.\\nAny suggestion on what is wrong?\\n(Extra annoying as i had the the setup working 3 weeks ago)\\nxpack:\\n  security:\\n    authc:\\n      realms:\\n        saml: \\n          cloud-saml: \\n            order: 2\\n            attributes.principal: \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\" \\n            attributes.groups: \"http://schemas.microsoft.com/ws/2008/06/identity/claims/groups\" \\n            idp.metadata.path: \"https://login.microsoftonline.com/e3d6688c-40b8-4418-9df0-b67df377ae68/federationmetadata/2007-06/federationmetadata.xml?appid=051691f5-d0bc-4b33-b872-c1a4cbea8a1a\" \\n            idp.entity_id: \"https://sts.windows.net/e3d6688c-40b8-4418-9df0-b67df377ae68/\" \\n            sp.entity_id: \"https://66d89f1c05584f95a20bd11f32fda1a1.eu-west-1.aws.found.io:9243\" \\n            sp.acs: \"https://66d89f1c05584f95a20bd11f32fda1a1.eu-west-1.aws.found.io:9243/api/security/v1/saml\"\\n            sp.logout: \"https://66d89f1c05584f95a20bd11f32fda1a1.eu-west-1.aws.found.io:9243/logout\"\\n\\t\\t\\t\\t\\t\\t\\n\\nxpack.security.authProviders: [saml,basic]\\nserver.xsrf.whitelist: [/api/security/v1/saml]\\nxpack.security.authc.saml.realm: cloud-saml\\nxpack.security.public:\\n  protocol: https\\n  hostname: 66d89f1c05584f95a20bd11f32fda1a1.eu-west-1.aws.found.io\\n  port: 9243\\n\\nError recieved after azure ad login:\\n{\"error\":\"no handler found for uri [/api/security/v1/saml] and method [POST]\"}'},\n",
       " {'text_field': 'I made an mistake referenced to elasticsearch url instead of the kibana url. Works now'},\n",
       " {'text_field': 'I add log4j2-ecs-layout dependency  to the application\\n\\napm.png929×834 30.5 KB\\n\\nlog4j2.xml\\n\\nlogapm.png886×446 12.5 KB\\n\\nCOMMAND:\\n—————————————————————\\njava -javaagent:/root/elastic-apm-agent-1.11.0.jar \\n-Delastic.apm.service_name=my-cool-service  -Delastic.apm.enable_log_correlation=true \\n-Delastic.apm.application_packages=org.example,org.another.example \\n-Delastic.apm.server_urls=http://localhost:8200 -jar target/dj-development-system-0.0.3-RELEASE.jar\\nI don\\'t know how to reference \"ecsjsonserializer\"\\nHow can I solve this error?\\nLOG：\\n—————————————————————\\n2019-11-04 02:24:03.536 [apm-server-healthcheck] INFO co.elastic.apm.agent.report.ApmServerHealthChecker - Elastic APM server is available: {  \"build_date\": \"2019-09-27T06:58:27Z\",  \"build_sha\": \"971d864356e4438bf4a799a1fa052cfd0ce680b4\",  \"version\": \"7.4.0\"}\\n2019-11-04 02:24:03.635 [main] INFO co.elastic.apm.agent.configuration.StartupInfo - Starting Elastic APM 1.11.0 as my-cool-service on Java 11.0.5 (Oracle Corporation) Linux 3.10.0-862.el7.x86_64\\n2019-11-04 02:24:03.649 [apm-remote-config-poller] INFO co.elastic.apm.agent.configuration.ApmServerConfigurationSource - Received new configuration from APM Server: {transaction_sample_rate=1}\\nException in thread \"main\" java.lang.reflect.InvocationTargetException\\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\\nat org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48)\\nat org.springframework.boot.loader.Launcher.launch(Launcher.java:87)\\nat org.springframework.boot.loader.Launcher.launch(Launcher.java:50)\\nat org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51)\\nCaused by: java.lang.NoClassDefFoundError: co/elastic/logging/EcsJsonSerializer\\nat co.elastic.logging.log4j2.EcsLayout.(EcsLayout.java:97)\\nat co.elastic.logging.log4j2.EcsLayout.(EcsLayout.java:63)\\nat co.elastic.logging.log4j2.EcsLayout$Builder.build(EcsLayout.java:408)\\nat co.elastic.logging.log4j2.EcsLayout$Builder.build(EcsLayout.java:324)\\nat org.apache.logging.log4j.core.config.plugins.util.PluginBuilder.build(PluginBuilder.java:122)\\nat org.apache.logging.log4j.core.config.AbstractConfiguration.createPluginObject(AbstractConfiguration.java:958)\\nat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:898)\\nat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:890)\\nat org.apache.logging.log4j.core.config.AbstractConfiguration.createConfiguration(AbstractConfiguration.java:890)\\nat org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:513)\\nat org.apache.logging.log4j.core.config.AbstractConfiguration.initialize(AbstractConfiguration.java:237)\\nat org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:249)\\nat org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:545)\\nat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:617)\\nat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:634)\\nat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:229)\\nat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:153)\\nat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)\\nat org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)\\nat org.apache.commons.logging.LogFactory$Log4jLog.(LogFactory.java:199)\\nat org.apache.commons.logging.LogFactory$Log4jDelegate.createLog(LogFactory.java:166)\\nat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:109)\\nat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:99)\\nat org.springframework.boot.SpringApplication.(SpringApplication.java:189)\\nat com.djcps.djfileserver.DjdevelopmentsysApplication.main(DjdevelopmentsysApplication.java:34)\\n... 8 more\\nCaused by: java.lang.ClassNotFoundException: co.elastic.logging.EcsJsonSerializer\\nat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\\nat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\\nat org.springframework.boot.loader.LaunchedURLClassLoader.loadClass(LaunchedURLClassLoader.java:93)\\nat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\\n... 33 more'},\n",
       " {'text_field': \"Sorry, I found out I made a mistake. I forgot to use the 'ECS logging core' package\"},\n",
       " {'text_field': \"Hi,\\nI'm trying out the MQTT module of the machinebeat beta after reading about it here: [https://www.elastic.co/blog/industrial-internet-of-things-iiot-with-the-elastic-stack] and I'm having a problem connecting to an MQTT broker using SSL (port 8883). I have specified the cert file paths (.pem format) in the module config, and I can use the same files to connect successfully to the broker using nodejs (node-red) and other MQTT clients.\\nIn the machinebeat logs I get:\\n2019-11-04T23:33:20.933+1300    INFO    instance/beat.go:416    machinebeat start running.\\n2019-11-04T23:33:20.933+1300    INFO    [monitoring]    log/log.go:117  Starting metrics logging every 30s\\n2019-11-04T23:33:20.933+1300    DEBUG   [service]       service/service_windows.go:72   Windows is interactive: true\\n2019-11-04T23:33:20.934+1300    DEBUG   [cfgfile]       cfgfile/reload.go:123   Checking module configs from: c:\\\\elk\\\\machinebeat/modules.d/*.yml\\n2019-11-04T23:33:20.937+1300    DEBUG   [cfgfile]       cfgfile/cfgfile.go:175  Load config from file: c:\\\\elk\\\\machinebeat\\\\modules.d\\\\mqtt.yml\\n2019-11-04T23:33:20.937+1300    DEBUG   [cfgfile]       cfgfile/reload.go:137   Number of module configs found: 1\\n2019-11-04T23:33:20.938+1300    WARN    [cfgwarn]       topic/mqtt.go:55        BETA: The MQTT metricset is beta.\\n2019-11-04T23:33:20.938+1300    INFO    topic/client.go:72      [MQTT] Connect to broker URL: localhost:8883\\n2019-11-04T23:33:20.938+1300    INFO    topic/client.go:85      [MQTT] Broker username: test\\n2019-11-04T23:33:20.939+1300    INFO    topic/client.go:94      [MQTT] Configure session to use SSL\\n2019-11-04T23:33:20.942+1300    INFO    topic/client.go:33      [MQTT] Set the CA\\n2019-11-04T23:33:20.943+1300    INFO    topic/client.go:56      [MQTT] Set the Certs\\n2019-11-04T23:33:20.950+1300    INFO    topic/client.go:107     Failed to connect to broker, waiting 5 seconds and retrying\\nIn the MQTT broker (mosquitto) logs I get:\\n1572863269: OpenSSL Error: error:140260FC:SSL routines:ACCEPT_SR_CLNT_HELLO:unknown protocol\\n1572863269: Socket error on client , disconnecting.\\nAnyone have any ideas on how I can troubleshoot this issue in more depth?\\nThanks,\\ngmackers\"},\n",
       " {'text_field': '\\n\\n\\n gmackers:\\n\\nmosquitto\\n\\n\\nHi gmackers,\\nlets handle that in the github issue you have opened: MQTT module: SSL connection fails · Issue #4 · elastic/Machinebeat · GitHub\\nBest regards\\nFelix'},\n",
       " {'text_field': 'Hey,\\nI would like to auto rollover my ES indexes every daily due the fact where it is still a bit unsure how the retention policy is going to look like. So i set it up like this:\\n\\nimage.png1170×155 16.4 KB\\n\\n\\nimage.png1039×469 28.9 KB\\n\\nThe problem im having is that it is not rolling over and the index says it should if i am not mistaken:\\n\\nimage.png878×190 15.6 KB\\n\\nWhat is wrong here?\\nThanks,\\nPim'},\n",
       " {'text_field': 'I noticed that on default the index lifecycle setting is not there:\\nPUT /_cluster/settings \\n{\\n  \"persistent\": {\\n    \"indices.lifecycle.poll_interval\": \"30m\"\\n  }\\n}\\n\\nIs 30m a normal value btw?'},\n",
       " {'text_field': 'Hi,\\nAfter installation on 1 filebeat agent to a new 7.4.1 Elastic cluster, I have the following errors on my filebeat indices:\\nIndex lifecycle error\\n\\nillegal_argument_exception: index [filebeat-7.4.1-2019.11.04] is not the write index for alias [filebeat-7.4.1]\\n\\nGET filebeat-7.4.1/_ilm/explain\\n    \"filebeat-7.4.1-2019.11.04\" : {\\n      \"index\" : \"filebeat-7.4.1-2019.11.04\",\\n      \"managed\" : true,\\n      \"policy\" : \"filebeat-7.4.1\",\\n      \"lifecycle_date_millis\" : 1572822004864,\\n      \"age\" : \"14.03h\",\\n      \"phase\" : \"hot\",\\n      \"phase_time_millis\" : 1572872508562,\\n      \"action\" : \"rollover\",\\n      \"action_time_millis\" : 1572822260874,\\n      \"step\" : \"check-rollover-ready\",\\n      \"step_time_millis\" : 1572872508562,\\n      \"phase_execution\" : {\\n        \"policy\" : \"filebeat-7.4.1\",\\n        \"phase_definition\" : {\\n          \"min_age\" : \"0ms\",\\n          \"actions\" : {\\n            \"rollover\" : {\\n              \"max_size\" : \"50gb\",\\n              \"max_age\" : \"30d\"\\n            }\\n          }\\n        },\\n        \"version\" : 7,\\n        \"modified_date_in_millis\" : 1572870698729\\n      }\\n    }\\n\\nBut - for me - everything seems fine:\\nGET filebeat-7.4.1-2019.11.04/_alias\\n{\\n  \"filebeat-7.4.1-2019.11.04\" : {\\n    \"aliases\" : {\\n      \"filebeat-7.4.1\" : {\\n        \"is_write_index\" : false\\n      }\\n    }\\n  }\\n}\\n\\nGET _alias/\\n[...]\\n \"filebeat-7.4.1-2019.11.04\" : {\\n    \"aliases\" : {\\n      \"filebeat-7.4.1\" : {\\n        \"is_write_index\" : false\\n      }\\n    }\\n  }\\n[...]\\n\\nGET _ilm/policy\\n[...]\\n  \"filebeat-7.4.1\" : {\\n    \"version\" : 7,\\n    \"modified_date\" : \"2019-11-04T12:31:38.729Z\",\\n    \"policy\" : {\\n      \"phases\" : {\\n        \"hot\" : {\\n          \"min_age\" : \"0ms\",\\n          \"actions\" : {\\n            \"rollover\" : {\\n              \"max_size\" : \"50gb\",\\n              \"max_age\" : \"30d\"\\n            }\\n          }\\n        }\\n      }\\n    }\\n  }\\n\\nWhen requesting for an ILM retry, the error disappear for a moment, but comes back after a while:\\nPOST filebeat-7.4.1-2019.11.04/_ilm/retry\\n{\\n  \"acknowledged\" : true\\n}\\n\\nAny idea?'},\n",
       " {'text_field': 'My bad, using\"index.lifecycle.indexing_complete\": \"true\" to the failed indices and then retry the rollover fixes the issue (https://www.elastic.co/guide/en/elasticsearch/reference/7.x/skipping-rollover.html)'},\n",
       " {'text_field': 'Hi, sorry for my kid question.\\nI have just a node with elasticsearch, also a logstash with filebeat in varius nodes.\\nMy ask is: ¿I can limit the total size of index? I haven´t may HDD and I need limit it.\\nAny idea?\\nI try rollover but, I think than not work .'},\n",
       " {'text_field': 'Yes, Index Lifecycle Management (ILM) or what you call it - rollover - is the right way to go here.\\nSet up a ILM-Policy and apply it to your Index.\\nFor starters see here: https://www.elastic.co/guide/en/elasticsearch/reference/7.4/getting-started-index-lifecycle-management.html'},\n",
       " {'text_field': 'I\\'m new to Kibana and I\\'m trying to automate the research we do every day.\\nI already asked a question on StackOverflow but I\\'m not satisfied with the reply.\\nI work in a company that produce a software for autonomous vehicles. When a booking is rejected we need to understand why.\\nI need to monitor just a few seconds of logs on 3 different machines. Each log is completely separated, there is no relation between the logs so I cannot write a query in discover, I need to run 3 separated queries.\\nEXAMPLE:\\n\\nA booking was rejected, so I open Chrome and I search on \"elk-prod.myhost.com\" for the BookingID: \"b2c729b5-6440-4829-8562-abd81991e2a0\"  and I have a dozen of logs returned during a range of 2 seconds (FROM:  September 3rd 2019, 21:28:22.155 , TO:  September 3rd 2019, 21:28:23.524 ).\\n\\n\\n\\n\\nNow I need to know what was happening on the car so I open a new Chrome tab and I search on \"elk-prod.myhost.com\" for the CarID:  \"Tesla-45-OU\"  on the time range FROM:  September 3rd 2019, 21:28:22.155 , TO:  September 3rd 2019, 21:28:23.524\\n\\n\\nNow I need to know why the server which calculate the matching rejected the booking so I open a new Chrome tab and I search for the word  CalculationMatrix  always on the time range FROM:  September 3rd 2019, 21:28:22.155 , TO:  September 3rd 2019, 21:28:23.524\\n\\n\\nCONCLUSION:  I want to stop to keep opening Chrome tabs by hand and automate the whole thing. I have no idea around what time the book was made so I first need to search for the BookingID  \"b2c729b5-6440-4829-8562-abd81991e2a0\" , then store the timestamp of first and last log and run a second and third query based on those timestamps.\\nThere is no relation between the 3 logs I search so there is no way to filter from the Discover, I need to automate 3 different query.'},\n",
       " {'text_field': \"Dev Tools is useful for running queries against elasticsearch. You can write more than one query and execute them one at a time. If you can write your queries in a useful way it might help you avoid switching windows. I don't think Dev Tools saves the contents so you'd want to copy your queries to a local text file. I think this would be a good starting point as it would be the basis for any more complicated solution.\"},\n",
       " {'text_field': 'Hello,\\nI have a document which contains a tree (of unknown depth) with a lot of information and i want to get only a subset of the data of each node (ex: only the key hello in the following example).\\n{\\n    \"id\": 42,\\n    \"tree\": [\\n        {\\n            \"hello\": \"world\",\\n            \"children\": [\\n                {\\n                    \"hello\": \"europe\",\\n                    \"children\": [\\n                        {\\n                            \"hello\": \"france\",\\n                            \"children\": []\\n                        }\\n                    ]\\n                }\\n            ]\\n        }\\n    ]\\n}\\n\\nI have tried with _source_includes=tree.*hello / tree.*.hello (and a lot of other combination) but i get only the first level. Is there a way to perform an inclusion like this ?\\nThanks for your support '},\n",
       " {'text_field': 'try\\nGET test/_search\\n{\\n  \"_source\": \"**.hello\"\\n}\\n'},\n",
       " {'text_field': 'All I want to do it subtract 6 from the \"hour_of_day\" field, because it\\'s six hours ahead. Which from my understanding, it shouldn\\'t be ahead b/c I used ruby. But whatever the case, it\\'s wrong.\\nHere\\'s my code:\\nruby {\\n\\t\\tcode =&gt; \"event.set(\\'[hour_of_day]\\',(event.get(\\'@timestamp\\').time.strftime(\\'%H\\') - 6))\"\\n\\t}\\n\\nI get error messages on the - b/c it (ruby?) doesn\\'t know what - is.\\nHow can I fix that?\\nMuch appreciated!'},\n",
       " {'text_field': 'strftime returns a string. You are effectively trying to do\\n\"12\" - 6\\n\\nIt will not coerce \"12\" into 12, so you need to use .to_i\\nruby { code =&gt; \"event.set(\\'[hour_of_day]\\', (event.get(\\'@timestamp\\').time.strftime(\\'%H\\').to_i - 6))\" }\\n\\nHow is @timestamp getting set? If it is set by a date filter then you can use the timezone option to adjust the hour.'},\n",
       " {'text_field': 'Hi there,\\nCan I indicate a concrete date in Split Series ---&gt; Date Range ?\\nNow I see only a relative date like that\\n\\nAnd I need to indicate, e.g. the 12th October and 2nd November'},\n",
       " {'text_field': 'Yes, you can specify absolute times in the format yyyy-mm-ddTHH:mm:ss like 2019-11-08T00:00:00'},\n",
       " {'text_field': 'Running version 7.3.1.\\nI have a single node cluster and this is operating fine.  I want to join two other nodes to this cluster.  Each time I try I either get another single node cluster, or elasticsearch will not start.  Here is the output of the yml on the single node.\\n---------------------------------- Cluster -----------------------------------\\n\\nUse a descriptive name for your cluster:\\n\\ncluster.name: Elasticsearch_Test\\n------------------------------------ Node ------------------------------------\\nUse a descriptive name for the node:\\nnode.name: master_node\\n---------------------------------- Network -----------------------------------\\nnetwork.host: a.b.c.d\\nSet a custom port for HTTP:\\nhttp.port: 9200\\n--------------------------------- Discovery ----------------------------------\\ndiscovery.seed_hosts: [\"master_node\", \"new_node1\", \" new_node2\"]\\ncluster.initial_master_nodes: master_node\\n---------------------------------- Gateway -----------------------------------\\nBlock initial recovery after a full cluster restart until N nodes are started:\\n#gateway.recover_after_nodes: 3\\nOutput of the new node.yml file\\n---------------------------------- Cluster -----------------------------------\\ncluster.name: Elasticsearch_Test\\n------------------------------------ Node ------------------------------------\\nnode.name: new_node1\\n---------------------------------- Network -----------------------------------\\nnetwork.host: a.b.c.d\\nSet a custom port for HTTP:\\nhttp.port: 9200\\n--------------------------------- Discovery ----------------------------------\\ndiscovery.seed_providers: master_node\\ndiscovery.seed_hosts: [\"new_node1\", \" new_node2\", \"master_node\"]\\ncluster.initial_master_nodes: master_node\\nminimum_master_nodes: 2\\n---------------------------------- Gateway -----------------------------------\\ngateway.recover_after_nodes: 2\\nTrying to start elasticsearch on new_node1 with these files causes ES to error out and not start. I can\\'t figure out if I have a configuration wrong or if something is preventing these hosts from connecting.'},\n",
       " {'text_field': 'I have resolved the issue.  Turns out you should disable firewalld...\\nThanks for the help.'},\n",
       " {'text_field': 'Hi.\\nI\\'ve created two conf files under /etc/logstash/conf.d. Because I have two businesses in two different countries, I want them to be displayed in two separately indexes.\\n\\nIndex name nz-*\\n\\nIndex name au-*\\n\\n\\nHowever, in Kibana when discovering the data, even if I select only index nz-*, I can still see data from the other one.\\n\\nScreenshot from 2019-11-05 08-31-33.png773×539 24.8 KB\\n\\nWhat am I doing wrong?\\nbusiness1.conf:\\noutput {\\n    elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        # Weekly index (for pruning)\\n        index =&gt; \"au-log-index-%{+YYYY.\\'w\\'ww}\"\\n    }\\n    stdout { codec =&gt; rubydebug }\\n}\\n\\nbusiness2.conf:\\noutput {\\n    elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        # Weekly index (for pruning)\\n        index =&gt; \"nz-log-index-%{+YYYY.\\'w\\'ww}\"\\n    }\\n    stdout { codec =&gt; rubydebug }\\n}'},\n",
       " {'text_field': 'If both configurations are running in the same pipeline then every event will get written to both elasticsearch outputs. You need to either run each business in its own pipeline or use conditionals in your processing.'},\n",
       " {'text_field': 'Curator 5.8.1\\nElasticsearch 7.3.0\\nThis is mostly informational.\\ncurator_cli was working fine with Elastic 6.6.0\\nWhen connecting into our new version (7.3.0), actions were failing with Failed to complete action: &lt;type \\'exceptions.KeyError\\'&gt;: \\'indices\\'\\nI spotted a few a posts out there and followed the step to increase logging.\\nThe logging revealed a 403 - unauthorized\\nIt seems like the first thing curator does is collect stats on all indices.\\nSecurity is use and the account has monitor, view_index_metadata permission to \"*\" indices.'},\n",
       " {'text_field': \"It was necessary to add allow_restricted_indices=true into account's role.\"},\n",
       " {'text_field': \"I have two fields with the same value, 99% of the time.  I'd like to filter down to just the entries where the two fields are NOT the same, is it possible to do something like [field1] != [field2]?\"},\n",
       " {'text_field': \"There are different ways of doing this depending upon which kibana features you're using. Here's how to express it in query form - Compare fields\\nIs that helpful? If not, let me know which version of kibana you're using and which feature and I'll figure out something more specific to your use case.\"},\n",
       " {'text_field': 'Hello,\\nI have an issue with timezone conversion in timelion, I care about event dates only (no hours) so I ingested data via logstash using date filter as follows:\\n\\ndate {\\nmatch =&gt; [\"EVENT_START_DATE\", \"yyyyMMdd\"]\\ntimezone =&gt; \"UTC\"\\ntarget =&gt; \"@timestamp\"\\n}\\n\\nIn Kibana advanced settings I set UTC timezone as well. I have checked Data table and Charts and they show the metrics correctly for each day, however, Timelion disregarded the Advanced Settings and considered the server timezone (UTC -03:00). Changing the server\\'s timezone to UTC seemed to help, but metric values are still being shown on incorrect dates regardless of the Interval used.\\n#Query results for reference\\n\\n{\\n\"key\" : 1570665600000,\\n\"key_as_string\" : \"20191010\",\\n\"doc_count\" : 8,\\n\"SUM_AMOUNT\" : {\\n\"value\" : 158.89230865366153\\n}\\n},\\n{\\n\"key\" : 1570579200000,\\n\"key_as_string\" : \"20191009\",\\n\"doc_count\" : 6,\\n\"SUM_AMOUNT\" : {\\n\"value\" : 432.98415810004917\\n}\\n}\\n\\n\\n#Timelion query - consider just the AMOUNT metric for this post\\n.es(split=TRAFFIC_TYPE:3,index=aggregated-teste, timefield=EVENT_START_DATE, metric=sum:CALL_COUNT, metric=sum:CHARGED_USAGE, metric=sum:DATA_VOLUME, metric=sum:AMOUNT)\\n\\nKibana 7.3.2 (also tested with 7.4.0)\\n\\ntimelion.jpg542×730 126 KB\\n\\nIs timelion capable of handling only dates correctly? I wanted to use a simple line chart but it does not have the Offset feature I will need.'},\n",
       " {'text_field': 'It looks like this has been fixed recently and will be in the upcoming 7.6 release. Apologies for the trouble.\\n\\n  \\n      github.com/elastic/kibana\\n  \\n  \\n    \\n  \\n    \\n  \\n\\n  \\n    \\n      Timelion: Move datemath parsing to the client\\n    \\n\\n    \\n      elastic:master ← flash1293:flash1293/timelion-timezone-12678\\n    \\n\\n    \\n      \\n        opened 12:20PM - 10 Oct 19 UTC\\n      \\n\\n      \\n        \\n          \\n          flash1293\\n        \\n      \\n\\n      \\n        \\n          +30\\n          -135\\n        \\n      \\n    \\n\\n  \\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n'},\n",
       " {'text_field': \"Hi,\\nWe are seeing the following deprecation log messages in our 6.8 cluster:\\nThe return type of [getDayOfWeek()] will change to an enum in 7.0. Use getDayOfWeekEnum().getValue().\\nHowever, having upgraded our test cluster to 7.4, we're still seeing getDayOfWeek() returning an integer. What's going on here? I imagine the advice is to switch to the new method regardless?\\nThanks.\"},\n",
       " {'text_field': 'Hey,\\nindeed. The code is still in there, but you should switch to using the enum nontheless, as the other code will be removed in the future.\\n--Alex'},\n",
       " {'text_field': '\\nkibana icon is too simple to me (ELK beginner)\\nso i want fix expand menu always but this page collapse menu continuously\\ncan i fix it?'},\n",
       " {'text_field': 'Hello,\\nThere is a lock icon next to collapse button. If you click on it - your side nav is always fixed.\\n\\ntest.png2880×1800 823 KB\\n\\nThanks,\\nBhavya'},\n",
       " {'text_field': 'Hi,\\nI am running Elasticsearch on eks cluster. for backup to s3 I have added repository-s3 plugin. When I run following command,\\ncurl -X PUT \"localhost:9200/_snapshot/my_s3_repository?pretty\" -H \\'Content-Type: application/json\\' -d\\'\\n{\\n  \"type\": \"s3\",\\n  \"settings\": {\\n    \"bucket\": \"uat-k8-elasticsearch-bckup\",\\n    \"region\": \"ap-south-1\",\\n    \"endpoint\": \"s3.ap-south-1.amazonaws.com\"\\n  }\\n}\\n\\'\\n\\nI receive ..\\n{\\n  \"acknowledged\" : true\\n}\\n\\nBut there is no upload on s3. Let me know how I can debug it.\\nThanks in advance.'},\n",
       " {'text_field': 'Hi @suratpyari,\\nDid you also run snapshot creation in addition to creating the repository?\\nYou need to do another call of the form:\\nPUT /_snapshot/my_s3_repository/snapshot_1?wait_for_completion=true\\n\\nto actually create a snapshot in the repository you just created.\\nSee the documentation here for details.'},\n",
       " {'text_field': 'Hello,\\nI am using Metricbeat in docker to monitor my systems Metrics as well as my docker metrics.\\nUnfortunately, i get an error in Kibana showing this: \\n2019-11-05_08h55_20.png350×1051 104 KB\\n\\nMy metricbeat config looks like this:\\n    metricbeat.autodiscover:\\n  providers:\\n    - type: docker\\n      hints.enabled: true\\n\\nmetricbeat.modules:\\n#- module: nginx\\n#  enabled: true\\n#  metricsets: [\"stubstatus\"]\\n#  period: 20s\\n#  hosts: [\"http://nginx:80\"]\\n#  server_status_path: \"server-status\"\\n\\n- module: docker\\n  metricsets:\\n    - \"container\"\\n    - \"cpu\"\\n    - \"diskio\"\\n    - \"event\"\\n    - \"healthcheck\"\\n    - \"info\"\\n    #- \"image\"\\n    - \"memory\"\\n    - \"network\"\\n  hosts: [\"unix:///var/run/docker.sock\"]\\n  period: 30s\\n  enabled: true\\n\\n- module: system\\n  metricsets:\\n    - cpu             # CPU usage\\n    - load            # CPU load averages\\n    - memory          # Memory usage\\n    - network         # Network IO\\n    - process         # Per process metrics\\n    - process_summary # Process summary\\n    - uptime          # System Uptime\\n    - socket_summary  # Socket summary\\n    #- core           # Per CPU core usage\\n    #- diskio         # Disk IO\\n    #- filesystem     # File system usage for each mountpoint\\n    #- fsstat         # File system summary metrics\\n    #- raid           # Raid\\n    #- socket         # Sockets and connection info (linux only)\\n  enabled: false\\n  period: 20s\\n  processes: [\\'.*\\']\\n\\n  # Configure the metric types that are included by these metricsets.\\n  cpu.metrics:  [\"percentages\"]  # The other available options are normalized_percentages and ticks.\\n  core.metrics: [\"percentages\"]  # The other available option is ticks.\\n\\nMy Docker Compose file looks like this:\\n\\nservices:\\nmetricbeat:\\ncontainer_name: metricbeat\\nbuild: ${PWD}/config/metricbeat\\nrestart: always\\nuser: root\\nvolumes:\\n-  /proc:/hostfs/proc:ro\\n-  /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro\\n- /:/hostfs:ro\\n- /var/run:/var/run\\nnetworks:\\n- push-network\\ncommand: metricbeat -e -system.hostfs=/hostfs\\n\\nIn discover, I can see the incoming data from my different docker containers:\\n\\n2019-11-05_09h03_37.png1035×676 54.2 KB\\n\\nDoes anyone has an idea how to fix this?\\nThanks! '},\n",
       " {'text_field': 'Thanks for the info, removing the index actually solved the issue.'},\n",
       " {'text_field': 'Hi,\\nI\\'m trying to launch logstash with my .conf file but it tell me an error that I don\\'t understand :\\n\\nOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.\\nWARNING: An illegal reflective access operation has occurred\\nWARNING: Illegal reflective access by com.headius.backport9.modules.Modules (file:/usr/share/logstash/logstash-core/lib/jars/jruby-complete-9.2.8.0.jar) to field java.io.FileDescriptor.fd\\nWARNING: Please consider reporting this to the maintainers of com.headius.backport9.modules.Modules\\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\\nWARNING: All illegal access operations will be denied in a future release\\nThread.exclusive is deprecated, use Thread::Mutex\\nWARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults\\nCould not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console\\n[WARN ] 2019-11-05 09:44:43.533 [LogStash::Runner] multilocal - Ignoring the \\'pipelines.yml\\' file because modules or command line options are specified\\n[INFO ] 2019-11-05 09:44:43.568 [LogStash::Runner] runner - Starting Logstash {\"logstash.version\"=&gt;\"7.4.1\"}\\n[ERROR] 2019-11-05 09:44:45.887 [Converge PipelineAction::Create] agent - Failed to execute action {:action=&gt;LogStash::PipelineAction::Create/pipeline_id:main, :exception=&gt;\"LogStash::ConfigurationError\", :message=&gt;\"Expected one of #, {, } at line 24, column 27 (byte 454) after #Fichier de configuration de Logstash\\\\n#Utilisation du plugin syslog\\\\n\\\\ninput {\\\\n# udp {\\\\n#  #Configuration de deux paramètre : chemin et type\\\\n#  port =&gt; 514\\\\n#  type =&gt; syslog\\\\n# }\\\\n# tcp {\\\\n#  port =&gt; 514\\\\n#  type =&gt; syslog\\\\n# }\\\\n file {\\\\n  path=&gt;[\"/home/logstash/testdata.log\"]\\\\n  sincedb_path =&gt; \"/dev/null\"\\\\n  start_position =&gt; \"beginning\\\\n }\\\\n}\\\\nfilter {\\\\n#Dans le cas de multiple filtres, ils sont traités par ordre d\\'apparence\\\\n}\\\\noutput {\\\\n# elasticsearch {hosts=&gt;[\"\", :backtrace=&gt;[\"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:41:in compile_imperative\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:49:in compile_graph\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:11:in block in compile_sources\\'\", \"org/jruby/RubyArray.java:2584:in map\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:10:in compile_sources\\'\", \"org/logstash/execution/AbstractPipelineExt.java:153:in initialize\\'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:26:in initialize\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:326:in block in converge_state\\'\"]}\\n[INFO ] 2019-11-05 09:44:46.465 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=&gt;9600}\\n[INFO ] 2019-11-05 09:44:51.245 [LogStash::Runner] runner - Logstash shut down.\\n\\nIf someone understand\\nThanks\\n\\nBlockquote\\n'},\n",
       " {'text_field': '\\n\\n\\n lgllrd:\\n\\nstart_position =&gt; \"beginning\\\\n }\\\\n}\\\\nfilter {\\\\n#Dans le cas de multiple filtres, ils sont traités par ordre d\\'apparence\\\\n}\\\\noutput {\\\\n# Elasticsearch {hosts=&gt;[\"\\n\\n\\nYou do not have a closing quote on the value of start_position, so it is consuming everything up to the opening quote of the hosts option on the Elasticsearch output, then failing to parse.'},\n",
       " {'text_field': 'Rererence : https://www.elastic.co/blog/kibana-plugin-developers-meet-elasticsearch-clusters\\nWhen using callWithInternalUser instead of callWithRequest  i got this error\\nCan someone tell me why this error and how we can fix it ?\\nDebug: internal, implementation, error\\n    TypeError: endpoint.split is not a function\\n    at split (C:\\\\kibana_dev\\\\kibana\\\\src\\\\core\\\\server\\\\elasticsearch/cluster_client.ts:73:31)\\n    at ClusterClient.callAPI [as callAsInternalUser] (C:\\\\kibana_dev\\\\kibana\\\\src\\\\core\\\\server\\\\elasticsearch/cluster_client.ts:155:18)\\n    at Cluster.callAsInternalUser [as callWithInternalUser] (C:\\\\kibana_dev\\\\kibana\\\\src\\\\legacy\\\\core_plugins\\\\elasticsearch\\\\lib/cluster.ts:45:37)\\n    at callWithInternalUser (C:\\\\kibana_dev\\\\kibana\\\\plugins\\\\s-one-case-management\\\\server\\\\routes/cases.js:51:40)\\n    at module.exports.internals.Manager.execute (C:\\\\kibana_dev\\\\kibana\\\\node_modules\\\\hapi\\\\lib\\\\toolkit.js:35:106)\\n    at Object.internals.handler (C:\\\\kibana_dev\\\\kibana\\\\node_modules\\\\hapi\\\\lib\\\\handler.js:50:48)\\n    at exports.execute (C:\\\\kibana_dev\\\\kibana\\\\node_modules\\\\hapi\\\\lib\\\\handler.js:35:36)\\n    at Request._lifecycle (C:\\\\kibana_dev\\\\kibana\\\\node_modules\\\\hapi\\\\lib'},\n",
       " {'text_field': \"Hey @ylasri, when using callWithRequest the first parameter is the Hapi Request object:\\nawait callWithRequest(req, 'search', query)\\n\\nwhere-as, when using callWithInternalUser this isn't required:\\ncallWithInternalUser('search', query)\\n\"},\n",
       " {'text_field': 'Hey there\\nI got product data with nested articles and the products got different types.\\nNow I should search in the product.name and product.description and in the product.articles.gtin\\nand should retrieve only products with the specified type.\\nTried filtered multi_match but got a parse error with that query:\\nGET products_1_en/_search\\n{\\n\"query\": {\\n\"filtered\" : {\\n\"query\": {\\n\"multi_match\": {\\n\"query\": \"Bar grills\",\\n\"fields\": [\\n\"name\",\\n\"description\",\\n\"articles.gtin\"\\n],\\n\"type\": \"best_fields\"\\n}\\n},\\n\"filter\": {\\n\"term\": {\\n\"type\": \"ARTICLE_PRODUCT\"\\n}\\n}\\n}\\n}\\n}\\nThat one is working\\nGET products_1_en/_search\\n{\\n\"query\":{\\n\"multi_match\": {\\n\"query\": \"Bar grills\",\\n\"fields\": [\\n\"name\",\\n\"description\",\\n\"articles.gtin\"\\n],\\n\"type\": \"best_fields\"\\n}\\n}\\n}\\nbut there is no filter and if I search for a gtin number I retrieve no hits.\\nI there a possibility to do that?\\n{\\n\"properties\":{\\n\"description\":{\\n\"type\":\"text\"\\n},\\n\"id\":{\\n\"type\":\"long\"\\n},\\n\"name\":{\\n\"type\":\"text\",\\n\"fields\":{\\n\"keyword\":{\\n\"type\":\"keyword\",\\n\"ignore_above\":256\\n}\\n}\\n},\\n\"type\":{\\n\"type\":\"keyword\"\\n},\\n\"articles\":{\\n\"type\":\"nested\",\\n\"properties\":{\\n\"description\":{\\n\"type\":\"text\",\\n\"fields\":{\\n\"keyword\":{\\n\"type\":\"keyword\",\\n\"ignore_above\":256\\n}\\n}\\n},\\n\"gtin\":{\\n\"type\":\"keyword\"\\n},\\n\"id\":{\\n\"type\":\"long\"\\n},\\n\"name\":{\\n\"type\":\"text\",\\n\"fields\":{\\n\"keyword\":{\\n\"type\":\"keyword\",\\n\"ignore_above\":256\\n}\\n}\\n},\\n\"productId\":{\\n\"type\":\"long\"\\n},\\n\"uid\":{\\n\"type\":\"keyword\"\\n}\\n}\\n}\\n}\\n}\\nThanks for help.'},\n",
       " {'text_field': 'The filtered query does not exist any more. Nowadays you have to use a bool query instead:\\nGET products_1_en/_search\\n{\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": {\\n        \"multi_match\": {\\n          \"query\": \"Bar grills\",\\n          \"fields\": [\\n            \"name\",\\n            \"description\",\\n            \"articles.gtin\"\\n          ],\\n          \"type\": \"best_fields\"\\n        }\\n      },\\n      \"filter\": {\\n        \"term\": {\\n          \"type\": \"ARTICLE_PRODUCT\"\\n        }\\n      }\\n    }\\n  }\\n}\\n'},\n",
       " {'text_field': 'Hey guys,\\nI\\'m a little bit struggling with Logtash parsing mysql-slow.log file.\\nI configured Filebeat to send logs from the file into Elasticsearch through Logstash so I can see recieved messages in Kibana like this:\\nNov 5, 2019 @ 12:14:26.250\\t/var/log/mysql/mysql-slow.log\\t\\n\\n# User@Host: user[user] @ localhost []  Id:     2\\n# Query_time: 0.000572  Lock_time: 0.000137 Rows_sent: 2  Rows_examined: 2\\nSET timestamp=1572952461;\\nselect * from Persons;\\n\\nI\\'d like to parse the message by using Logstash but every pattern I used led to some problem in communication between Filebeat and Logstash. For example I used the following configuration file for Logstash:\\ninput {\\n       beats { port =&gt; 5044 \\n       codec =&gt; multiline{ \\n             pattern =&gt; \"^# Time:\" \\n             negate =&gt; true \\n             what =&gt; previous } \\n      } \\n}\\n    filter {\\n\\n      grok { match =&gt; [ \"message\", \"^# User@Host: %{USER:query_user}(?:[[^]]+])?s+@s+%{HOSTNAME:query_host}?s+[%{IP:query_ip}?]\" ] }\\n\\n      grok { match =&gt; [ \"message\", \"^# Thread_id: %{NUMBER:thread_id:int}s+Schema: %{USER:schema}s+Last_errno: %{NUMBER:last_errno:int}s+Killed: %{NUMBER:killed:int}\"] }\\n\\n      grok { match =&gt; [ \"message\", \"^# Query_time: %{NUMBER:query_time:float}s+Lock_time: %{NUMBER:lock_time}s+ Rows_sent: %{NUMBER:rows_sent:int} s+Rows_examined: %{NUMBER:rows_examined:int}s+Rows_affected: %{NUMBER:rows_affected:int}s+Rows_read: %{NUMBER:rows_read:int}\"] }\\n\\n      grok { match =&gt; [ \"message\", \"^# Bytes_sent: %{NUMBER:bytes_sent:float}\"] }\\n\\n      grok { match =&gt; [ \"message\", \"^SET timestamp=%{NUMBER:timestamp}\" ] }\\n\\n      grok { match =&gt; [ \"message\", \"^SET timestamp=%{NUMBER};s+%{GREEDYDATA:query}\" ] }\\n\\n      date { match =&gt; [ \"timestamp\", \"UNIX\" ] }\\n\\n      mutate { remove_field =&gt; \"timestamp\" }\\n\\n    }\\n\\noutput {\\n  elasticsearch {\\nhosts =&gt; [\"localhost:9200\"]\\nindex =&gt; \"mysql_logs\"\\n  }\\n}\\n\\nAnd it doesn\\'t work because multiline codec isn\\'t supported with beats... I set multiline in input section of filebeat.yml file:\\nmultiline.pattern: \\'^#\\'\\nmultiline.negate: true\\nmultiline.match: after\\n\\nSo now I can run Logstash and Filebeat but filters don\\'t match with lines and it doesn\\'t give me any fields that are defined in config file in Logstash.\\nDoes anyone hav an idea what could be wrong?\\nThank you for any help!'},\n",
       " {'text_field': 'OK, so if you are combining lines in filebeat then logstash will see a multi-line event. You can use grok on that just like any other event\\n    grok {\\n        break_on_match =&gt; false\\n        match =&gt; {\\n            \"message\" =&gt; [\\n                \"Time: %{NOTSPACE:ts}\",\\n                \"User@Host: %{USER:user}\\\\[%{USER}\\\\] @ %{HOSTNAME:hostname}\",\\n                \"Query_time: %{NUMBER:queryTime:float}\",\\n                \"Rows_sent: %{NUMBER:rowsSent:int}\"\\n            ]\\n        }\\n    }\\n\\netc.'},\n",
       " {'text_field': 'Hi everyone,\\nI\\'m pacing days on the ELK + filebeat to make it work with my Glok filter.\\nI have started installing ELK + filebeat, then defined logstash.conf:\\n\\ninput {   file {\\n        path =&gt; \"D:\\\\ElisticLogs\\\\MyLog.log\"\\n    \\tstart_position =&gt; \"beginning\"\\n    \\t}\\n    \\t}\\n    filter {\\n    \\t\\t  grok {\\n    \\t\\t\\tmatch =&gt; { \"message\" =&gt; \"(?&lt;test&gt;.*\" }\\n    \\t}\\n    }\\n\\noutput {\\n\\tstdout { codec =&gt; rubydebug }\\n\\telasticsearch { hosts =&gt; [\"localhost:9200\"] }\\n}\\n\\n\\nEverything seems to be working but there is no rubydebug messages in the console of logstash, glok filter doesn\\'t create my field.\\nLogstash log:\\n\\n[2019-11-05T15:31:13,280][WARN ][logstash.config.source.multilocal] Ignoring the \\'pipelines.yml\\' file because modules or command line options are specified [2019-11-05T15:31:13,311][INFO ][logstash.runner          ] Starting Logstash {\"logstash.version\"=&gt;\"7.4.2\"} [2019-11-05T15:31:14,998][INFO ][org.reflections.Reflections] Reflections took 47 ms to scan 1 urls, producing 20 keys and 40 values  [2019-11-05T15:31:15,780][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization.  It is recommended to log an issue to the responsible developer/development team. [2019-11-05T15:31:15,780][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=&gt;\"main\", \"pipeline.workers\"=&gt;12, \"pipeline.batch.size\"=&gt;125, \"pipeline.batch.delay\"=&gt;50, \"pipeline.max_inflight\"=&gt;1500, :thread=&gt;\"#&lt;Thread:0x1ea9b1e1 run&gt;\"} [2019-11-05T15:31:16,139][INFO ][logstash.inputs.beats    ][main] Beats inputs: Starting input listener {:address=&gt;\"0.0.0.0:5043\"} [2019-11-05T15:31:16,155][INFO ][logstash.javapipeline    ][main] Pipeline started {\"pipeline.id\"=&gt;\"main\"} [2019-11-05T15:31:16,217][INFO ][logstash.agent           ] Pipelines running {:count=&gt;1, :running_pipelines=&gt;[:main], :non_running_pipelines=&gt;[]} [2019-11-05T15:31:16,233][INFO ][org.logstash.beats.Server][main] Starting server on port: 5043 [2019-11-05T15:31:16,514][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=&gt;9600}\\n\\nStack version info:\\nlogstash-7.4.2\\nkibana-7.4.2\\nelasticsearch-7.4.2\\nfilebeat-7.4.2\\nDo you have any idea my this might be happening?'},\n",
       " {'text_field': '\\n\\n\\n Andrii_Fon:\\n\\npath =&gt; \"D:\\\\ElisticLogs\\\\MyLog.log\"\\n\\n\\nDo not use backslash in the path option on a file input, use forward slash.'},\n",
       " {'text_field': 'Hi all,\\nim stuck in how to use the search in right way.\\nWhat I did to get all out of my Index:\\nPUT /myindex/_settings\\n{ \"index\" : { \"max_result_window\" : 100000 } }\\n\\nNow with following GET Request I can show all the index entries = Check\\nGET /myindex/_search\\n{\\n  \"size\": 50000,\\n  \"aggs\": {\\n    \"allEntries\": {\\n      \"terms\": {\\n        \"field\": \"Nr.keyword\"\\n      }\\n    }\\n  }\\n}\\n\\nThe result is as json with more then 3milion lines...wow...ok\\nWhat I want is a result that shrinked down to just show me the 2 fields :\\n\"Nr.keyword\"\\n\"Nr_alternate.keyword\"\\nAt least around 15000 entries\\nHowever, I do not want all the other fields around so how can I get just 1 or 2 fields as json result?\\nThanks and Brgds'},\n",
       " {'text_field': 'Hi!\\nOf course you can get just 1 or 2 fields in the result! You can use a source-filter as described here.\\nYour query then will look something like this:\\nGET /myindex/_search\\n{\\n  \"size\": 50000,\\n  \"aggs\": {\\n    \"allEntries\": {\\n      \"terms\": {\\n        \"field\": \"Nr.keyword\"\\n      }\\n    }\\n  },\"_source\": [\"Nr.keyword\", \"Nr_alternate.keyword\"]\\n}\\n\\nor whichever fields you want to have returned. Just put them in the array...'},\n",
       " {'text_field': 'Hi there,\\nI have tried a few things with IP filtering and I have a question about how can I restrict traffic to the cloud UI.\\nWe are using Terminalserver where we want to access the Cloud UI and block every other connection.\\nFor now I just tried to create a ruleset and added it to the \"admin-console-elasticsearch\". After that I was able to login but when I Login I get:\\nThere was a problem communicating with the system cluster. Origin status code [403 Forbidden].\\n\\nSo my question is, is there a way to restrict the Cloud UI with the IP-Filter and how?'},\n",
       " {'text_field': 'Unfortunately I believe the only way of IP restricting traffic to the Cloud UI is externally to ECE, eg using OS level IP filtering on the UI/API ports (12443)'},\n",
       " {'text_field': 'Running Elasticsearch with the following command\\ndocker run -d --name elasticsearch --rm --name es -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:6.8.4\\n\\nI\\'m getting the following error on a basic INFO API request:\\njava.io.IOException: Unable to parse response body for Response{requestLine=GET / HTTP/1.1, host=http://localhost:9200, response=HTTP/1.1 200 OK}\\n\\n    at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1275)\\n    at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1231)\\n    at org.elasticsearch.client.RestHighLevelClient.info(RestHighLevelClient.java:393)\\n    at io.test.ElasticTest.should get info(ElasticTest.groovy:40)\\nCaused by: org.elasticsearch.common.xcontent.XContentParseException: [15:3] [org.elasticsearch.action.main.MainResponse] failed to parse field [version]\\n    at org.elasticsearch.common.xcontent.ObjectParser.parseValue(ObjectParser.java:316)\\n    at org.elasticsearch.common.xcontent.ObjectParser.parseSub(ObjectParser.java:326)\\n    at org.elasticsearch.common.xcontent.ObjectParser.parse(ObjectParser.java:168)\\n    at org.elasticsearch.common.xcontent.ObjectParser.apply(ObjectParser.java:182)\\n    at org.elasticsearch.action.main.MainResponse.fromXContent(MainResponse.java:150)\\n    at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1406)\\n    at org.elasticsearch.client.RestHighLevelClient.lambda$performRequestAndParseEntity$9(RestHighLevelClient.java:1232)\\n    at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1273)\\n    ... 3 more\\nCaused by: java.lang.IllegalStateException: unexpected distribution type [docker]; your distribution is broken\\n    at org.elasticsearch.Build$Type.fromDisplayName(Build.java:106)\\n    at org.elasticsearch.action.main.MainResponse.lambda$static$4(MainResponse.java:140)\\n    at org.elasticsearch.common.xcontent.ObjectParser.lambda$declareField$1(ObjectParser.java:213)\\n    at org.elasticsearch.common.xcontent.ObjectParser.parseValue(ObjectParser.java:314)\\n\\nAny clue what\\'s wrong?'},\n",
       " {'text_field': \"Hi David, I've solved upgrading the client to the same version as Elasticsearch server. thanks!\"},\n",
       " {'text_field': 'Hi all,\\n    I have a data format as shown in the attached image -\\n\\nI\\'m able to import the data via the CSV input plugin which works great.  However, I\\'m stuck on the next step which is to map the rows to numbers so that the data can be analyzed in kibana.  The issue is that the values should generally be numbers as in the second two columns, however, if there is an error with the data point at some point in time, an error code will be generated as shown in the last two columns.  Is there some way to map the columns to number datatype while also handling the occasional case where the value will be a string?\\nFor reference, below is my current logstash config which needs to be expanded upon\\ninput {\\nfile {\\npath =&gt; \"C:/Users/zach/Downloads/pdr*.csv\"\\nstart_position =&gt; \"beginning\"\\nsincedb_path =&gt; \"NUL\"\\n}\\n}\\nfilter {\\ncsv {\\nseparator =&gt; \",\"\\nautodetect_column_names =&gt; true\\nautogenerate_column_names =&gt; true\\n}\\n}\\noutput {\\nstdout { codec =&gt; rubydebug }\\nelasticsearch {\\nhosts =&gt; [\"localhost:9200\"]\\nindex =&gt; \"pdr-data\"\\n}\\n}'},\n",
       " {'text_field': '\\n\\n\\n zmink-pxc:\\n\\nIs there some way to map the columns to number datatype while also handling the occasional case where the value will be a string?\\n\\n\\nIn elasticsearch, if you have a template, then if a field is expected to be an integer I think (I have not tested) that you would get a mapping exception if you try to ingest a document where it is a string that cannot be parsed as a number.\\nIf you do not have a template then you run the risk that the first document indexed contains a string in that field and the field type gets set to text.\\nYou could record the fact that the field contained an error in another field, and then remove it. Something like\\n    ruby {\\n        code =&gt; \\'\\n            errors = []\\n            event.to_hash.each { |k, v|\\n                if k =~ /column[0-9]+/\\n                    unless v.to_f.to_s == v.to_s\\n                        event.remove(k)\\n                        errors &lt;&lt; k\\n                    end\\n                end\\n            if errors != []\\n                event.set(\"errorFields\", errors)\\n            end\\n            }\\n        \\'\\n    }'},\n",
       " {'text_field': 'Всем привет, есть конфиг:\\n#recieve syslog on Ports\\ninput {\\n  tcp {\\n    port =&gt; 5044\\n    type =&gt; syslog\\n    tags =&gt; \"ise\"\\n  }\\n  udp {\\n    port =&gt; 5044\\n    type =&gt; syslog\\n    tags =&gt; \"ise\"\\n  }\\n}\\n\\nfilter {\\n  if \"ise\" in [tags] {\\n    grok {\\n      patterns_dir =&gt; [\"/etc/logstash/paterns\"]\\n      match =&gt; { \"message\" =&gt; \"&lt;181&gt;%{WORD:month}  %{NUMBER:trash1} %{TIME:trash2} %{DATA:Server} %{DATA:type_r} %{DATA:trash3} %{NUMBER:trash4} %{NUMBER:trash5} %{DATE_12:Logged_at} %{DATA:trash6} %{NUMBER:trash7} %{NUMBER:trash8} %{WORD:facility} Radius-Accounting: %{DATA:type_request}, ConfigVersionId=%{NUMBER:trash9}, Device IP Address=%{IP:device_ip}, RequestLatency=%{NUMBER:trash10}, NetworkDeviceName=%{DATA:device_name}, User-Name=%{DATA:Identity}, NAS-IP-Address=%{IP:NAS-IP-Address}, NAS-Port=%{NUMBER:trash11}, Service-Type=%{DATA:trash12}, Framed-Protocol=%{DATA:protocol}, Framed-IP-Address=%{IP:Framed-IP-Address}, Class=%{DATA:class}, Called-Station-ID=%{IP:Called-Station-ID}, Calling-Station-ID=%{IP:Calling-Station-ID}, Acct-Status-Type=%{WORD:type_req}, Acct-Delay-Time=%{NUMBER:Delay}, Acct-Input-Octets=%{NUMBER:input_octets}, Acct-Output-Octets=%{NUMBER:output_octets}, Acct-Session-Id=%{DATA:session_id}, Acct-Authentic=%{DATA:acc_auth}, Acct-Session-Time=%{NUMBER:duration}, Acct-Input-Packets=%{NUMBER:input_packets}, Acct-Output-Packets=%{NUMBER:output_packets}, Acct-Terminate-Cause=%{DATA:Account_terminate_cause}, NAS-Port-Type=%{DATA:nas_port}, Tunnel-Client-Endpoint=%{DATA:trash14} %{IP:endpoint_id}, cisco-av-pair=mdm-tlv=device-platform=%{DATA:device_os}, cisco-av-pair=mdm-tlv=device-platform-version=%{DATA:device_os_version}, \" }\\n    }\\n  }\\n}\\n\\n#send data to elastic\\noutput {\\n  if \"ise\" in [tags] {\\n    elasticsearch {\\n      hosts =&gt; [\"http://127.0.0.1:9200\"]\\n      index=&gt;[\"ise-%{+YYYY.MM.dd}\"]\\n    }\\n  }\\n}\\n\\nЕще есть pattern:\\nDATE_12 %{YEAR}[/-]%{MONTHNUM}[/-]%{MONTHDAY}[ ]%{TIME}\\n\\nВ grokdebugger все парсится нормально, а вот в logstash нет, сообщение прилетает с тэгом _grokparsefailure.\\nPS: весь конфиг не влез, скопировал только тот шаблон по которому не парсится.'},\n",
       " {'text_field': 'У вас pattern ожидает пробел в конце, но в message у вас этого пробела нет.'},\n",
       " {'text_field': 'this happens when I look for a word in discover the kibana server stops and displays the following message:\\n\\n\\nvarsion kibana : 7.4.0\\nelasticsearch rest runing , when I look in kibana log files there is nothing'},\n",
       " {'text_field': \"It's resolved by increase memory heap for kibana . Thanks marttkime\"},\n",
       " {'text_field': 'Hi guys,\\nI was using the \"SortScriptDescriptor\" class in this part of a c# app:\\n                    SortScriptDescriptor&lt;dynamic&gt; sortScriptDescriptor = new SortScriptDescriptor&lt;dynamic&gt;();\\n                    if (sortItem.Dir == \"asc\")\\n                    {\\n                        sortScriptDescriptor.Ascending();\\n                    }\\n                    else\\n                    {\\n                        sortScriptDescriptor.Descending();\\n                    }\\n                        var mgFieldList = Schema.GetDimensions(indexGroup, measureGroup, null, null);\\n                        var parsedField = ScriptGenerator.GetAggregationScript(mgField.Expression, mgFieldList.Values.ToList());\\n                        var scriptSortType = mgField.FieldType.Contains(\"String\") ? \"string\" : \"number\";\\n                        sortDescriptor.Script(s =&gt; sortScriptDescriptor.Type(scriptSortType).Script(scr =&gt; scr.Source(parsedField.ElasticScript)));\\n\\nSince this class was deleted in NEST 7...  what would be its replacement?\\nThanks'},\n",
       " {'text_field': 'FYI this works:\\n                    sortDescriptor.Script(s =&gt; \\n                                             (sortItem.Dir.ToLower() == \"asc\" ? \\n                                                       s.Ascending().Script(s =&gt; s.Source(parsedField.ElasticScript)) : \\n                                                       s.Descending().Script(s =&gt; s.Source(parsedField.ElasticScript))\\n                                          ));\\n                }'},\n",
       " {'text_field': 'I have UTF-8 characters ingested into Elastic search. They got ingested as Json String. When I try to search the documents using Search Query I am getting Parse Error as below:\\nParse Error:\\nreason : Cannot parse \\'m-attributes.a_path_url:Gällande/PP/Batchprotocololl Kvalitetskontroll/lk000131.pdf\\' Lexical error at line 1 ,column 82. encountered after \"/lk000131.pdf\"\"\\nSearch Query : http://localhost:9200/system_metadata_index/_search?q=m-attributes.a_path_url:Gällande/PP/Batchprotocololl Kvalitetskontroll/lk000131.pdf\\nDocument in Elastic Search: a_path_url:Gällande/PP/Batchprotocololl Kvalitetskontroll/lk000131.pdf\\nBut when I search without UTF-8 characters, search is working fine. Please guide me how to search UTF-8 characters using search query.\\nI searched regarding UTF-8 characters. But i could not get the right answer.'},\n",
       " {'text_field': 'just an assumption here: maybe the URL encoding on your side is not correct. In order to rule that out, can you use a the body of a HTTP request for search and use something like this\\nPOST system_metadata_index/_search\\n{\\n  \"query\" : {\\n    \"query_string\" : {\\n            \"query\" : \"m-attributes.a_path_url:Gällande/PP/Batchprotocololl Kvalitetskontroll/lk000131.pdf\"\\n     }\\n  }\\n}\\n'},\n",
       " {'text_field': 'Hi it\\'s again me !\\nNow I\\'m trying to listen the port 514 with udp, it\\'s for syslog. Of course I\\'ve searched on the internet and all the port under 1024 are privileged. Alright so I\\'m trying the setcap command and I have this error message :\\n\\n[2019-11-06T08:58:54,323][INFO ][logstash.inputs.udp      ][main] Starting UDP listener {:address=&gt;\"0.0.0.0:514\"}\\n[2019-11-06T08:58:54,326][ERROR][logstash.inputs.udp      ][main] UDP listener died {:exception=&gt;#&lt;Errno::EADDRINUSE: Address already in use - bind - Adresse dÃ©jÃ  utilisÃ©e&gt;, :backtrace=&gt;[\"org/jruby/ext/socket/RubyUDPSocket.java:203:in bind\\'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:116:in udp_listener\\'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-udp-3.3.4/lib/logstash/inputs/udp.rb:68:in run\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:314:in inputworker\\'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:306:in `block in start_input\\'\"]}\\n\\nI don\\'t understand why it\\'s speaking about the IP address.\\nthanks'},\n",
       " {'text_field': \"EDIT : It's because of rsyslog wich is listening too on the 514 port\"},\n",
       " {'text_field': 'Hello!\\nI have this input:\\n\\n&lt;189&gt;date=2019-11-06 time=09:18:24 devname=\"FW\" devid=\"FG200D4Q645\" logid=\"0000000011\" type=\"traffic\" subtype=\"forward\" level=\"notice\" vd=\"josep\" eventtime=1573045874 srcip=192.168.1.1 srcport=22222 srcintf=\"vlan11\" srcintfrole=\"undefined\" dstip=4.221.55.87 dstport=443 dstintf=\"vlan55\" dstintfrole=\"undefined\" poluuid=\"8az53d526-5480-5je6-d2f3-1c0d252b8ccde\" sessionid=1457621354 proto=2 action=\"accept\" policyid=11 policytype=\"policy\" service=\"HTTPS\" dstcountry=\"Country\" srccountry=\"Reserved\" trandisp=\"snat\" transip=155.84.27.111 transport=22222 appid=41245 app=\"HTTPS.BROWSER\" appcat=\"Web.Client\" apprisk=\"medium\" applist=\"AppControl_General\" appact=\"detected\" duration=1120 sentbyte=4449 rcvdbyte=12391 sentpkt=223 rcvdpkt=222 sentdelta=222 rcvddelta=222\\n\\nI set the following input:\\n\\nsyslog {\\nport =&gt; 5444\\ntype =&gt; \\'firewall\\'\\n}\\n\\nThe data is coming to logstash but does not recognize the fields. I understand that if it is an input syslog would I have to do the automatic grok? Also I have a tag that says this:\\n\\ntags:  _grokparsefailure_sysloginput\\n\\nAny idea what can happen? Thanks for your time!'},\n",
       " {'text_field': 'Your syslog might be UDP or TCP - tcpdump to confirm and change the input.\\nAlso use kvdata instead since your message is already in a key value pair format.\\nhttps://www.elastic.co/guide/en/logstash/current/plugins-filters-kv.html\\nBasic example:\\ninput { \\n  udp {\\n    port =&gt; 5444\\n  }\\n}\\n\\nfilter {\\n  kv {\\n      source =&gt; \"message\"\\n  }\\n}\\n\\noutput {\\n  stdout {\\n    codec =&gt; \"rubydebug\"\\n  }\\n}\\n\\n\\nTest that and see if it works. It\\'s a very basic example just to put you on the right path.'},\n",
       " {'text_field': 'When accessing both the admin and the application using app search within the same browser, app search always tries to authenticate a request via the admin cookie, even when the search token is provided. So when the cookie is expired, you see an http auth dialog popping when doing a search.\\nThis issue was already discussed here Failed authentication after period of time - #6 by Dustin_Hughes'},\n",
       " {'text_field': 'After looking at the change log, this issue appears to be fixed from version 7.3.1.\\nI will try to update and open a new thread if the issue persists.\\nthank you'},\n",
       " {'text_field': 'Hi everyone, I started a few weeks ago to work with ELK and Filebeat in a Docker system for an university project. I have working the ELK, Kibana detect everything, including Beats, now I\\'m configuring Filebeat to send the logs to Logstash but I have an error when Filebeat tries to send something.\\n\\nERROR [centralmgmt.event_reporter] api/event_reporter.go:90 could not send events, error: 1 error: Beat \"id_number\" not found\\n\\nThe id_number of my Filebeat and the id_number that Kibana give me when I enroll Beats are different. I read about it and I think that I can\\'t put that number in a manual way, it\\'s automatic, and I don\\'t know how to resolve this.\\nI put my Filebeat configuration, maybe I have things wrong.\\nmanagement:\\n  enabled: true\\n  period: 1m0s\\n  events_reporter:\\n    period: 30s\\n    max_batch_size: 1000\\n  access_token: (Kibana_enroll_number)\\n  kibana:\\n    protocol: http\\n    host: localhost:5601\\n    ssl: null\\n    timeout: 10s\\n    ignoreversion: true\\n  blacklist:\\n    output: console|file\\n\\nfilebeat.inputs:\\n- type: log\\n  enabled: true\\n  paths:\\n    - (...)\\\\logs\\\\*.log\\noutput.logstash:\\n  hosts: [\"localhost:5000\"]    &lt; The port usually is 5044 but I put this.\\n\\nGreetings.'},\n",
       " {'text_field': 'I already solved my problem, I honestly don\\'t understand exactly the error, maybe I modified too much without knowing, I started again, I enroll again and without touching anything of the Filebeat configuration (.yml files), it worked, I configured the input and output of the Beat in Kibana and I had no more problems.\\nIn Kibana, in the Beat section, you have the \"Enrolled Beats\" and the \"Configuration tags\", the last one is used to create configurations of the output, input, modules, etc. (Configuration blocks) and use them as a tag for enrolled beats.\\nI think I didn\\'t change anything else.\\nI use this command for send the logs with Filebeat and it works, I see in Kibana the Logs.\\n\\nC:\\\\Program Files\\\\Filebeat&gt; ./filebeat -c filebeat.yml -e\\n\\nGreetings and good luck.\\n(My English is not the best  )'},\n",
       " {'text_field': 'Hi, I want to create custom Region map for all the cities in UK. I tried the United Kingdom Subdivisions in the vector map dropdown, but it won\\'t show all the drop down.\\nI followed this link: https://www.elastic.co/blog/custom-region-maps-in-kibana-6-0\\nAnd have pasted the Custom Region Map config in the yml file below.\\nPlease let me know what I\\'m doing wrong and how to do this.\\nKibana is served by a back end server. This setting specifies the port to use.\\n#server.port: 5601\\nSpecifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.\\nThe default is \\'localhost\\', which usually means remote machines will not be able to connect.\\nTo allow connections from remote users, set this parameter to a non-loopback address.\\n#server.host: \"localhost\"\\nEnables you to specify a path to mount Kibana at if you are running behind a proxy.\\nUse the server.rewriteBasePath setting to tell Kibana if it should remove the basePath\\nfrom requests it receives, and to prevent a deprecation warning at startup.\\nThis setting cannot end in a slash.\\n#server.basePath: \"\"\\nSpecifies whether Kibana should rewrite requests that are prefixed with\\n\\nserver.basePath or require that they are rewritten by your reverse proxy.\\nThis setting was effectively always false before Kibana 6.3 and will\\ndefault to true starting in Kibana 7.0.\\n#server.rewriteBasePath: false\\nThe maximum payload size in bytes for incoming server requests.\\n#server.maxPayloadBytes: 1048576\\nThe Kibana server\\'s name.  This is used for display purposes.\\n#server.name: \"your-hostname\"\\nThe URLs of the Elasticsearch instances to use for all your queries.\\n#elasticsearch.hosts: [\"http://localhost:9200\"]\\nWhen this setting\\'s value is true Kibana uses the hostname specified in the server.host\\nsetting. When the value of this setting is false, Kibana uses the hostname of the host\\nthat connects to this Kibana instance.\\n#elasticsearch.preserveHost: true\\nKibana uses an index in Elasticsearch to store saved searches, visualizations and\\ndashboards. Kibana creates a new index if the index doesn\\'t already exist.\\n#kibana.index: \".kibana\"\\nThe default application to load.\\n#kibana.defaultAppId: \"home\"\\nIf your Elasticsearch is protected with basic authentication, these settings provide\\nthe username and password that the Kibana server uses to perform maintenance on the Kibana\\nindex at startup. Your Kibana users still need to authenticate with Elasticsearch, which\\nis proxied through the Kibana server.\\n#elasticsearch.username: \"kibana\"\\n#elasticsearch.password: \"pass\"\\nEnables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively.\\nThese settings enable SSL for outgoing requests from the Kibana server to the browser.\\n#server.ssl.enabled: false\\n#server.ssl.certificate: /path/to/your/server.crt\\n#server.ssl.key: /path/to/your/server.key\\nOptional settings that provide the paths to the PEM-format SSL certificate and key files.\\nThese files validate that your Elasticsearch backend uses the same key files.\\n#elasticsearch.ssl.certificate: /path/to/your/client.crt\\n#elasticsearch.ssl.key: /path/to/your/client.key\\nOptional setting that enables you to specify a path to the PEM file for the certificate\\nauthority for your Elasticsearch instance.\\n#elasticsearch.ssl.certificateAuthorities: [ \"/path/to/your/CA.pem\" ]\\nTo disregard the validity of SSL certificates, change this setting\\'s value to \\'none\\'.\\n#elasticsearch.ssl.verificationMode: full\\nTime in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of\\nthe elasticsearch.requestTimeout setting.\\n#elasticsearch.pingTimeout: 1500\\nTime in milliseconds to wait for responses from the back end or Elasticsearch. This value\\nmust be a positive integer.\\n#elasticsearch.requestTimeout: 30000\\nList of Kibana client-side headers to send to Elasticsearch. To send no client-side\\nheaders, set this value to  (an empty list).\\n#elasticsearch.requestHeadersWhitelist: [ authorization ]\\nHeader names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten\\nby client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.\\n#elasticsearch.customHeaders: {}\\nTime in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.\\n#elasticsearch.shardTimeout: 30000\\nTime in milliseconds to wait for Elasticsearch at Kibana startup before retrying.\\n#elasticsearch.startupTimeout: 5000\\nLogs queries sent to Elasticsearch. Requires logging.verbose set to true.\\n#elasticsearch.logQueries: false\\nSpecifies the path where Kibana creates the process ID file.\\n#pid.file: /var/run/kibana.pid\\nEnables you specify a file where Kibana stores log output.\\n#logging.dest: stdout\\nSet the value of this setting to true to suppress all logging output.\\n#logging.silent: false\\nSet the value of this setting to true to suppress all logging output other than error messages.\\n#logging.quiet: false\\nSet the value of this setting to true to log all events, including system usage information\\nand all requests.\\n#logging.verbose: false\\nSet the interval in milliseconds to sample system and process performance\\nmetrics. Minimum is 100ms. Defaults to 5000.\\n#ops.interval: 5000\\nSpecifies locale to be used for all localizable strings, dates and number formats.\\nSupported languages are the following: English - en , by default , Chinese - zh-CN .\\n#i18n.locale: \"en\"\\nCustom Region Maps\\n#regionmap:\\n#layers:\\n#- name: \"UK Cities\"\\n#url: \"https://gist.githubusercontent.com/duhaime/1d6d5a8dc77c86128fcc1a05a72726c9/raw/8b8522cbc69498b6c4983a9f58c045c2b451cb89/british-isles-counties.geojson\"\\n   #fields:\\n      #- name: \"CITY_NAME\"\\n        #description: \"City Name\"'},\n",
       " {'text_field': 'My apologies. I meant you should only remove the # before the lines where you define your custom region map. That may be the cause of the error.'},\n",
       " {'text_field': 'Some of the docs in an index having the time filter is null/none/no value. How to create a GET query to filter out those documents?'},\n",
       " {'text_field': 'you can use the exists query, see https://www.elastic.co/guide/en/elasticsearch/reference/7.4/query-dsl-exists-query.html\\nUse a bool query with a must_not clause, that contains the exists query, basically searching for a document where this field does not exist.\\n--Alex'},\n",
       " {'text_field': 'I\\'m preparing an upgrade of ELK for a customer, moving from 6.2.0 to 7.4.1. The upgrade of ELK to 6.8.4 worked fine, however after upgrading to 7.4.1 logstash no longer sends data to ES.\\nES and Kibana are both on 7.4.1 and seem to be working fine.\\nI ran a debug of both versions...\\nLogstash 6.8.4:\\n[2019-11-06T15:32:02,187][DEBUG][logstash.filters.grok    ] Running grok filter {:event=&gt;#&lt;LogStash::Event:0x67d8770a&gt;}\\n[2019-11-06T15:32:02,188][DEBUG][logstash.filters.grok    ] Event now:  {:event=&gt;#&lt;LogStash::Event:0x67d8770a&gt;}\\n[2019-11-06T15:32:02,189][DEBUG][logstash.pipeline        ] output received {\"event\"=&gt; ...\\n\\nLogstash 7.4.1 with same config/data\\n[2019-11-06T16:15:26,393][DEBUG][logstash.filters.grok    ][main] Running grok filter {:event=&gt;#&lt;LogStash::Event:0x5d46d9de&gt;}\\n[2019-11-06T16:15:26,394][DEBUG][logstash.filters.grok    ][main] Event now:  {:event=&gt;#&lt;LogStash::Event:0x5d46d9de&gt;}\\n\\nLogstash 6.8.4 has a pipeline/output entry after the grok filters, however with 7.4.1 no pipeline/output is shown in the debug output.\\nMy config:\\ninput {\\n\\tfile {\\n\\t\\tpath =&gt; \"E:/syslog-data/SyslogCatchAll*.txt\"\\n\\t\\ttype =&gt; \"syslog\"\\n\\t\\tsincedb_path =&gt; \"E:/elk-data/logstash/sincedb/SyslogCatchAll.idx\"\\n\\t}\\n}\\n\\nfilter {\\n\\tgrok {\\n\\t\\tmatch =&gt; {\\n\\t\\t\\t\"message\" =&gt; \"%{TIMESTAMP_ISO8601:syslog_timestamp}\\\\t%{WORD:syslog_facility}\\\\.%{WORD:syslog_level}\\\\t%{IPV4:syslog_host_address}\\\\t%{GREEDYDATA:syslog_message}\"\\n\\t\\t}\\n\\t}\\n\\n\\tif [syslog_host_address] == \"10.255.255.2\" {\\n\\t\\tmutate {\\n\\t\\t\\tadd_field =&gt; {\\n\\t\\t\\t\\t\"syslog_srcdevice\" =&gt; \"fw\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tkv {\\n\\t\\t\\tsource =&gt; \"syslog_message\" \\n\\t\\t\\tprefix =&gt; \"fw_\"\\n\\t\\t}\\n\\t}\\n\\n\\tif [syslog_host_address] in [\"172.16.100.250\",\"172.16.100.251\",\"172.16.100.252\",\"172.16.100.253\"] {\\n\\t\\tmutate {\\n\\t\\t\\tadd_field =&gt; {\\n\\t\\t\\t\\t\"syslog_srcdevice\" =&gt; \"ap\"\\n\\t\\t\\t}\\n\\t\\t\\tgsub =&gt; [\\n\\t\\t\\t\\t\"syslog_message\", \"\\\\[\", \"(\",\\n\\t\\t\\t\\t\"syslog_message\", \"\\\\]\", \")\"\\n\\t\\t\\t]\\n\\t\\t}\\n\\t\\t\\n\\t\\tgrok {\\n\\t\\t\\tmatch =&gt; {\\n\\t\\t\\t\\t\"syslog_message\" =&gt; \"\\\\{%{GREEDYDATA:ap_message}\\\\}%{GREEDYDATA} Reason: %{GREEDYDATA:ap_reason}\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tkv {\\n\\t\\t\\tsource =&gt; \"ap_message\"\\n\\t\\t\\tfield_split =&gt; \" ,\"\\n\\t\\t\\tvalue_split =&gt; \":\"\\n\\t\\t\\ttrim_key =&gt; \"\\\\\"\"\\n\\t\\t\\ttrim_value =&gt; \"\\\\\"\"\\n\\t\\t\\tprefix =&gt; \"ap_\"\\n\\t\\t}\\n\\t}\\n}\\n\\noutput {\\n\\telasticsearch { \\n\\t\\thosts =&gt; [\"10.0.0.1:9200\"]\\n\\t}\\n}\\n\\nI did not see any breaking changes which might explain this behaviour.\\nDid I miss something? Do I need to add something to the elastic search output config with version 7+ ?'},\n",
       " {'text_field': 'I think this is related to ILM. However, \"logstash\" is supposed to be the rollover alias, which should get aliased to a different index name.'},\n",
       " {'text_field': 'Hi,\\nWith Kibana spaces and user/role privileges, is there a way in which I can give users permission to create new visualizations/dashboards, but at the same time making sure that the user doesnt have right to delete any existing visualization?\\nOr, is there a way in which I can specify read privileges on individual dashboards/visualizations?'},\n",
       " {'text_field': 'Thanks guys..'},\n",
       " {'text_field': 'Hello!\\nI have files being sent in CSV format. My logstash config ONLY parses them into different fields. It does nothing else.\\nFor some reason, whenever there is a date in the CSV file, the field that should contain that date has the wrong date. The field also looks like a timestamp; it has a time and a date instead of just \"Month Day YYYY\". The added timestamp for every single field with this issue is \"18:00:00:000\" That time never changes.\\nAny help would be awesome!\\nThank you.'},\n",
       " {'text_field': 'I would guess they are being indexed in elasticsearch with type date (possibly due to a template) and that your timezone is 6 hours behind UTC.'},\n",
       " {'text_field': 'How can I replace the ID values at the bottom of this graph with something like A, B, C using Elasticsearch or Kibana UI tool?\\n'},\n",
       " {'text_field': \"Thank you! This worked.\\nAlso, to anyone using the solution, you might have to use the &lt;field_name&gt;.keyword instead of just the &lt;field_name&gt; to get the result that you want.\\nEdit:\\nChecking if the field exists is also helpful to avoid searching for fields that don't exist in your documents in your index pattern. Returning null will return '-' in Kibana discover page and void shard failure.\\nBetter solution: \\nif (!doc['YOUR_ID_FIELD.keyword'].empty){\\n\\tif (doc['YOUR_ID_FIELD'].value == 'id12345') {\\n\\t  return 'A'\\n\\t} else {\\n\\t  return 'B'\\n\\t}\\n} else {\\n    return null;\\n}\"},\n",
       " {'text_field': \"Guys,\\nI'm supposed to work on an existing product which is using ELK. The product has APM support for web services. Now my task is to develop a Batch framework for to be used by that product. Is it possible to configure the APM for batch programs??\\nIf possible, please share me the docs for it.\\nI've been searching for it, but most of the results were like ELK's APM supports only for spring boot and servlets.\\nHelp me throughout the problem, guys!!\"},\n",
       " {'text_field': 'Sure thing. Just make sure you configure application_packages if you use the annotations API.'},\n",
       " {'text_field': 'I\\'m learning to use filebeat, I was wondering if there is a way to convert the ingest pipeline from elasticsearch to logstash config\\nI\\'ve had a look here\\nhttps://www.elastic.co/guide/en/logstash/current/ingest-converter.html\\nCan\\'t really find a json for Cisco module in filebeat\\nShould I be able to copy this to a logstash conf?\\ncurl -XGET localhost:9200/_ingest/pipeline?pretty\\nfilebeat-7.4.2-cisco-ftd-asa-ftd-pipeline\" : {\\n    \"description\" : \"Pipeline for Cisco FTD logs\",\\n..truncated\\n\\nI have also tried to send output directly to elasticsearch but for some reason the timestamp is in the future.\\nRunning 7.4.2 on Ubuntu 18.04\\nThanks\\nIs there a reason why I can\\'t install the Cisco ASA dashboard when I install through the linux filebeat from the server\\nIt only works if i Install from a windows version of filebeat???'},\n",
       " {'text_field': \"Hi @VamPikmin,\\nI don't know much about the ingest converter, but in principle it should be possible to use the ingest pipeline you are getting with curl. You cannot find the json pipeline in the filebeat repository because some pipelines, including the Cisco ones are developed in YAML format, for readability and improved maintainability.\\n\\n\\n\\n VamPikmin:\\n\\nI have also tried to send output directly to elasticsearch but for some reason the timestamp is in the future.\\n\\n\\nIf you are not using UTC this is probably related to an issue with timezones we had in some modules. This will be solved in 7.5.0 (see https://github.com/elastic/beats/issues/13877).\\n\\n\\n\\n VamPikmin:\\n\\nIs there a reason why I can't install the Cisco ASA dashboard when I install through the linux filebeat from the server\\nIt only works if i Install from a windows version of filebeat???\\n\\n\\nIt should work the same with Linux or Windows following these instructions.\"},\n",
       " {'text_field': 'I have configured VPC Flow logs to ship to S3 and then an SQS message queue to notify Filebeat.\\nThis creates records in ES but they appear like this! Any ideas?\\n\\ns3-to-es.png1248×619 76.7 KB\\n'},\n",
       " {'text_field': \"Thanks for the info! Yeah it's because VPC logs are gz files. https://github.com/elastic/beats/pull/13980 added the support and will be released in 7.5 Filebeat.\"},\n",
       " {'text_field': 'Hello\\nI have several large Clusters and I have some clients that want to the Scroll API can the CCQC handles sending the scroll wanting data from multiple indexes or does the scroll only work targeting the specific cluster.\\nCurrently we are using 6.3 across all clusters and CCQC\\nAny help be appreciate.'},\n",
       " {'text_field': 'Scroll for Cross cluster search appears to be available in 6.3:\\n\\n  \\n      github.com/elastic/elasticsearch\\n  \\n  \\n    \\n  \\n    \\n  \\n\\n  \\n    \\n      Add Cross Cluster Search support for scroll searches\\n    \\n\\n    \\n      elastic:master ← s1monw:scroll_cross_cluster\\n    \\n\\n    \\n      \\n        opened 07:59AM - 07 Jun 17 UTC\\n      \\n\\n      \\n        \\n          \\n          s1monw\\n        \\n      \\n\\n      \\n        \\n          +390\\n          -148\\n        \\n      \\n    \\n\\n  \\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n'},\n",
       " {'text_field': 'Hello,\\nIn lumberjack output, I see hosts is an array.\\n2 questions:\\n\\nIs it failover only or also load balancing? The documentation is not very talkative.\\nssl_certificate... Are we unable to have one cert per host also?\\n\\nhttps://www.elastic.co/guide/en/logstash/current/plugins-outputs-lumberjack.html\\nThank you '},\n",
       " {'text_field': \"In my experience it's kind of both.\\nI  do not use lumberjack output, but i do know what you mean by other .conf files.\\nI put in 3 Hosts, and i noticed when i was reindexing a lot of data, that one of my  Elastic-Nodes was under heavy traffic. So i took a look in my Network tool, and the Outputs stopped choosing the host which was so busy. I guess it has something to do with the response time from your certain hosts.\\nAnd Of course, if one host from your array is not reachable, this option works as a failover by choosing a node which is reachable.\\nCan't answer your second question, sorry.\\nHope this helps..\"},\n",
       " {'text_field': 'Hello,\\nThis is a screenshot of the load of one of our nodes in our monitoring cluster:\\n\\nI really like this way of presenting stuff and I was wondering if it is possible to reproduce this kind of visualisation in Kibana on other data (inclusive the arrow indicating the data representing is increasing or decreasing)?\\nGrtz\\nWillem'},\n",
       " {'text_field': 'See https://github.com/elastic/kibana/issues/51231'},\n",
       " {'text_field': 'A single host installation is running out of disk well before I would expect given the amount of data stored in clusters. However I am struggling to understand the contact points to use to diagnose this with ECE or eventually to control it.\\nGET /_cluster/allocation/explain tells \"can_allocate\" : \"no, allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes\\nGET _cat/shards shows only some indices running to low hundreds of MB - totalling probably a couple of GB at the most\\nGET _cat/allocation shows disk used 18.4GB avail 1.5GB\\nUsing df and du on the underlying ECE node on AWS suggests, the disk is filling.\\nSurprisingly I found the proxyv2 directory was comparable in size to the allocator one - this looked suspicious.\\nDrilling down deeper I found that the majority of the proxyv2 usage was uncompressed logs which were huge.\\nSo tactical question - how can I change the logging policy for proxy nodes to avoid storing these relatively useless logs or at the very least, compress them?\\nAnd strategic question, what is best practice for storage management within ECE?\\nI will delete some of these logs to get the ECE off its knees but would like a better solution.'},\n",
       " {'text_field': \"Apparently the proxy v2 logging has the following max sizes:\\n\\nby default 14 files of 500Mb [...]\\nThat's for requests, there's 4 more 500Mb files (transport, errors, etc) by default\\n\\nSo you can indeed get up to 9GB of data. We're looking into either reducing that or making it configurable, but until then it will be necessary to ensure enough disk space to be safe - sorry for the inconvenience\"},\n",
       " {'text_field': 'Hello,\\nI want to know if is possible to create now or in the future credential for searching, where we can limit value user can search and retrieve.\\nExample, I got a Document with this schema:\\n\\n{ title: string, description: string, country: string }\\n\\nI want user can just search title and description and retrieve it.\\nAnd another user with another credential search can search and retrieve all values.\\nTell me if this can be possible.\\nThanks'},\n",
       " {'text_field': 'Hi @Jonathan_Gautier \\nHave you checked out Signed Search Keys?'},\n",
       " {'text_field': 'Hello,\\nLet\\'s assume that i have something like that in elastic search\\nalarm_id sequence date          alarm_text\\n1        1        2019-11-7     text of alarm1\\n2        1        2019-11-7     text of alarm2\\n2        2        2019-11-8     text of alarm2\\n2        3        2019-11-9     text of alarm2\\n3        1        2019-11-8     text of alarm3\\n3        2        2019-11-10    text of alarm3\\n\\nand i want to group this information like that in kibana:\\nalarm_id      count         date          alarm_text\\n1             1             2019-11-7     text of alarm1\\n2             3             2019-11-9     text of alarm2\\n3             2             2019-11-10    text of alarm3\\n\\nwhich means that i want to have a grouping by alarm id, a column \"count\" which keeps the maximum number of \"sequence\" column for that alarm_id (or alternatively the number of occurrences of that alarm_id), the newest date and its alarm text\\ni managed to have only the first two columns using a data table visualization but i couldn\\'t have the rest columns too. Is there any way (visualization or search) to do it in kibana or is not feasible?\\nThanks in advance'},\n",
       " {'text_field': 'When you are applying filters they are applied before the aggregation is run, so you can not apply them partially.'},\n",
       " {'text_field': 'hello, I am on ELK 7.4.\\nWhen using geoip in a Logstash conf the geoip.location doesn\\'t appear to be displayed in the index fields in Kibana.\\nBelow is the Logstash conf - showing the fields filter input under the geoip filter. (if this is now included then the fields do not show and are unable to see displayed geo data in Kibana - but the geoip.location is not shown in the index mapping - so am creating the new coordinates field).\\ninput {\\n  file {\\n    path=&gt;\"/route/to/file/*.csv\"\\n    start_position=&gt;\"beginning\"\\n  }\\n}\\nfilter {\\n    csv {\\n       separator=&gt;\",\"\\n       columns=&gt;[\"Id\",\"Date/Time\",\"SenderIP\",\"Workload\"]\\n    }\\n    geoip {\\n      source =&gt; [\"SenderIP\"]\\n      target =&gt; \"geoip\"\\n      fields =&gt; [\"continent_code\", \"longitude\", \"city_name\", \"region_code\", \"country_name\", \"location\", \"ip\", \"latitude\"]\\n      add_field =&gt; [ \"[geoip][coordinates]\", \"%{[geoip][longitude]}\" ]\\n      add_field =&gt; [ \"[geoip][coordinates]\", \"%{[geoip][latitude]}\" ]\\n    }\\n    mutate {\\n      convert =&gt; [ \"[geoip][coordinates]\", \"float\"  ]\\n    }\\n}\\noutput {\\n  elasticsearch\\t{\\n    hosts =&gt; [\"localhost:9200\"]\\n    index =&gt; \"info_file\"\\n    manage_template =&gt;false\\n  }\\n}\\n\\nWhen saying geoip.location, the data is then displayed in to two separate fields of geoip.location.lon and geoip.location.lat, instead of showing, for an example, as -\\ngeoip.location                      {\\n                                      lon: 0.0001\\n                                      lat: 0.0001\\n                                    } \\n\\nI am able to add_field and mutate this into an new field, i.e. geoip.coordinates (as seen in the above conf). and that will be added to the index mapping.\\nIs there a way to format this into a index template so that it will be able to be formatted correctly for the following/ future creation of index\\'s, being able to use geoip.location correctly?\\nAlso, when creating a point/ visualization on the maps function in Kibana, the index in question is not displayed. Am I missing a trick here? or if this because of the above? with the non matching of geoip.location and therefore geo_point formation type?\\nThank you!'},\n",
       " {'text_field': 'You need to use an index template. It could look very much like the default template for logstash-*, which defines geoip.location as a geo_point. Just change the index_patterns field.'},\n",
       " {'text_field': 'I have an index in elastic, which is fed by two pipelines.\\npipeline 1 provides the base data for the document\\nfields: id, name, number\\npipeline 2 needs to add new fields to the existing document based on the ID.\\nfields: id, color, comment\\nWhen pipeline 1 sends data, the document looks like this:\\n{ \\'id\\': \\'1234\\', \\'name\\': \\'john smith\\', \\'number\\': \\'123-345-567\\' }\\nWhen pipeline 2 sends data, it overwrites the original document (based on fingerprint of id), and removes the existing data.\\nSo now the document looks like:\\n{ \\'id\\': \\'1234\\', \\'color\\': \\'red\\', \\'comment\\': \\'he likes red\\' }\\nI understand that elastic doesn\\'t actually update, instead it deletes the existing document, and replaces it.\\nBut is there some way I can tell logstash, to tell elastic that \"take the data already stored, and combine it with this new data, and create a new document\".\\nSo my final document should look like this:\\n{ \\'id\\': \\'1234\\',  \\'name\\': \\'john smith\\', \\'number\\': \\'123-345-567\\',  \\'color\\': \\'red\\', \\'comment\\': \\'he likes red\\'  }\\nthanks in advance for any help.'},\n",
       " {'text_field': 'I have solved it myself \\nbased on this post: doc_as_upsert vs upsert properties\\nIt was required to make it an upsert.\\nso my logstash config looks something like this\\n\\ninput {\\nudp {\\nport =&gt; 10001\\ncodec =&gt; json\\n}\\n}\\nfilter {\\nfingerprint {\\nsource =&gt; \"id\"\\ntarget =&gt; \"[@metadata][fingerprint]\"\\nmethod =&gt; \"MURMUR3\"\\n}\\n}\\noutput {\\nelasticsearch {\\nhosts =&gt; [\"https://127.0.0.1:9200\"]\\nuser =&gt; \"logstash_account\"\\npassword =&gt; \"some_password_here\"\\ncacert =&gt; \"/etc/logstash/conf.d/cacert.cer\"\\nssl_certificate_verification =&gt; false\\ndocument_id =&gt; \"%{[@metadata][fingerprint]}\"\\nindex =&gt; \"some_index\"\\ndoc_as_upsert =&gt; true\\naction =&gt; update\\n}\\n}\\n'},\n",
       " {'text_field': 'Hi,\\nI am installing elasticsearch in a kubernetes environment with IPv6 network. Using helm charts from https://github.com/elastic/helm-charts/tree/master/elasticsearch.\\n\\nI configured network.host as [\"::\"], but with this setting, the masters do not form cluster. All 3 masters fail with masterNotDiscoveredException.\\n\\n&gt; {\"level\":\"INFO\",\"logger\":\"o.e.t.TransportService\",**\"log\":\"publish_address {127.0.0.1:9300}, bound_addresses {[::]:9300}\"}**\\n&gt; {\"level\":\"INFO\",\"logger\":\"o.e.b.BootstrapChecks\",\"log\":\"bound or publishing to a non-loopback address, enforcing bootstrap checks\"}\\n&gt; {\"level\":\"WARN\",\"logger\":\"o.e.c.c.ClusterFormationFailureHelper\",\"log\":\"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and this node must discover master-eligible nodes [elk-es-master-0, elk-es-master-1, elk-es-master-2] to bootstrap a cluster: have discovered []; discovery will continue using [[2019:280:4000:2001:9473:52ea:17a3:7b60]:9300, [2019:280:4000:2001:9496:22ed:bb1c:dd50]:9300, [2019:280:4000:2001:c262:ac59:17ae:fa66]:9300] from hosts providers and [{elk-es-master-0}{VbeA4QrPRv2YsNF1GA0gAQ}{0fpINSVoQSqhRc4N3OPJCg}{127.0.0.1}{127.0.0.1:9300}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\"}\\n&gt; {\"level\":\"WARN\",\"logger\":\"o.e.n.Node\",\"log\":\"timed out while waiting for initial discovery state - timeout: 30s\"}\\n&gt; {\"level\":\"INFO\",\"logger\":\"o.e.h.AbstractHttpServerTransport\",\"log\":\"publish_address {127.0.0.1:9200}, bound_addresses {[::]:9200}\"}\\n&gt; {\"level\":\"INFO\",\"logger\":\"o.e.n.Node\",\"log\":\"started\"}\\n&gt; {\"level\":\"WARN\",\"logger\":\"r.suppressed\",\"log\":\"path: /_cluster/health, params: {wait_for_status=green, timeout=1s}\"}\\n&gt; org.elasticsearch.discovery.MasterNotDiscoveredException: null\\n&gt;         at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:259) [elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:322) [elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:249) [elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:555) [elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:681) [elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\\n&gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\\n&gt;         at java.lang.Thread.run(Thread.java:834) [?:?]\\n\\nThis is the network interfaces info from inside the container:\\n&gt; [elasticsearch@elk-es-master-0 ~]$ ifconfig\\n&gt; **eth0**: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\\n&gt;         inet6 2019:280:4000:2001:9473:52ea:17a3:7b60  prefixlen 128  scopeid 0x0&lt;global&gt;\\n&gt;         inet6 fe80::50c0:4fff:fe95:e035  prefixlen 64  scopeid 0x20&lt;link&gt;\\n&gt;         ether 52:c0:4f:95:e0:35  txqueuelen 0  (Ethernet)\\n&gt;         RX packets 33854  bytes 3342419 (3.1 MiB)\\n&gt;         RX errors 0  dropped 0  overruns 0  frame 0\\n&gt;         TX packets 33847  bytes 3341284 (3.1 MiB)\\n&gt;         TX errors 0  dropped 1 overruns 0  carrier 0  collisions 0\\n&gt; \\n&gt; lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536\\n&gt;         inet 127.0.0.1  netmask 255.0.0.0\\n&gt;         inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;\\n&gt;         loop  txqueuelen 1000  (Local Loopback)\\n&gt;         RX packets 271244  bytes 13308286 (12.6 MiB)\\n&gt;         RX errors 0  dropped 0  overruns 0  frame 0\\n&gt;         TX packets 271244  bytes 13308286 (12.6 MiB)\\n&gt;         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\\n\\n\\nSo, next I configured network.host as \"_ eth0_:ipv6\". With this all 3 master pods fail with the error\\n\\n&gt; \"o.e.b.ElasticsearchUncaughtExceptionHandler\",\"log\":\"uncaught exception in thread [main]\"}\\n&gt; org.elasticsearch.bootstrap.StartupException: BindTransportException[**Failed to resolve host [_eth0_:ipv6]]; nested: UnknownHostException[_eth0_:ipv6: Name or service not** known];\\n&gt;         at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt; Caused by: org.elasticsearch.transport.BindTransportException: Failed to resolve host [_eth0_:ipv6]\\n&gt;         at org.elasticsearch.transport.TcpTransport.bindServer(TcpTransport.java:361) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.transport.netty4.Netty4Transport.doStart(Netty4Transport.java:136) ~[?:?]\\n&gt;         at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:61) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.transport.TransportService.doStart(TransportService.java:228) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:61) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.node.Node.start(Node.java:662) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:263) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:334) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.0.1.jar:7.0.1]\\n&gt;         ... 6 more\\n\\nPlease let me know what would be right configuration for network_host &amp; are any other network settings needed for IPv6 environment?\\nThanks,\\nShivani'},\n",
       " {'text_field': 'Thanks. I think\\nnetwork.host: \"_eth0:ipv6_\"\\n\\nshould work for you, assuming that this picks the addresses to which elasticsearch-master-headless resolves. Could you try that (NB not _eth0_:ipv6)?'},\n",
       " {'text_field': 'My sample GeoJSON file..\\n\"type\": \"FeatureCollection\",\\n  \"features\": [\\n    {\\n      \"type\": \"Feature\",\\n      \"properties\": {\\n        \"FIDindex\": 6035\\n      },\\n      \"geometry\": {\\n        \"type\": \"Polygon\",\\n        \"coordinates\": [\\n          [\\n            [\\n              -79.51092606323992,\\n              48.344613038602226\\n            ],\\n            [\\n              -79.5331359417738,\\n              48.330644012951296\\n            ],\\n            [\\n              -79.54767721776007,\\n              48.34385594693975\\n            ],\\n            [\\n              -79.54098581498502,\\n              48.34806594628507\\n            ],\\n            [\\n              -79.51092606323992,\\n              48.344613038602226\\n            ]\\n          ]\\n        ]\\n      }\\n    },\\n    {\\n      \"type\": \"Feature\",\\n      \"properties\": {\\n        \"FIDindex\": 6036\\n      },\\n      \"geometry\": {\\n        \"type\": \"Polygon\",\\n        \"coordinates\": [\\n          [\\n            [\\n              -79.49765416107279,\\n              48.50665405729244\\n            ],\\n            [\\n              -79.50452010147922,\\n              48.497367987214574\\n            ],\\n            [\\n              -79.52998110253995,\\n              48.49616195107807\\n            ],\\n            [\\n              -79.51270010258166,\\n              48.52528298266071\\n            ],\\n            [\\n              -79.49765416107279,\\n              48.50665405729244\\n            ]\\n          ]\\n        ]\\n      }\\n    },\\n    {\\n      \"type\": \"Feature\",\\n      \"properties\": {\\n        \"FIDindex\": 6037\\n      },\\n      \"geometry\": {\\n        \"type\": \"Polygon\",\\n        \"coordinates\": [\\n          [\\n            [\\n              -79.4937419793212,\\n              49.08821604910215\\n            ],\\n            [\\n              -79.52318087215492,\\n              49.02943208366772\\n            ],\\n            [\\n              -79.55202111684834,\\n              49.03141796850029\\n            ],\\n            [\\n              -79.56778415808171,\\n              49.056759963348156\\n            ],\\n            [\\n              -79.4937419793212,\\n              49.08821604910215\\n            ]\\n          ],\\n          [\\n            [\\n              -79.5365910174132,\\n              49.04234909537448\\n            ],\\n            [\\n              -79.53981414365047,\\n              49.04198099108133\\n            ],\\n            [\\n              -79.5416999721036,\\n              49.04055804587344\\n            ],\\n            [\\n              -79.538320178897,\\n              49.04091392507655\\n            ],\\n            [\\n              -79.5365910174132,\\n              49.04234909537448\\n            ]\\n          ],\\n          [\\n            [\\n              -79.53314181028657,\\n              49.047972036859456\\n            ],\\n            [\\n              -79.54218399465174,\\n              49.04691700341087\\n            ],\\n            [\\n              -79.54466192708877,\\n              49.04621599583252\\n            ],\\n            [\\n              -79.53695381365907,\\n              49.044765936954086\\n            ],\\n            [\\n              -79.53314181028657,\\n              49.047972036859456\\n            ]\\n          ],\\n          [\\n            [\\n              -79.52812883212242,\\n              49.050876877000356\\n            ],\\n            [\\n              -79.53206385269057,\\n              49.051771037330965\\n            ],\\n            [\\n              -79.5346651964943,\\n              49.05115596741972\\n            ],\\n            [\\n              -79.5336059608879,\\n              49.05080696502161\\n            ],\\n            [\\n              -79.52812883212242,\\n              49.050876877000356\\n            ]\\n          ],\\n          [\\n            [\\n              -79.5202589809016,\\n              49.05320498871566\\n            ],\\n            [\\n              -79.52311796828306,\\n              49.0535590070871\\n            ],\\n            [\\n              -79.5249519759151,\\n              49.0516320071246\\n            ],\\n            [\\n              -79.52163902727364,\\n              49.05194802154083\\n            ],\\n            [\\n              -79.5202589809016,\\n              49.05320498871566\\n            ]\\n          ]\\n        ]\\n      }\\n    },\\n\\n..... goes on like this ..\\nam trying to index this in Kibana..\\nIt is a Shapefile .. which I converted to GeoJSON .. I dint upload it directly on Kibana because of the size limitation and also I have this GeoJSON size(7 MB) still takes forever to index it when uploaded as GeoJSON in Kibana maps directly.. Looking for a best way to load this GeoJSON on to kibana/elastic\\nHow can this be done using Logstash / elastic ?'},\n",
       " {'text_field': 'Hi @Jayasri_Cimba,\\nUnfortunately, GDAL version 2.4.2 will not work with your Elasticsearch 7.3 version. You will have to upgrade to GDAL 3.1.0 by building from the source code.\\nI also have a Mac and this is how I build GDAL from source.\\n\\nClone the gdal repository to my computer\\n\\ncd into the gdal subdirectory of the repository\\nRun ./configure\\n\\nRun make (this will take a long time)\\nRun make install\\n\\nRun ogr2ogr --version to confirm I am running 3.1.0-dev-*.\\n'},\n",
       " {'text_field': 'I am not able to find the issue with this error and how to understand whats happening! Please help.\\nlog   [19:35:49.863] [info][listening] Server running at http://localhost:5601\\n  log   [19:35:49.878] [info][server][Kibana][http] http server running at http://localhost:5601\\n  log   [19:35:49.890] [warning][telemetry] Error scheduling task, received [task:oss_telemetry-vis_telemetry]: version conflict, document already exists (current version [7]): [version_conflict_engine_exception] [task:oss_telemetry-vis_telemetry]: version conflict, document already exists (current version [7]), with { index_uuid=\"vpLXqEc_QlOXhqnPG1bOeQ\" &amp; shard=\"0\" &amp; index=\".kibana_task_manager_1\" }\\n  log   [19:35:49.891] [warning][maps] Error scheduling telemetry task, received [task:Maps-maps_telemetry]: version conflict, document already exists (current version [33]): [version_conflict_engine_exception] [task:Maps-maps_telemetry]: version conflict, document already exists (current version [33]), with { index_uuid=\"vpLXqEc_QlOXhqnPG1bOeQ\" &amp; shard=\"0\" &amp; index=\".kibana_task_manager_1\" }\\n  log   [19:35:49.892] [info][status][plugin:spaces@7.4.0] Status changed from yellow to green - Ready\\n'},\n",
       " {'text_field': \"Hello Mehak,\\nYou should be able to safely ignore this issue. There's a bug in how these tasks are handled that's being addressed as part of the resolution for this issue. Effectively, this means the task has already been created and can't be overwritten (which is ok) when you restart Kibana.\\nRegards,\\nAaron\"},\n",
       " {'text_field': \"Hi!\\nI'm working with the open source version of Elasticsearch 7.4. I'm trying to conect to my index from Oracle Analytics Desktop and it requires me to enter username and password. xPack is not configured in my cluster.\\nWhich username and password should I use?\"},\n",
       " {'text_field': \"We were on this all day long yesterday.\\nApparently, the error comes from the driver we're using. The driver is for maximum Elasticsearch 6.8, but we're using 7.4. This is the driver required by the tool (Oracle Analytics Desktop), so I guess there's not much we can do...\\n\\n  \\n      \\n      GitHub\\n  \\n  \\n    \\n\\nAnchormen/sql4es\\n\\nsql4es: JDBC driver for Elasticsearch. Contribute to Anchormen/sql4es development by creating an account on GitHub.\\n\\n\\n  \\n  \\n    \\n    \\n  \\n  \\n\\n\\nRegards,\"},\n",
       " {'text_field': 'Hi,\\nI am new to Kibana and I am trying to visualize on how many times a particular windows event happened per day.\\nFor an example lets say I want to count the number of successful logons for a server per day. The relevant Windows event ID is 4624. I am successfully getting the windows security log via Winlogbeat to my elasticsearch engine but couldnt figure out the proper settings for a graph.\\nA vertical bar or a Line graph is preferred.\\nY -axis should be the count of number of events with ID 4624 and X -axis should be the day.\\nAny help is highly appreciated.\\nThank you.'},\n",
       " {'text_field': 'Hi @forestgump,\\nyou can do this by chossing the line visualization when navigating to \"Visualize\" and clicking the \"Add new visualization\" there.\\nTo have a per-day date histogram on the x axis you have add a bucket aggregation, select date histogram as the aggregation type and select your time field. In the interval field you can specify the interval (in this case Daily).\\nTo only take the logon events into account you have to apply a filter in the top bar - click \"Add filter\" there and set it to \"your event id field name is 4624\".\\n\\nScreenshot 2019-11-09 at 17.15.41.png710×1570 68.7 KB\\n\\nThe filter will be saved along with the visualization.'},\n",
       " {'text_field': 'I have 10 servers that i have Filebeat installed in. Each server monitors 2 applications, a total of 20 applications.\\nI have one Logstash server which collects all the above logs and passes it to Elasticsearch after filtering of these logs.\\nTo read  one file from one server , I use the below Logstash configuration:\\ninput {\\n  beats {\\n    port =&gt; 5044\\n  }\\n}\\nfilter {\\n    grok {\\nmatch =&gt; {\"message\" =&gt;\"\\\\[%{TIMESTAMP_ISO8601:timestamp}\\\\]%{SPACE}\\\\[%{DATA:Severity}\\\\]%{SPACE}\\\\[%{DATA:Plugin}\\\\]%{SPACE}\\\\[%{DATA:Servername}\\\\](?&lt;short_message&gt;(.|\\\\r|\\\\n)*)\"}\\n    }\\n} \\noutput {\\n  elasticsearch {\\n    hosts =&gt; [\"&lt;ESserverip&gt;:9200\"]\\n    index =&gt; \"groklogs\"\\n}\\n          stdout { codec =&gt; rubydebug }\\n}\\n\\nAnd this is the filebeat configuration:\\npaths:\\n    - D:\\\\ELK 7.1.0\\\\elasticsearch-7.1.0-windows-x86_64\\\\elasticsearch-7.1.0\\\\logs\\\\*.log\\n\\noutput.logstash:\\n  hosts: [\"&lt;logstaship&gt;:5044\"]\\n\\nCan anyone please give me an example of\\n\\nHow i should convert the above to receive from multiple applications from multiple servers.\\nShould i configure multiple ports? How?\\nHow should i use multiple Groks?\\nHow can i optimize it in a single or minimal logstash configuration files?\\n\\nHow will a typical set up look. Please help me.'},\n",
       " {'text_field': 'You can add tags to each prospector in filebeat, then use conditionals based on the tags to determine which filters to apply.'},\n",
       " {'text_field': 'I want to remove the timelion option from the sidebar.\\nI was successfully able to remove other option by adjusting the link array from src\\\\legacy\\\\core_plugins\\\\kibana\\\\index.js file.\\nTimelion option is not there. Please help if anyone know where to edit to remove the timelion option completely from the sidebar.\\nThanks in advance.'},\n",
       " {'text_field': 'Hi @amanciao,\\nwhich version of Kibana are you using? Starting from 7.0 the Timelion app in the sidebar is disabled by default.\\nIf you are using 6.8, the Timelion app is located in https://github.com/elastic/kibana/blob/6.8/src/legacy/core_plugins/timelion/index.js - removing the app property of the uiExports object should remove the app.'},\n",
       " {'text_field': 'Hello,\\nI installed filebeat, used with apache module a few month ago.\\nEverything was ok since I try to add the response time to my apache logs using this tutorial : https://www.partiallydisassembled.net/posts/filebeat-fields.html.\\nThen, the access logs are not sent to  elastic search anymore, but the error logs is still working.\\nI spent my whole day trying to debug the issue but cannot understand where the problem really is.\\nI don\\'t see any issues explaining it in the filebeat logs\\nCould you help me ?\\nSome complementaty informations :\\n#/etc/filebeat/filebeat.yml\\n\\n#========================= Central Management =================================\\n# Beats is configured under central management, you can define most settings\\n# from the Kibana UI. You can update this file to configure the settings that\\n# are not supported by Kibana Beats management.\\nmanagement:\\n  enabled: true\\n  period: 1m0s\\n  events_reporter:\\n    period: 30s\\n    max_batch_size: 1000\\n  access_token: ${management.accesstoken}\\n  kibana:\\n    protocol: https\\n    host: my-HOST\\n    ssl: null\\n    timeout: 10s\\n    ignoreversion: true\\n  blacklist:\\n    output: console|file\\n#=============================== General =====================================\\n# The name of the shipper that publishes the network data. It can be used to group\\n# all the transactions sent by a single shipper in the web interface.\\n#name:\\n# The tags of the shipper are included in their own field with each\\n# transaction published.\\n#tags: [\"service-X\", \"web-tier\"]\\n# Optional fields that you can specify to add additional information to the\\n# output.\\n#fields:\\n#  env: staging\\n#================================ Logging =====================================\\n# Sets log level. The default log level is info.\\n# Available log levels are: error, warning, info, debug\\nlogging.level: debug\\n# At debug level, you can selectively enable logging only for some components.\\n# To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\",\\n# \"publish\", \"service\".\\n#logging.selectors: [\"*\"]\\n\\n#============================== Xpack Monitoring ===============================\\n# filebeat can export internal metrics to a central Elasticsearch monitoring\\n# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The\\n# reporting is disabled by default.\\n# Set to true to enable the monitoring reporter.\\n#monitoring.enabled: false\\n# Uncomment to send the metrics to Elasticsearch. Most settings from the\\n# Elasticsearch output are accepted here as well.\\n# Note that the settings should point to your Elasticsearch *monitoring* cluster.\\n# Any setting that is not set is automatically inherited from the Elasticsearch\\n# output configuration, so if you have the Elasticsearch output configured such\\n# that it is pointing to your Elasticsearch monitoring cluster, you can simply\\n# uncomment the following line.\\n#monitoring.elasticsearch:\\n#============================= Elastic Cloud ==================================\\n# These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/).\\n# The cloud.id setting overwrites the `output.elasticsearch.hosts` and\\n# `setup.kibana.host` options.\\n# You can find the `cloud.id` in the Elastic Cloud web UI.\\ncloud.id: MY_ID\\n# The cloud.auth setting overwrites the `output.elasticsearch.username` and\\n# `output.elasticsearch.password` settings. The format is `&lt;user&gt;:&lt;pass&gt;`.\\ncloud.auth: MY:CREDENTIALS\\n#============================= Modules ============================================\\nfilebeat.config.modules:\\n  enabled: true\\n  path: ${path.config}/modules.d/*.yml\\nfilebeat.overwrite_pipelines: true\\n#setup.template.name: \"filebeat\"\\n\\n\\n\\n\\n#/etc/filebeat/modules.d/apache.yml \\n# Module: apache\\n# Docs: https://www.elastic.co/guide/en/beats/filebeat/7.3/filebeat-module-apache.html\\n- module: apache\\n  # Access logs\\n  access:\\n    enabled: true\\n    # Set custom paths for the log files. If left empty,\\n    # Filebeat will choose the paths depending on your OS.\\n   # var.paths: \\n   #   - \"/var/log/apache2/access.log*\"\\n  # Error logs\\n  error:\\n    enabled: true\\n    # Set custom paths for the log files. If left empty,\\n    # Filebeat will choose the paths depending on your OS.\\n    #var.paths: \\n    #  - \"/var/log/apache2/error.log*\"\\n\\napache log example :\\n127.0.0.1 - - [08/Nov/2019:06:32:51 +0000] \"GET /server-status?auto= HTTP/1.1\" 1199 200 593 \"-\" \"Go-http-client/1.1\" \\n127.0.0.1 - - [08/Nov/2019:06:33:01 +0000] \"GET /server-status?auto= HTTP/1.1\" 1428 200 591 \"-\" \"Go-http-client/1.1\" \\n108.162.221.146 - jeremy.gachet@gmail.com [08/Nov/2019:06:33:02 +0000] \"HEAD / HTTP/1.1\" 178080 200 2171 \"http://spoon-elastic.com\" \"Mozilla/5.0+(compatible; UptimeRobot/2.0; http://www.uptimerobot.com/)\" \\n127.0.0.1 - - [08/Nov/2019:06:33:51 +0000] \"GET /server-status?auto= HTTP/1.1\" 1523 200 595 \"-\" \"Go-http-client/1.1\" \\n172.69.94.62 - - [08/Nov/2019:06:33:57 +0000] \"GET / HTTP/1.1\" 293976 200 7949 \"-\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36\" \\n172.69.94.44 - - [08/Nov/2019:06:33:58 +0000] \"GET /wp-content/uploads/2015/06/tv-dashboard-office.png HTTP/1.1\" 1190 304 2100 \"https://spoon-elastic.com/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78\\n.0.3904.70 Safari/537.36\" \\n127.0.0.1 - - [08/Nov/2019:06:34:01 +0000] \"GET /server-status?auto= HTTP/1.1\" 1240 200 595 \"-\" \"Go-http-client/1.1\"\\n\\nMy pipeline (well imported in ES) :\\n{\\n\"filebeat-7.3.1-apache-access-default\" : {\\n(...)\\n\"processors\" : [\\n{\\n\"grok\" : {\\n\"field\" : \"message\",\\n\"patterns\" : [\\n\"\"\"%{IPORHOST:source.address} - %{DATA:user.name} [%{HTTPDATE:apache.access.time}] \"(?:%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}|-)?\" %{NUMBER:http.response.time:long} %{NUMBER:http.response.status_code:long} (?:%{NUMBER:bytes}|-) ( \"%{DATA:http.request.referrer}\")?( \"%{DATA:user_agent.original}\")?\"\"\",\\n\"\"\"%{IPORHOST:source.address} - %{DATA:user.name} [%{HTTPDATE:apache.access.time}] \"-\" %{NUMBER:http.response.status_code:long}\"\"\",\\n\"\"\"[%{HTTPDATE:apache.access.time}] %{IPORHOST:source.address} %{DATA:apache.access.ssl.protocol} %{DATA:apache.access.ssl.cipher} \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\" %{NUMBER:http.response.body.bytes:long}\"\"\"\\n],\\n\"ignore_missing\" : true'},\n",
       " {'text_field': 'Hi and thank you for your answer.\\nOf course. I had access to error logs but not to access logs.\\nFinally I managed to fix my issue by totally remove and reinstall filebeat from the beginning.'},\n",
       " {'text_field': 'Hello there,\\nI\\'m migrating my live data from our live elasticsearch server to our archive elasticsearch server. Both are single nodes. While doing this I missed to close indices on the archive server. Using curator I got the error \"Validation Failed: 1: this action would add [2] total shards, but this cluster currently has [999]/[1000] maximum shards open;\" somewhere while executing the task. I tried to close indices on the archive server, but I always get \"master_not_discovered_exception\", regardless of what I\\'m trying to do, e. g. quering some statistics from the archive:\\n\\nroot@elk-archive-01 ~ # curl -X GET \"10.0.0.210:9200/_cluster/health?wait_for_status=yellow&amp;timeout=50s&amp;pretty\"\\n{\\n  \"error\" : {\\n    \"root_cause\" : [\\n      {\\n        \"type\" : \"master_not_discovered_exception\",\\n        \"reason\" : null\\n      }\\n    ],\\n    \"type\" : \"master_not_discovered_exception\",\\n    \"reason\" : null\\n  },\\n  \"status\" : 503\\n}\\n\\n\\nIn the logs the following lines are suspicous:\\n\\nfailing [elected-as-master ([1] nodes joined)[{elk-archive-01-n1}{QGFmDMAITRu4JNov0SwH1Q}{RbaWF3t7QKyXKWDa3fEbYw}{10.0.0.210}{10.0.0.210:9300}{dilm}{ml.machine_memory=33449811968, xpack.installed=true, ml.max_open_jobs=20} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_]]: failed to commit cluster state version [19845]\\norg.elasticsearch.cluster.coordination.FailedToCommitClusterStateException: publication failed\\n[...]\\nCaused by: org.elasticsearch.ElasticsearchException: publication cancelled before committing: timed out after 30s\\n\\n\\nUnfortunately I don\\'t know where to upload the whole logfile \\nHow can I close the indices to get the server back up again? Thanks a lot for your help!'},\n",
       " {'text_field': 'Ok, following \"Can not elect master when restarting cluster from 7.3 upgrade\" fixed the problem.'},\n",
       " {'text_field': \"Hello guys, i tried to implement an ilm-policy.\\nNow i always get the following error:\\n\\nillegal_argument_exception: source alias [Syslog] does not point to a write index\\n\\n\\ngrafik.png806×267 10.3 KB\\n\\nNow, i don't really get how i make this index my write index. Anybody got an idea how to solve this, without loosing data?\\nGreets,\\nMoritz\"},\n",
       " {'text_field': 'So i got to fix it now. I messed up big times.\\nMy predecessor put in the alias \"Syslog\" in both the template for logstash* &amp; for the ILM, after my post i repeatedly got the error that the \"alias was found in a template and therefore doubled\". I deleted the Alias from the Index-Template, and finally i change my \"Index =&gt; \" Config in my logstash.conf file to the Alias which i would assign later on as my ILM Alias, then i again opened a new Index (deleted the old logstash-vmw-000001) Now it\\'s all working!\\nThanks anyway!'},\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_questions = pd_df_clean[\"question.text\"].values\n",
    "docs_solutions = pd_df_clean[\"solution.text\"].values\n",
    "\n",
    "docs = []\n",
    "r = docs_questions.size\n",
    "for i in range (0,r):\n",
    "    docs.append({\"text_field\" : docs_questions[i]})\n",
    "    docs.append({\"text_field\" : docs_solutions[i]})\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker setup + model import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure things run smoothly with all the requirements and versioning for pytorch and the Elastic clients, we recommend using the docker containers for the NLP tasks. \n",
    "\n",
    "See the docs here: https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html#ml-nlp-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have recently published a docker client for Eland so you can directly pull it from here rather than building the container locally:\n",
    "\n",
    "https://container-library.elastic.co/r/eland/eland:8.9.0\n",
    "\n",
    "```\n",
    "docker pull docker.elastic.co/eland/eland:8.9.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build the docker in your environment by running the following code in your terminal (and using your credentials locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export CLOUD_ID=change_me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker run -it --rm docker.elastic.co/eland/eland:8.9.0 \\\n",
    "    eland_import_hub_model \\\n",
    "      --cloud-id $CLOUD_ID \\\n",
    "      -u $USER -p $PASSWORD \\\n",
    "      --hub-model-id distilbert-base-uncased-finetuned-sst-2-english \\\n",
    "      --task-type text_classification \\\n",
    "      --start \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Sentiment Analysis from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/sm4kh0y91fs5_fft4375wwh00000gn/T/ipykernel_3232/78141079.py:1: DeprecationWarning: Importing from the 'elasticsearch.client' module is deprecated. Instead use 'elasticsearch' module for importing the client.\n",
      "  from elasticsearch.client import MlClient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'routing_state': 'started'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch.client import MlClient\n",
    "\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "models = MlClient.get_trained_models(client, model_id=model_id)\n",
    "models.body\n",
    "\n",
    "stats = MlClient.get_trained_models_stats(client, model_id=model_id)\n",
    "stats.body['trained_model_stats'][0]['deployment_stats']['nodes'][0]['routing_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'routing_state': 'started'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = MlClient.get_trained_models_stats(client, model_id=model_id)\n",
    "stats.body['trained_model_stats'][0]['deployment_stats']['nodes'][0]['routing_state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make calls to the model locally, or create a pipeline that applies the transformation to your entire index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = MlClient.infer_trained_model(client, model_id=model_id, docs=docs[0:10], timeout=None)[\"inference_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "   docs[i][\"probability\"] = response[i][\"predicted_value\"]\n",
    "   docs[i][\"score\"] = response[i][\"prediction_probability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text_field': 'Hello,\\nI have a problem with my filter, i get the \"_grokparsefailure\" tag when the concerned logs are processed.\\nHere is my filter file:\\nfilter {\\nif [source] == \"/var/log/auth.log\"{\\ngrok {\\nmatch =&gt;  [ \"message\", \"%{SYSLOGTIMESTAMP:date} %{SYSLOGHOST:host} %{DATA:program}(?:[%{POSINT:pid}])?: %{GREEDYDATA:smt}: %{GREEDYDATA:smt2} for user %{USER:user}\" ]\\nadd_field =&gt; [ \"received_at\", \"%{@timestamp}\" ]\\nadd_field =&gt; [ \"received_from\", \"%{host}\" ]\\n}\\ndate {\\nmatch =&gt;  [ \"date\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ]\\n}\\n}\\n}\\nI tried it with the input: \"Aug  6 12:17:01 stack CRON[14336]: pam_unix(cron:session): session closed for user root\" on  http://grokdebug.herokuapp.com/  and it works fine.\\nIs there any problem with my code ?\\nThank you,\\nManal',\n",
       "  'probability': 'POSITIVE',\n",
       "  'score': 0.9826626663460409},\n",
       " {'text_field': \"A space was missing , that's why it wasn't working. I fixed it. Thank you !\",\n",
       "  'probability': 'POSITIVE',\n",
       "  'score': 0.9610335595916407},\n",
       " {'text_field': 'TL;DR\\nHow do I continuously update an alias to capture new matching index patterns?\\nLonger Version...\\nWe use a non-standard format that looks like this:\\nmyapp-beatname-version-YYYY.mm.dd\\n\\nI.e., a heartbeat index will look like:\\nmyapp-heartbeat-7.2.0-2019.08.06\\n\\nWhen using the Uptime module, the heartbeat documentation notes that the Uptime module matches the index pattern heartbeat-7* and suggests using an alias if the index format doesn\\'t match that. However, the alias documentation makes it clear that the following alias specification:\\n{\\n    \"actions\" : [\\n        { \"add\" : { \"index\" : \"*-heartbeat-*\", \"alias\" : \"heartbeat-7.x.x\" } }\\n    ]\\n}\\n\\nHowever, the index alias documentation states that:\\n\\nIn this case, the alias is a point-in-time alias that will group all current indices that match, it will not automatically update as new indices that match this pattern are added/removed.\\n\\nObviously, this does not work well with the Uptime module, where the alias must continuously update and capture newly-created aliases.\\nI\\'m certain there is a way around this, but I\\'m still enough of a novice in the ELK stack that I don\\'t see it and haven\\'t been able to find it in my searches.',\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.999617286279932},\n",
       " {'text_field': 'My workaround for this was to add the following to the template:\\n\"aliases\": {\\n  \"heartbeat-7.3.0-alias\": {}\\n},\\n\\nThis has resolved the issue. I already use a script to upload templates to Elasticsearch. This script also edits the templates on the fly to add an additional index pattern match in order to apply it to our alternate index-name convention, so I should be able to update it to add the alias on the fly as well.',\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.9973071725909045},\n",
       " {'text_field': \"Hi there,\\nI am attempting to stream logs from AWS Lambda into Elasticsearch/Kibana.\\nI am doing so via a POST to the Elasticsearch endpoint, + the index and document type (e.g. POST https://example-es-domain.com/alblogs/alb-access-logs/).\\nI have verified that the lambda is sending out the request by sending to requestbin, and have verified that the information contained in each doc is correct.\\nHowever I can't see any of the information sent to Elasticsearch being loaded into Kibana, and was wondering if I was missing something.\\nThe full flow for the operation is:\\nALB -&gt; S3 -&gt; AWS Lambda -&gt; Elasticsearch\",\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.9982413812756282},\n",
       " {'text_field': 'Please try the following, tested locally in 6.8, I\\'ve used you index + type naming, which is in your case identical.\\nPUT a pattern for you index\\nPUT alb-access-logs\\n{\\n  \"mappings\": {\\n    \"alb-access-logs\": {\\n      \"properties\": {\\n        \"timestamp\": {\\n          \"type\": \"date\"\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nPOST an example\\nPOST alb-access-logs/alb-access-logs\\n {\\n    \"timestamp\": \"2019-08-06T15:54:46.701974Z\"\\n }\\n\\nLet\\'s check the results:\\n GET alb-access-logs/_search \\n{\\n  \"sort\" : [\\n      {\"timestamp\" : {\"order\" : \"asc\", \"mode\" : \"avg\"}}\\n   ]\\n}\\n\\nthis should work',\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.9217321490268277},\n",
       " {'text_field': '\\n\\nThere is no pipeline plugin. Unlike other inputs it is implemented in the logstash core.\\n\\n\\nYes, you can keep using files, no need to use config.string. config.string is simpler to use in the documentation.\\n\\n\\nNo, weblogs is not the pipeline id, it is the virtual address of the pipeline input. The pipeline can be called anything.\\n\\n',\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.9907977387584275},\n",
       " {'text_field': 'Okay. I have solved the pipeline virtual address problem. The name cannot contain - character, so renaming it to \"legacyva\" resolved my problem.\\nThere is some weirdness with yaml that I don\\'t quite understand. As you have described, putting quotes around the [type] variable causes the pipeline to fail to load. Also, setting the type in filebeat to anything other than log causes filebeat to be unable to start. I\\'m thinking it may be expecting certain inputs, but in any case I cannot seem to name this what I want, with or without quotes.\\nSo I really need \"tags\" or some other construct in order to perform the loop properly, and I must put quotes and must not put dashes. I am concerned that using \"tags\" affects the logic, as I am only looking for a single flag match, and == I believe means matches exactly. I\\'ll keep testing to see if I can work out this last piece.\\nedit&gt; Did some searching on \"logstash tags conditionals\" and found that the syntax is different. Not sure where this is documented but I copied a gentleman\\'s example and changed from if [tags] == \"tagname\" to if \"tagname\" in [tags]. I do feel better about this, even if I don\\'t understand it, as that == was really bothering me since it needed to be more of a \"contains\" rather than \"is equal to\" (if I\\'m remembering things correctly). Anyways, glad I learned something, and now my conditional statement is catching my file based on a tag! HOORAY!\\nBig thanks to @Badger, I kept trying to drive off the course and you definitely got me back on path more than once!\\nFor anyone else who is trying to do specifically this thing in the future, that is to say ship many different files with filebeat to a single logstash listener, then separate the pipelines by some pre-defined value so that many filters can be used and some modularity is provided for the index choice, here is the base template you need for your pipeline.conf files:\\nmaster-pipeline.conf\\ninput {\\n  beats {\\n    port =&gt; \"5000\"\\n  }\\n}\\noutput {\\n### conditional statement to separate logs by tag ###\\n  if \"primary\" in [tags] {\\n    pipeline { send_to =&gt; primaryvirtualaddress }\\n  } else {\\n    pipeline { send_to =&gt; fallbackvirtualaddress }\\n  }\\n}\\n\\neach subsequent pipeline:\\nprimary-pipeline.conf\\ninput {\\n  pipeline {\\n    address =&gt; \"primaryvirtualaddress\"\\n  }\\n}\\noutput {\\n  elasticsearch {\\n    hosts =&gt; [\"ES CLUSTER ADDRESS\"]\\n    index =&gt; \"primary_index\"\\n  }\\n}\\n\\nYou may be able to remove the quotes from the address, but I\\'m quite sick of re-testing at this point and I know this template works ',\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.9984135220801649},\n",
       " {'text_field': 'Hi all,\\nI am using version 7.2.\\nI encountered some trouble while using the filter elasticsearch plugin.\\nThe behavior is very random, some of the data uploaded correctly but I also get some error\\nHere is the log from logstash\\n\\n[2019-08-06T10:07:02,177][WARN ][logstash.filters.elasticsearch] Failed to query elasticsearch for previous event {:index=&gt;\"mapping_device_sn\", :error=&gt;\"[400] {\"error\":{\"root_cause\":[{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"mapping_device_sn\",\"node\":\"kuAYIY0yTlCZM7Du464gYg\",\"reason\":{\"type\":\"query_shard_exception\",\"reason\":\"Failed to parse query [sn:%{[device_sn]}]\",\"index_uuid\":\"vnxHfJgsRz6f4KIne-zZrA\",\"index\":\"mapping_device_sn\",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Cannot parse \\'sn:%{[device_sn]}\\': Encountered \\\\\" \\\\\"]\\\\\" \\\\\"] \\\\\"\\\\\" at line 1, column 15.\\\\nWas expecting:\\\\n    \\\\\"TO\\\\\" ...\\\\n    \",\"caused_by\":{\"type\":\"parse_exception\",\"reason\":\"Encountered \\\\\" \\\\\"]\\\\\" \\\\\"] \\\\\"\\\\\" at line 1, column 15.\\\\nWas expecting:\\\\n    \\\\\"TO\\\\\" ...\\\\n    \"}}}}]},\"status\":400}\"}\\n\\nLog from elasticsearch\\n[elasticsearch.server][DEBUG] All shards failed for phase: [query]\\n\\n[elasticsearch.server][DEBUG] [0], node[kuAYIY0yTlCZM7Du464gYg], [P], s[STARTED], a[id=NKeTm6rjTgCj_zwuFPyacA]: Failed to execute [SearchRequest{searchType=QUERY_THEN_FETCH, indices=[mapping_device_sn], indicesOptions=IndicesOptions[ignore_unavailable=false, allow_no_indices=true, expand_wildcards_open=true, expand_wildcards_closed=false, allow_aliases_to_multiple_indices=true, forbid_closed_indices=true, ignore_aliases=false, ignore_throttled=true], types=[], routing=\\'null\\', preference=\\'null\\', requestCache=null, scroll=null, maxConcurrentShardRequests=0, batchedReduceSize=512, preFilterShardSize=128, allowPartialSearchResults=true, localClusterAlias=null, getOrCreateAbsoluteStartMillis=-1, ccsMinimizeRoundtrips=true, source={\"size\":1,\"query\":{\"query_string\":{\"query\":\"sn:%{[device_sn]}\",\"fields\":[],\"type\":\"best_fields\",\"default_operator\":\"or\",\"max_determinized_states\":10000,\"enable_position_increments\":true,\"fuzziness\":\"AUTO\",\"fuzzy_prefix_length\":0,\"fuzzy_max_expansions\":50,\"phrase_slop\":0,\"analyze_wildcard\":false,\"escape\":false,\"auto_generate_synonyms_phrase_query\":true,\"fuzzy_transpositions\":true,\"boost\":1.0}},\"sort\":[{\"@timestamp\":{\"order\":\"desc\"}}]}}]\\n\\nMy logstash config\\ninput {\\n    file {\\n        path =&gt; \"/data/ELK_raw/IPS/data/*/ips_aggregate.csv\"\\n        sincedb_path =&gt; \"/dev/null\"\\n        mode =&gt; \"read\"\\n        file_completed_action =&gt; \"log\"\\n        file_completed_log_path =&gt; \"/data/ELK/read_log/ips_read_log.txt\"\\n        type =&gt; \"ips\"\\n    }\\n}\\n\\nfilter {\\n    csv {\\n        autodetect_column_names =&gt; \"true\"\\n        autogenerate_column_names =&gt; \"true\"\\n        skip_header =&gt; \"true\"\\n        separator =&gt; \",\"\\n    }\\nelasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_ips\"\\n        query =&gt; \"id:%{[id]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \" signature_name\" =&gt; \"signature_name\"\\n            \" engine_rule\" =&gt; \"engine_rule\"\\n        }\\n    }\\n\\n   elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"mapping_device_sn\"\\n        query =&gt; \"sn:%{[device_sn]}\"\\n        result_size =&gt; 1\\n        fields =&gt; {\\n            \"first_industry\" =&gt; \"first_industry\"\\n            \"customer\" =&gt; \"customer\"\\n            \"is_trial\" =&gt; \"is_trial\"\\n            \"product_type\" =&gt; \"product_type\"\\n            \"second_industry\" =&gt; \"second_industry\"\\n            \"warranty_date\" =&gt; \"warranty_date\"\\n        }\\n    }\\n\\n   mutate {\\n        remove_field =&gt; [ \"@timestamp\" ]\\n        remove_field =&gt; [ \"@version\" ]\\n        remove_field =&gt; [ \"host\" ]\\n        remove_field =&gt; [ \"message\" ]\\n        remove_field =&gt; [ \"path\" ] \\n        remove_field =&gt; [ \"type\" ] \\n    }   \\n}\\n\\noutput {\\n    elasticsearch {\\n        hosts =&gt; [\"localhost:9200\"]\\n        index =&gt; \"cv_ips\"\\n    }\\n}\\n\\nHave anyone encountered this before?\\nThanks',\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.9996519075042721},\n",
       " {'text_field': \"Can you do the 'grep -rn ...' and verify that the line number is always 1?\\nYou do have --pipeline.workers 1 set, right?\",\n",
       "  'probability': 'NEGATIVE',\n",
       "  'score': 0.9978876015717107}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add NLP enrichment to index through pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create this pipeline in your dev tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUT _ingest/pipeline/sentiment\n",
    "{\n",
    "  \"processors\": [\n",
    "    {\n",
    "      \"inference\": {\n",
    "        \"model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"field_map\": {\n",
    "          \"Question.text\": \"text_field\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "POST _reindex?wait_for_completion=false\n",
    "{\n",
    "  \"source\": {\n",
    "    \"index\": \"bytes-discuss-06\"\n",
    "  },\n",
    "  \"dest\": {\n",
    "    \"index\": \"bytes-discuss-06-with-sentiment\",\n",
    "    \"pipeline\": \"sentiment\"\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
